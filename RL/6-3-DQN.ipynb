{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9BBqrZ+F3mDK01dUJ+99B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSJeong-me/AI-Innovation-2024/blob/main/RL/6-3-DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qndzg0YJf7fD",
        "outputId": "bf0969ab-a75e-460e-e93b-72bedf2dab7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (3.1.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement random (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for random\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement collections (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for collections\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting torch.optim\n",
            "  Downloading torch_optim-0.0.4-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting deap>=1.3.1 (from torch.optim)\n",
            "  Downloading deap-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting pytorch-ignite>=0.4.8 (from torch.optim)\n",
            "  Downloading pytorch_ignite-0.5.1-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting thop>=0.0.31 (from torch.optim)\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torch.optim) (2.4.1+cu121)\n",
            "Collecting torch-pruning>=0.2.7 (from torch.optim)\n",
            "  Downloading torch_pruning-1.4.3-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: torchvision>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from torch.optim) (0.19.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deap>=1.3.1->torch.optim) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytorch-ignite>=0.4.8->torch.optim) (24.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torch.optim) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torch.optim) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torch.optim) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torch.optim) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torch.optim) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torch.optim) (2024.6.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.11.1->torch.optim) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torch.optim) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torch.optim) (1.3.0)\n",
            "Downloading torch_optim-0.0.4-py3-none-any.whl (59.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deap-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_ignite-0.5.1-py3-none-any.whl (312 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.7/312.7 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Downloading torch_pruning-1.4.3-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.9/62.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: deap, torch-pruning, thop, pytorch-ignite, torch.optim\n",
            "Successfully installed deap-1.4.1 pytorch-ignite-0.5.1 thop-0.1.1.post2209072238 torch-pruning-1.4.3 torch.optim-0.0.4\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch.nn (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch.nn\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install gym\n",
        "!pip install torch\n",
        "!pip install numpy\n",
        "!pip install random\n",
        "!pip install collections\n",
        "!pip install torch.optim\n",
        "!pip install torch.nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "# Q-Network 모델 정의\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, 24)\n",
        "        self.fc2 = nn.Linear(24, 24)\n",
        "        self.fc3 = nn.Linear(24, action_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.fc1(state))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "# Hyperparameters\n",
        "state_size = 4\n",
        "action_size = 2\n",
        "batch_size = 64\n",
        "gamma = 0.99  # 할인율\n",
        "epsilon = 1.0  # 탐험률\n",
        "epsilon_min = 0.01\n",
        "epsilon_decay = 0.995\n",
        "learning_rate = 0.001\n",
        "target_update = 10\n",
        "\n",
        "# Replay Memory\n",
        "memory = deque(maxlen=2000)\n",
        "\n",
        "# 모델과 타겟 네트워크 초기화\n",
        "q_network = QNetwork(state_size, action_size)\n",
        "target_network = QNetwork(state_size, action_size)\n",
        "target_network.load_state_dict(q_network.state_dict())  # 가중치 복사\n",
        "optimizer = optim.Adam(q_network.parameters(), lr=learning_rate)\n",
        "\n",
        "# 경험 샘플링 함수\n",
        "def replay(memory, batch_size):\n",
        "    if len(memory) < batch_size:\n",
        "        return\n",
        "    minibatch = random.sample(memory, batch_size)\n",
        "    states, actions, rewards, next_states, dones = zip(*minibatch)\n",
        "\n",
        "    states = torch.tensor(states, dtype=torch.float32)\n",
        "    actions = torch.tensor(actions, dtype=torch.long)\n",
        "    rewards = torch.tensor(rewards, dtype=torch.float32)\n",
        "    next_states = torch.tensor(next_states, dtype=torch.float32)\n",
        "    dones = torch.tensor(dones, dtype=torch.float32)\n",
        "\n",
        "    # 현재 상태에서의 Q값 계산\n",
        "    q_values = q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "    # 다음 상태에서의 Q값 계산 (타겟 네트워크 사용)\n",
        "    next_q_values = target_network(next_states).max(1)[0]\n",
        "    expected_q_values = rewards + (gamma * next_q_values * (1 - dones))\n",
        "\n",
        "    # 손실 계산 및 역전파\n",
        "    loss = nn.MSELoss()(q_values, expected_q_values.detach())\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# 행동 선택 함수 (ε-greedy)\n",
        "def choose_action(state, epsilon):\n",
        "    if np.random.rand() <= epsilon:\n",
        "        return random.randrange(action_size)\n",
        "    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        q_values = q_network(state)\n",
        "    return np.argmax(q_values.numpy())\n",
        "\n",
        "# CartPole 환경 설정\n",
        "env = gym.make('CartPole-v1')\n",
        "episodes = 1000\n",
        "\n",
        "# 학습 루프\n",
        "for episode in range(episodes):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # 행동 선택\n",
        "        action = choose_action(state, epsilon)\n",
        "\n",
        "        # 환경에서 한 단계 진행\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        # 보상 조정\n",
        "        reward = reward if not done else -10\n",
        "\n",
        "        # 메모리에 저장\n",
        "        memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "        # 상태 업데이트\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        # 경험 리플레이\n",
        "        replay(memory, batch_size)\n",
        "\n",
        "    # 탐험률 감소\n",
        "    if epsilon > epsilon_min:\n",
        "        epsilon *= epsilon_decay\n",
        "\n",
        "    # 타겟 네트워크 업데이트\n",
        "    if episode % target_update == 0:\n",
        "        target_network.load_state_dict(q_network.state_dict())\n",
        "\n",
        "    print(f\"Episode: {episode}, Total reward: {total_reward}, Epsilon: {epsilon}\")\n",
        "\n",
        "# 학습 완료 후 에이전트 테스트\n",
        "for i in range(10):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        env.render()\n",
        "        action = choose_action(state, epsilon_min)  # 최종 탐험률로 행동 선택\n",
        "        next_state, _, done, _ = env.step(action)\n",
        "        state = next_state\n",
        "env.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeroV4fbf-aj",
        "outputId": "b4aeb512-3dc6-4570-f6cd-dcaa7dc9a5c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n",
            "<ipython-input-2-4a33df966720>:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  states = torch.tensor(states, dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 0, Total reward: 23.0, Epsilon: 0.995\n",
            "Episode: 1, Total reward: 24.0, Epsilon: 0.990025\n",
            "Episode: 2, Total reward: 12.0, Epsilon: 0.985074875\n",
            "Episode: 3, Total reward: 10.0, Epsilon: 0.9801495006250001\n",
            "Episode: 4, Total reward: 15.0, Epsilon: 0.9752487531218751\n",
            "Episode: 5, Total reward: 13.0, Epsilon: 0.9703725093562657\n",
            "Episode: 6, Total reward: 18.0, Epsilon: 0.9655206468094844\n",
            "Episode: 7, Total reward: 4.0, Epsilon: 0.960693043575437\n",
            "Episode: 8, Total reward: 87.0, Epsilon: 0.9558895783575597\n",
            "Episode: 9, Total reward: 5.0, Epsilon: 0.9511101304657719\n",
            "Episode: 10, Total reward: 24.0, Epsilon: 0.946354579813443\n",
            "Episode: 11, Total reward: 13.0, Epsilon: 0.9416228069143757\n",
            "Episode: 12, Total reward: 25.0, Epsilon: 0.9369146928798039\n",
            "Episode: 13, Total reward: 22.0, Epsilon: 0.9322301194154049\n",
            "Episode: 14, Total reward: 5.0, Epsilon: 0.9275689688183278\n",
            "Episode: 15, Total reward: 6.0, Epsilon: 0.9229311239742362\n",
            "Episode: 16, Total reward: 13.0, Epsilon: 0.918316468354365\n",
            "Episode: 17, Total reward: 19.0, Epsilon: 0.9137248860125932\n",
            "Episode: 18, Total reward: 4.0, Epsilon: 0.9091562615825302\n",
            "Episode: 19, Total reward: 8.0, Epsilon: 0.9046104802746175\n",
            "Episode: 20, Total reward: 7.0, Epsilon: 0.9000874278732445\n",
            "Episode: 21, Total reward: 12.0, Epsilon: 0.8955869907338783\n",
            "Episode: 22, Total reward: 26.0, Epsilon: 0.8911090557802088\n",
            "Episode: 23, Total reward: 11.0, Epsilon: 0.8866535105013078\n",
            "Episode: 24, Total reward: 4.0, Epsilon: 0.8822202429488013\n",
            "Episode: 25, Total reward: 11.0, Epsilon: 0.8778091417340573\n",
            "Episode: 26, Total reward: 5.0, Epsilon: 0.8734200960253871\n",
            "Episode: 27, Total reward: 22.0, Epsilon: 0.8690529955452602\n",
            "Episode: 28, Total reward: 3.0, Epsilon: 0.8647077305675338\n",
            "Episode: 29, Total reward: 6.0, Epsilon: 0.8603841919146962\n",
            "Episode: 30, Total reward: 2.0, Epsilon: 0.8560822709551227\n",
            "Episode: 31, Total reward: 3.0, Epsilon: 0.851801859600347\n",
            "Episode: 32, Total reward: 17.0, Epsilon: 0.8475428503023453\n",
            "Episode: 33, Total reward: 16.0, Epsilon: 0.8433051360508336\n",
            "Episode: 34, Total reward: 4.0, Epsilon: 0.8390886103705794\n",
            "Episode: 35, Total reward: 3.0, Epsilon: 0.8348931673187264\n",
            "Episode: 36, Total reward: 9.0, Epsilon: 0.8307187014821328\n",
            "Episode: 37, Total reward: 21.0, Epsilon: 0.8265651079747222\n",
            "Episode: 38, Total reward: 5.0, Epsilon: 0.8224322824348486\n",
            "Episode: 39, Total reward: 40.0, Epsilon: 0.8183201210226743\n",
            "Episode: 40, Total reward: 57.0, Epsilon: 0.8142285204175609\n",
            "Episode: 41, Total reward: 21.0, Epsilon: 0.810157377815473\n",
            "Episode: 42, Total reward: 2.0, Epsilon: 0.8061065909263957\n",
            "Episode: 43, Total reward: 21.0, Epsilon: 0.8020760579717637\n",
            "Episode: 44, Total reward: 21.0, Epsilon: 0.798065677681905\n",
            "Episode: 45, Total reward: 22.0, Epsilon: 0.7940753492934954\n",
            "Episode: 46, Total reward: 39.0, Epsilon: 0.7901049725470279\n",
            "Episode: 47, Total reward: 14.0, Epsilon: 0.7861544476842928\n",
            "Episode: 48, Total reward: 30.0, Epsilon: 0.7822236754458713\n",
            "Episode: 49, Total reward: 10.0, Epsilon: 0.778312557068642\n",
            "Episode: 50, Total reward: 21.0, Epsilon: 0.7744209942832988\n",
            "Episode: 51, Total reward: 29.0, Epsilon: 0.7705488893118823\n",
            "Episode: 52, Total reward: 27.0, Epsilon: 0.7666961448653229\n",
            "Episode: 53, Total reward: 32.0, Epsilon: 0.7628626641409962\n",
            "Episode: 54, Total reward: 54.0, Epsilon: 0.7590483508202912\n",
            "Episode: 55, Total reward: 11.0, Epsilon: 0.7552531090661897\n",
            "Episode: 56, Total reward: 15.0, Epsilon: 0.7514768435208588\n",
            "Episode: 57, Total reward: 36.0, Epsilon: 0.7477194593032545\n",
            "Episode: 58, Total reward: 11.0, Epsilon: 0.7439808620067382\n",
            "Episode: 59, Total reward: 3.0, Epsilon: 0.7402609576967045\n",
            "Episode: 60, Total reward: 3.0, Epsilon: 0.736559652908221\n",
            "Episode: 61, Total reward: 1.0, Epsilon: 0.7328768546436799\n",
            "Episode: 62, Total reward: 62.0, Epsilon: 0.7292124703704616\n",
            "Episode: 63, Total reward: 41.0, Epsilon: 0.7255664080186093\n",
            "Episode: 64, Total reward: 23.0, Epsilon: 0.7219385759785162\n",
            "Episode: 65, Total reward: 30.0, Epsilon: 0.7183288830986236\n",
            "Episode: 66, Total reward: 37.0, Epsilon: 0.7147372386831305\n",
            "Episode: 67, Total reward: 49.0, Epsilon: 0.7111635524897149\n",
            "Episode: 68, Total reward: 7.0, Epsilon: 0.7076077347272662\n",
            "Episode: 69, Total reward: 6.0, Epsilon: 0.7040696960536299\n",
            "Episode: 70, Total reward: 3.0, Epsilon: 0.7005493475733617\n",
            "Episode: 71, Total reward: 21.0, Epsilon: 0.697046600835495\n",
            "Episode: 72, Total reward: -1.0, Epsilon: 0.6935613678313175\n",
            "Episode: 73, Total reward: 90.0, Epsilon: 0.6900935609921609\n",
            "Episode: 74, Total reward: 22.0, Epsilon: 0.6866430931872001\n",
            "Episode: 75, Total reward: 17.0, Epsilon: 0.6832098777212641\n",
            "Episode: 76, Total reward: 13.0, Epsilon: 0.6797938283326578\n",
            "Episode: 77, Total reward: 57.0, Epsilon: 0.6763948591909945\n",
            "Episode: 78, Total reward: 13.0, Epsilon: 0.6730128848950395\n",
            "Episode: 79, Total reward: 63.0, Epsilon: 0.6696478204705644\n",
            "Episode: 80, Total reward: 41.0, Epsilon: 0.6662995813682115\n",
            "Episode: 81, Total reward: 108.0, Epsilon: 0.6629680834613705\n",
            "Episode: 82, Total reward: 16.0, Epsilon: 0.6596532430440636\n",
            "Episode: 83, Total reward: 47.0, Epsilon: 0.6563549768288433\n",
            "Episode: 84, Total reward: 19.0, Epsilon: 0.653073201944699\n",
            "Episode: 85, Total reward: 15.0, Epsilon: 0.6498078359349755\n",
            "Episode: 86, Total reward: 36.0, Epsilon: 0.6465587967553006\n",
            "Episode: 87, Total reward: 20.0, Epsilon: 0.6433260027715241\n",
            "Episode: 88, Total reward: 57.0, Epsilon: 0.6401093727576664\n",
            "Episode: 89, Total reward: 20.0, Epsilon: 0.6369088258938781\n",
            "Episode: 90, Total reward: 47.0, Epsilon: 0.6337242817644086\n",
            "Episode: 91, Total reward: 5.0, Epsilon: 0.6305556603555866\n",
            "Episode: 92, Total reward: 69.0, Epsilon: 0.6274028820538087\n",
            "Episode: 93, Total reward: 11.0, Epsilon: 0.6242658676435396\n",
            "Episode: 94, Total reward: 123.0, Epsilon: 0.6211445383053219\n",
            "Episode: 95, Total reward: 106.0, Epsilon: 0.6180388156137953\n",
            "Episode: 96, Total reward: 79.0, Epsilon: 0.6149486215357263\n",
            "Episode: 97, Total reward: 36.0, Epsilon: 0.6118738784280476\n",
            "Episode: 98, Total reward: 5.0, Epsilon: 0.6088145090359074\n",
            "Episode: 99, Total reward: 148.0, Epsilon: 0.6057704364907278\n",
            "Episode: 100, Total reward: 54.0, Epsilon: 0.6027415843082742\n",
            "Episode: 101, Total reward: 9.0, Epsilon: 0.5997278763867329\n",
            "Episode: 102, Total reward: 88.0, Epsilon: 0.5967292370047992\n",
            "Episode: 103, Total reward: 90.0, Epsilon: 0.5937455908197752\n",
            "Episode: 104, Total reward: 4.0, Epsilon: 0.5907768628656763\n",
            "Episode: 105, Total reward: 57.0, Epsilon: 0.5878229785513479\n",
            "Episode: 106, Total reward: 59.0, Epsilon: 0.5848838636585911\n",
            "Episode: 107, Total reward: 29.0, Epsilon: 0.5819594443402982\n",
            "Episode: 108, Total reward: 35.0, Epsilon: 0.5790496471185967\n",
            "Episode: 109, Total reward: 74.0, Epsilon: 0.5761543988830038\n",
            "Episode: 110, Total reward: 135.0, Epsilon: 0.5732736268885887\n",
            "Episode: 111, Total reward: 6.0, Epsilon: 0.5704072587541458\n",
            "Episode: 112, Total reward: 40.0, Epsilon: 0.567555222460375\n",
            "Episode: 113, Total reward: 76.0, Epsilon: 0.5647174463480732\n",
            "Episode: 114, Total reward: 115.0, Epsilon: 0.5618938591163328\n",
            "Episode: 115, Total reward: 120.0, Epsilon: 0.5590843898207511\n",
            "Episode: 116, Total reward: 73.0, Epsilon: 0.5562889678716474\n",
            "Episode: 117, Total reward: 130.0, Epsilon: 0.5535075230322891\n",
            "Episode: 118, Total reward: 121.0, Epsilon: 0.5507399854171277\n",
            "Episode: 119, Total reward: 46.0, Epsilon: 0.547986285490042\n",
            "Episode: 120, Total reward: 136.0, Epsilon: 0.5452463540625918\n",
            "Episode: 121, Total reward: 12.0, Epsilon: 0.5425201222922789\n",
            "Episode: 122, Total reward: 18.0, Epsilon: 0.5398075216808175\n",
            "Episode: 123, Total reward: 46.0, Epsilon: 0.5371084840724134\n",
            "Episode: 124, Total reward: 120.0, Epsilon: 0.5344229416520513\n",
            "Episode: 125, Total reward: 193.0, Epsilon: 0.531750826943791\n",
            "Episode: 126, Total reward: 55.0, Epsilon: 0.5290920728090721\n",
            "Episode: 127, Total reward: 52.0, Epsilon: 0.5264466124450268\n",
            "Episode: 128, Total reward: 138.0, Epsilon: 0.5238143793828016\n",
            "Episode: 129, Total reward: 5.0, Epsilon: 0.5211953074858876\n",
            "Episode: 130, Total reward: 131.0, Epsilon: 0.5185893309484582\n",
            "Episode: 131, Total reward: 89.0, Epsilon: 0.5159963842937159\n",
            "Episode: 132, Total reward: 132.0, Epsilon: 0.5134164023722473\n",
            "Episode: 133, Total reward: 94.0, Epsilon: 0.510849320360386\n",
            "Episode: 134, Total reward: 78.0, Epsilon: 0.5082950737585841\n",
            "Episode: 135, Total reward: 229.0, Epsilon: 0.5057535983897912\n",
            "Episode: 136, Total reward: 16.0, Epsilon: 0.5032248303978422\n",
            "Episode: 137, Total reward: 7.0, Epsilon: 0.500708706245853\n",
            "Episode: 138, Total reward: 64.0, Epsilon: 0.4982051627146237\n",
            "Episode: 139, Total reward: 5.0, Epsilon: 0.49571413690105054\n",
            "Episode: 140, Total reward: 99.0, Epsilon: 0.4932355662165453\n",
            "Episode: 141, Total reward: 48.0, Epsilon: 0.4907693883854626\n",
            "Episode: 142, Total reward: 55.0, Epsilon: 0.4883155414435353\n",
            "Episode: 143, Total reward: 0.0, Epsilon: 0.4858739637363176\n",
            "Episode: 144, Total reward: 135.0, Epsilon: 0.483444593917636\n",
            "Episode: 145, Total reward: 55.0, Epsilon: 0.4810273709480478\n",
            "Episode: 146, Total reward: 96.0, Epsilon: 0.47862223409330756\n",
            "Episode: 147, Total reward: 105.0, Epsilon: 0.47622912292284103\n",
            "Episode: 148, Total reward: 11.0, Epsilon: 0.4738479773082268\n",
            "Episode: 149, Total reward: 58.0, Epsilon: 0.47147873742168567\n",
            "Episode: 150, Total reward: 62.0, Epsilon: 0.46912134373457726\n",
            "Episode: 151, Total reward: 102.0, Epsilon: 0.46677573701590436\n",
            "Episode: 152, Total reward: 110.0, Epsilon: 0.46444185833082485\n",
            "Episode: 153, Total reward: 161.0, Epsilon: 0.46211964903917074\n",
            "Episode: 154, Total reward: 14.0, Epsilon: 0.4598090507939749\n",
            "Episode: 155, Total reward: 136.0, Epsilon: 0.457510005540005\n",
            "Episode: 156, Total reward: 26.0, Epsilon: 0.45522245551230495\n",
            "Episode: 157, Total reward: 131.0, Epsilon: 0.4529463432347434\n",
            "Episode: 158, Total reward: 135.0, Epsilon: 0.4506816115185697\n",
            "Episode: 159, Total reward: 138.0, Epsilon: 0.4484282034609769\n",
            "Episode: 160, Total reward: 89.0, Epsilon: 0.446186062443672\n",
            "Episode: 161, Total reward: 3.0, Epsilon: 0.4439551321314536\n",
            "Episode: 162, Total reward: 142.0, Epsilon: 0.4417353564707963\n",
            "Episode: 163, Total reward: 103.0, Epsilon: 0.43952667968844233\n",
            "Episode: 164, Total reward: 22.0, Epsilon: 0.43732904629000013\n",
            "Episode: 165, Total reward: 132.0, Epsilon: 0.4351424010585501\n",
            "Episode: 166, Total reward: 145.0, Epsilon: 0.43296668905325736\n",
            "Episode: 167, Total reward: 35.0, Epsilon: 0.43080185560799106\n",
            "Episode: 168, Total reward: 72.0, Epsilon: 0.4286478463299511\n",
            "Episode: 169, Total reward: 129.0, Epsilon: 0.42650460709830135\n",
            "Episode: 170, Total reward: 6.0, Epsilon: 0.42437208406280985\n",
            "Episode: 171, Total reward: 134.0, Epsilon: 0.4222502236424958\n",
            "Episode: 172, Total reward: 61.0, Epsilon: 0.42013897252428334\n",
            "Episode: 173, Total reward: 136.0, Epsilon: 0.4180382776616619\n",
            "Episode: 174, Total reward: 118.0, Epsilon: 0.4159480862733536\n",
            "Episode: 175, Total reward: 93.0, Epsilon: 0.41386834584198684\n",
            "Episode: 176, Total reward: 103.0, Epsilon: 0.4117990041127769\n",
            "Episode: 177, Total reward: 129.0, Epsilon: 0.40974000909221303\n",
            "Episode: 178, Total reward: 163.0, Epsilon: 0.40769130904675194\n",
            "Episode: 179, Total reward: 37.0, Epsilon: 0.40565285250151817\n",
            "Episode: 180, Total reward: 84.0, Epsilon: 0.4036245882390106\n",
            "Episode: 181, Total reward: 8.0, Epsilon: 0.4016064652978155\n",
            "Episode: 182, Total reward: 128.0, Epsilon: 0.3995984329713264\n",
            "Episode: 183, Total reward: 108.0, Epsilon: 0.3976004408064698\n",
            "Episode: 184, Total reward: 7.0, Epsilon: 0.39561243860243744\n",
            "Episode: 185, Total reward: 106.0, Epsilon: 0.3936343764094253\n",
            "Episode: 186, Total reward: 88.0, Epsilon: 0.39166620452737816\n",
            "Episode: 187, Total reward: -1.0, Epsilon: 0.3897078735047413\n",
            "Episode: 188, Total reward: 110.0, Epsilon: 0.3877593341372176\n",
            "Episode: 189, Total reward: 121.0, Epsilon: 0.3858205374665315\n",
            "Episode: 190, Total reward: 62.0, Epsilon: 0.38389143477919885\n",
            "Episode: 191, Total reward: 41.0, Epsilon: 0.3819719776053028\n",
            "Episode: 192, Total reward: 117.0, Epsilon: 0.3800621177172763\n",
            "Episode: 193, Total reward: 14.0, Epsilon: 0.37816180712868996\n",
            "Episode: 194, Total reward: 65.0, Epsilon: 0.37627099809304654\n",
            "Episode: 195, Total reward: 44.0, Epsilon: 0.3743896431025813\n",
            "Episode: 196, Total reward: -1.0, Epsilon: 0.37251769488706843\n",
            "Episode: 197, Total reward: 125.0, Epsilon: 0.3706551064126331\n",
            "Episode: 198, Total reward: 107.0, Epsilon: 0.36880183088056995\n",
            "Episode: 199, Total reward: 106.0, Epsilon: 0.3669578217261671\n",
            "Episode: 200, Total reward: 61.0, Epsilon: 0.36512303261753626\n",
            "Episode: 201, Total reward: 110.0, Epsilon: 0.3632974174544486\n",
            "Episode: 202, Total reward: 110.0, Epsilon: 0.3614809303671764\n",
            "Episode: 203, Total reward: 3.0, Epsilon: 0.3596735257153405\n",
            "Episode: 204, Total reward: 54.0, Epsilon: 0.3578751580867638\n",
            "Episode: 205, Total reward: 92.0, Epsilon: 0.35608578229633\n",
            "Episode: 206, Total reward: 106.0, Epsilon: 0.3543053533848483\n",
            "Episode: 207, Total reward: 110.0, Epsilon: 0.35253382661792404\n",
            "Episode: 208, Total reward: 45.0, Epsilon: 0.3507711574848344\n",
            "Episode: 209, Total reward: 120.0, Epsilon: 0.34901730169741024\n",
            "Episode: 210, Total reward: 2.0, Epsilon: 0.3472722151889232\n",
            "Episode: 211, Total reward: 121.0, Epsilon: 0.3455358541129786\n",
            "Episode: 212, Total reward: 96.0, Epsilon: 0.3438081748424137\n",
            "Episode: 213, Total reward: 69.0, Epsilon: 0.3420891339682016\n",
            "Episode: 214, Total reward: 101.0, Epsilon: 0.3403786882983606\n",
            "Episode: 215, Total reward: 47.0, Epsilon: 0.3386767948568688\n",
            "Episode: 216, Total reward: 89.0, Epsilon: 0.33698341088258443\n",
            "Episode: 217, Total reward: 102.0, Epsilon: 0.3352984938281715\n",
            "Episode: 218, Total reward: 0.0, Epsilon: 0.33362200135903064\n",
            "Episode: 219, Total reward: -1.0, Epsilon: 0.33195389135223546\n",
            "Episode: 220, Total reward: 99.0, Epsilon: 0.3302941218954743\n",
            "Episode: 221, Total reward: 15.0, Epsilon: 0.32864265128599696\n",
            "Episode: 222, Total reward: 103.0, Epsilon: 0.326999438029567\n",
            "Episode: 223, Total reward: 6.0, Epsilon: 0.3253644408394192\n",
            "Episode: 224, Total reward: 98.0, Epsilon: 0.3237376186352221\n",
            "Episode: 225, Total reward: 98.0, Epsilon: 0.322118930542046\n",
            "Episode: 226, Total reward: 91.0, Epsilon: 0.32050833588933575\n",
            "Episode: 227, Total reward: 100.0, Epsilon: 0.31890579420988907\n",
            "Episode: 228, Total reward: 95.0, Epsilon: 0.3173112652388396\n",
            "Episode: 229, Total reward: 104.0, Epsilon: 0.3157247089126454\n",
            "Episode: 230, Total reward: 88.0, Epsilon: 0.3141460853680822\n",
            "Episode: 231, Total reward: 96.0, Epsilon: 0.3125753549412418\n",
            "Episode: 232, Total reward: 60.0, Epsilon: 0.31101247816653554\n",
            "Episode: 233, Total reward: 93.0, Epsilon: 0.30945741577570285\n",
            "Episode: 234, Total reward: 88.0, Epsilon: 0.3079101286968243\n",
            "Episode: 235, Total reward: 54.0, Epsilon: 0.3063705780533402\n",
            "Episode: 236, Total reward: 54.0, Epsilon: 0.30483872516307353\n",
            "Episode: 237, Total reward: 19.0, Epsilon: 0.3033145315372582\n",
            "Episode: 238, Total reward: 87.0, Epsilon: 0.3017979588795719\n",
            "Episode: 239, Total reward: 88.0, Epsilon: 0.30028896908517405\n",
            "Episode: 240, Total reward: 89.0, Epsilon: 0.2987875242397482\n",
            "Episode: 241, Total reward: 2.0, Epsilon: 0.29729358661854943\n",
            "Episode: 242, Total reward: 85.0, Epsilon: 0.29580711868545667\n",
            "Episode: 243, Total reward: 84.0, Epsilon: 0.2943280830920294\n",
            "Episode: 244, Total reward: 78.0, Epsilon: 0.29285644267656924\n",
            "Episode: 245, Total reward: 34.0, Epsilon: 0.2913921604631864\n",
            "Episode: 246, Total reward: 6.0, Epsilon: 0.28993519966087045\n",
            "Episode: 247, Total reward: 85.0, Epsilon: 0.2884855236625661\n",
            "Episode: 248, Total reward: 21.0, Epsilon: 0.28704309604425327\n",
            "Episode: 249, Total reward: 84.0, Epsilon: 0.285607880564032\n",
            "Episode: 250, Total reward: 88.0, Epsilon: 0.28417984116121187\n",
            "Episode: 251, Total reward: 14.0, Epsilon: 0.2827589419554058\n",
            "Episode: 252, Total reward: 8.0, Epsilon: 0.28134514724562876\n",
            "Episode: 253, Total reward: 87.0, Epsilon: 0.2799384215094006\n",
            "Episode: 254, Total reward: 81.0, Epsilon: 0.27853872940185365\n",
            "Episode: 255, Total reward: 88.0, Epsilon: 0.27714603575484437\n",
            "Episode: 256, Total reward: 88.0, Epsilon: 0.2757603055760701\n",
            "Episode: 257, Total reward: 4.0, Epsilon: 0.2743815040481898\n",
            "Episode: 258, Total reward: 31.0, Epsilon: 0.2730095965279488\n",
            "Episode: 259, Total reward: 8.0, Epsilon: 0.27164454854530906\n",
            "Episode: 260, Total reward: 10.0, Epsilon: 0.2702863258025825\n",
            "Episode: 261, Total reward: 69.0, Epsilon: 0.2689348941735696\n",
            "Episode: 262, Total reward: 21.0, Epsilon: 0.26759021970270175\n",
            "Episode: 263, Total reward: 92.0, Epsilon: 0.2662522686041882\n",
            "Episode: 264, Total reward: 5.0, Epsilon: 0.2649210072611673\n",
            "Episode: 265, Total reward: 64.0, Epsilon: 0.26359640222486147\n",
            "Episode: 266, Total reward: 86.0, Epsilon: 0.26227842021373715\n",
            "Episode: 267, Total reward: 6.0, Epsilon: 0.2609670281126685\n",
            "Episode: 268, Total reward: 87.0, Epsilon: 0.25966219297210513\n",
            "Episode: 269, Total reward: 94.0, Epsilon: 0.2583638820072446\n",
            "Episode: 270, Total reward: 88.0, Epsilon: 0.2570720625972084\n",
            "Episode: 271, Total reward: 91.0, Epsilon: 0.25578670228422234\n",
            "Episode: 272, Total reward: 87.0, Epsilon: 0.25450776877280124\n",
            "Episode: 273, Total reward: 13.0, Epsilon: 0.2532352299289372\n",
            "Episode: 274, Total reward: 88.0, Epsilon: 0.2519690537792925\n",
            "Episode: 275, Total reward: 19.0, Epsilon: 0.2507092085103961\n",
            "Episode: 276, Total reward: 29.0, Epsilon: 0.2494556624678441\n",
            "Episode: 277, Total reward: 89.0, Epsilon: 0.24820838415550486\n",
            "Episode: 278, Total reward: 85.0, Epsilon: 0.24696734223472733\n",
            "Episode: 279, Total reward: 26.0, Epsilon: 0.2457325055235537\n",
            "Episode: 280, Total reward: 99.0, Epsilon: 0.24450384299593592\n",
            "Episode: 281, Total reward: 86.0, Epsilon: 0.24328132378095624\n",
            "Episode: 282, Total reward: 42.0, Epsilon: 0.24206491716205145\n",
            "Episode: 283, Total reward: 66.0, Epsilon: 0.2408545925762412\n",
            "Episode: 284, Total reward: 87.0, Epsilon: 0.23965031961336\n",
            "Episode: 285, Total reward: 91.0, Epsilon: 0.2384520680152932\n",
            "Episode: 286, Total reward: 42.0, Epsilon: 0.23725980767521673\n",
            "Episode: 287, Total reward: 22.0, Epsilon: 0.23607350863684065\n",
            "Episode: 288, Total reward: 42.0, Epsilon: 0.23489314109365644\n",
            "Episode: 289, Total reward: 35.0, Epsilon: 0.23371867538818816\n",
            "Episode: 290, Total reward: 92.0, Epsilon: 0.23255008201124722\n",
            "Episode: 291, Total reward: 6.0, Epsilon: 0.231387331601191\n",
            "Episode: 292, Total reward: 42.0, Epsilon: 0.23023039494318503\n",
            "Episode: 293, Total reward: 3.0, Epsilon: 0.2290792429684691\n",
            "Episode: 294, Total reward: 18.0, Epsilon: 0.22793384675362674\n",
            "Episode: 295, Total reward: 26.0, Epsilon: 0.22679417751985861\n",
            "Episode: 296, Total reward: 34.0, Epsilon: 0.22566020663225933\n",
            "Episode: 297, Total reward: 12.0, Epsilon: 0.22453190559909803\n",
            "Episode: 298, Total reward: 21.0, Epsilon: 0.22340924607110255\n",
            "Episode: 299, Total reward: 42.0, Epsilon: 0.22229219984074702\n",
            "Episode: 300, Total reward: 34.0, Epsilon: 0.2211807388415433\n",
            "Episode: 301, Total reward: 95.0, Epsilon: 0.22007483514733558\n",
            "Episode: 302, Total reward: 87.0, Epsilon: 0.2189744609715989\n",
            "Episode: 303, Total reward: 104.0, Epsilon: 0.2178795886667409\n",
            "Episode: 304, Total reward: 96.0, Epsilon: 0.2167901907234072\n",
            "Episode: 305, Total reward: 13.0, Epsilon: 0.21570623976979014\n",
            "Episode: 306, Total reward: 102.0, Epsilon: 0.21462770857094118\n",
            "Episode: 307, Total reward: 4.0, Epsilon: 0.21355457002808648\n",
            "Episode: 308, Total reward: 130.0, Epsilon: 0.21248679717794605\n",
            "Episode: 309, Total reward: 114.0, Epsilon: 0.21142436319205632\n",
            "Episode: 310, Total reward: 1.0, Epsilon: 0.21036724137609603\n",
            "Episode: 311, Total reward: 9.0, Epsilon: 0.20931540516921554\n",
            "Episode: 312, Total reward: -1.0, Epsilon: 0.20826882814336947\n",
            "Episode: 313, Total reward: 91.0, Epsilon: 0.20722748400265262\n",
            "Episode: 314, Total reward: 126.0, Epsilon: 0.20619134658263935\n",
            "Episode: 315, Total reward: 195.0, Epsilon: 0.20516038984972615\n",
            "Episode: 316, Total reward: 33.0, Epsilon: 0.2041345879004775\n",
            "Episode: 317, Total reward: 150.0, Epsilon: 0.2031139149609751\n",
            "Episode: 318, Total reward: 104.0, Epsilon: 0.20209834538617025\n",
            "Episode: 319, Total reward: 119.0, Epsilon: 0.2010878536592394\n",
            "Episode: 320, Total reward: 103.0, Epsilon: 0.2000824143909432\n",
            "Episode: 321, Total reward: 27.0, Epsilon: 0.19908200231898848\n",
            "Episode: 322, Total reward: 16.0, Epsilon: 0.19808659230739353\n",
            "Episode: 323, Total reward: 89.0, Epsilon: 0.19709615934585656\n",
            "Episode: 324, Total reward: 29.0, Epsilon: 0.19611067854912728\n",
            "Episode: 325, Total reward: 101.0, Epsilon: 0.19513012515638165\n",
            "Episode: 326, Total reward: 109.0, Epsilon: 0.19415447453059972\n",
            "Episode: 327, Total reward: 103.0, Epsilon: 0.19318370215794672\n",
            "Episode: 328, Total reward: 111.0, Epsilon: 0.192217783647157\n",
            "Episode: 329, Total reward: 107.0, Epsilon: 0.1912566947289212\n",
            "Episode: 330, Total reward: 42.0, Epsilon: 0.1903004112552766\n",
            "Episode: 331, Total reward: 338.0, Epsilon: 0.18934890919900021\n",
            "Episode: 332, Total reward: 138.0, Epsilon: 0.18840216465300522\n",
            "Episode: 333, Total reward: 231.0, Epsilon: 0.18746015382974018\n",
            "Episode: 334, Total reward: 130.0, Epsilon: 0.1865228530605915\n",
            "Episode: 335, Total reward: 125.0, Epsilon: 0.18559023879528855\n",
            "Episode: 336, Total reward: 114.0, Epsilon: 0.1846622876013121\n",
            "Episode: 337, Total reward: 149.0, Epsilon: 0.18373897616330553\n",
            "Episode: 338, Total reward: 119.0, Epsilon: 0.182820281282489\n",
            "Episode: 339, Total reward: 131.0, Epsilon: 0.18190617987607657\n",
            "Episode: 340, Total reward: 128.0, Epsilon: 0.18099664897669618\n",
            "Episode: 341, Total reward: 125.0, Epsilon: 0.1800916657318127\n",
            "Episode: 342, Total reward: 213.0, Epsilon: 0.17919120740315364\n",
            "Episode: 343, Total reward: 136.0, Epsilon: 0.17829525136613786\n",
            "Episode: 344, Total reward: 154.0, Epsilon: 0.17740377510930716\n",
            "Episode: 345, Total reward: 156.0, Epsilon: 0.17651675623376062\n",
            "Episode: 346, Total reward: 141.0, Epsilon: 0.1756341724525918\n",
            "Episode: 347, Total reward: 139.0, Epsilon: 0.17475600159032884\n",
            "Episode: 348, Total reward: 129.0, Epsilon: 0.17388222158237718\n",
            "Episode: 349, Total reward: 119.0, Epsilon: 0.1730128104744653\n",
            "Episode: 350, Total reward: 115.0, Epsilon: 0.17214774642209296\n",
            "Episode: 351, Total reward: 275.0, Epsilon: 0.1712870076899825\n",
            "Episode: 352, Total reward: 99.0, Epsilon: 0.17043057265153258\n",
            "Episode: 353, Total reward: 157.0, Epsilon: 0.16957841978827493\n",
            "Episode: 354, Total reward: 26.0, Epsilon: 0.16873052768933355\n",
            "Episode: 355, Total reward: 4.0, Epsilon: 0.1678868750508869\n",
            "Episode: 356, Total reward: 58.0, Epsilon: 0.16704744067563246\n",
            "Episode: 357, Total reward: 135.0, Epsilon: 0.1662122034722543\n",
            "Episode: 358, Total reward: 147.0, Epsilon: 0.16538114245489302\n",
            "Episode: 359, Total reward: 163.0, Epsilon: 0.16455423674261854\n",
            "Episode: 360, Total reward: 140.0, Epsilon: 0.16373146555890544\n",
            "Episode: 361, Total reward: 169.0, Epsilon: 0.16291280823111093\n",
            "Episode: 362, Total reward: 104.0, Epsilon: 0.16209824418995536\n",
            "Episode: 363, Total reward: 139.0, Epsilon: 0.16128775296900558\n",
            "Episode: 364, Total reward: 131.0, Epsilon: 0.16048131420416054\n",
            "Episode: 365, Total reward: 135.0, Epsilon: 0.15967890763313974\n",
            "Episode: 366, Total reward: 132.0, Epsilon: 0.15888051309497406\n",
            "Episode: 367, Total reward: 114.0, Epsilon: 0.1580861105294992\n",
            "Episode: 368, Total reward: 145.0, Epsilon: 0.1572956799768517\n",
            "Episode: 369, Total reward: 113.0, Epsilon: 0.15650920157696743\n",
            "Episode: 370, Total reward: 140.0, Epsilon: 0.1557266555690826\n",
            "Episode: 371, Total reward: 134.0, Epsilon: 0.1549480222912372\n",
            "Episode: 372, Total reward: 163.0, Epsilon: 0.15417328217978102\n",
            "Episode: 373, Total reward: 215.0, Epsilon: 0.1534024157688821\n",
            "Episode: 374, Total reward: 208.0, Epsilon: 0.1526354036900377\n",
            "Episode: 375, Total reward: 160.0, Epsilon: 0.1518722266715875\n",
            "Episode: 376, Total reward: 171.0, Epsilon: 0.15111286553822956\n",
            "Episode: 377, Total reward: 157.0, Epsilon: 0.15035730121053842\n",
            "Episode: 378, Total reward: 175.0, Epsilon: 0.14960551470448571\n",
            "Episode: 379, Total reward: 117.0, Epsilon: 0.14885748713096328\n",
            "Episode: 380, Total reward: 173.0, Epsilon: 0.14811319969530845\n",
            "Episode: 381, Total reward: 316.0, Epsilon: 0.1473726336968319\n",
            "Episode: 382, Total reward: 236.0, Epsilon: 0.14663577052834775\n",
            "Episode: 383, Total reward: 275.0, Epsilon: 0.14590259167570602\n",
            "Episode: 384, Total reward: 205.0, Epsilon: 0.1451730787173275\n",
            "Episode: 385, Total reward: 188.0, Epsilon: 0.14444721332374086\n",
            "Episode: 386, Total reward: 239.0, Epsilon: 0.14372497725712216\n",
            "Episode: 387, Total reward: 189.0, Epsilon: 0.14300635237083656\n",
            "Episode: 388, Total reward: 266.0, Epsilon: 0.14229132060898236\n",
            "Episode: 389, Total reward: 197.0, Epsilon: 0.14157986400593744\n",
            "Episode: 390, Total reward: 181.0, Epsilon: 0.14087196468590776\n",
            "Episode: 391, Total reward: 262.0, Epsilon: 0.14016760486247823\n",
            "Episode: 392, Total reward: 60.0, Epsilon: 0.13946676683816583\n",
            "Episode: 393, Total reward: 50.0, Epsilon: 0.138769433003975\n",
            "Episode: 394, Total reward: 200.0, Epsilon: 0.13807558583895513\n",
            "Episode: 395, Total reward: 213.0, Epsilon: 0.13738520790976036\n",
            "Episode: 396, Total reward: 233.0, Epsilon: 0.13669828187021155\n",
            "Episode: 397, Total reward: 489.0, Epsilon: 0.13601479046086049\n",
            "Episode: 398, Total reward: 80.0, Epsilon: 0.1353347165085562\n",
            "Episode: 399, Total reward: 127.0, Epsilon: 0.1346580429260134\n",
            "Episode: 400, Total reward: 0.0, Epsilon: 0.13398475271138335\n",
            "Episode: 401, Total reward: 9.0, Epsilon: 0.13331482894782642\n",
            "Episode: 402, Total reward: 79.0, Epsilon: 0.13264825480308728\n",
            "Episode: 403, Total reward: 15.0, Epsilon: 0.13198501352907185\n",
            "Episode: 404, Total reward: -1.0, Epsilon: 0.1313250884614265\n",
            "Episode: 405, Total reward: 31.0, Epsilon: 0.13066846301911936\n",
            "Episode: 406, Total reward: -2.0, Epsilon: 0.13001512070402377\n",
            "Episode: 407, Total reward: 23.0, Epsilon: 0.12936504510050365\n",
            "Episode: 408, Total reward: 6.0, Epsilon: 0.12871821987500112\n",
            "Episode: 409, Total reward: -1.0, Epsilon: 0.12807462877562611\n",
            "Episode: 410, Total reward: -1.0, Epsilon: 0.12743425563174798\n",
            "Episode: 411, Total reward: 13.0, Epsilon: 0.12679708435358925\n",
            "Episode: 412, Total reward: 1.0, Epsilon: 0.1261630989318213\n",
            "Episode: 413, Total reward: 22.0, Epsilon: 0.1255322834371622\n",
            "Episode: 414, Total reward: 0.0, Epsilon: 0.12490462201997637\n",
            "Episode: 415, Total reward: -1.0, Epsilon: 0.1242800989098765\n",
            "Episode: 416, Total reward: -1.0, Epsilon: 0.12365869841532712\n",
            "Episode: 417, Total reward: -1.0, Epsilon: 0.12304040492325048\n",
            "Episode: 418, Total reward: -2.0, Epsilon: 0.12242520289863423\n",
            "Episode: 419, Total reward: 14.0, Epsilon: 0.12181307688414106\n",
            "Episode: 420, Total reward: 3.0, Epsilon: 0.12120401149972035\n",
            "Episode: 421, Total reward: 451.0, Epsilon: 0.12059799144222175\n",
            "Episode: 422, Total reward: 275.0, Epsilon: 0.11999500148501063\n",
            "Episode: 423, Total reward: 226.0, Epsilon: 0.11939502647758558\n",
            "Episode: 424, Total reward: 242.0, Epsilon: 0.11879805134519765\n",
            "Episode: 425, Total reward: 364.0, Epsilon: 0.11820406108847166\n",
            "Episode: 426, Total reward: 285.0, Epsilon: 0.1176130407830293\n",
            "Episode: 427, Total reward: 210.0, Epsilon: 0.11702497557911415\n"
          ]
        }
      ]
    }
  ]
}