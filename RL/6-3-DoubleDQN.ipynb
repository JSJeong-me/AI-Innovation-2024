{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/PjS/yQsMfH8Oq+XNxHXs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSJeong-me/AI-Innovation-2024/blob/main/RL/6-3-DoubleDQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "# Dueling Q-Network 모델 정의\n",
        "class DuelingQNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(DuelingQNetwork, self).__init__()\n",
        "        # 상태-가치 함수 부분\n",
        "        self.fc1 = nn.Linear(state_size, 128)\n",
        "        self.fc_value = nn.Linear(128, 128)\n",
        "        self.value = nn.Linear(128, 1)\n",
        "\n",
        "        # 이득(advantage) 함수 부분\n",
        "        self.fc_advantage = nn.Linear(128, 128)\n",
        "        self.advantage = nn.Linear(128, action_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.fc1(state))\n",
        "\n",
        "        # 가치 함수\n",
        "        value = torch.relu(self.fc_value(x))\n",
        "        value = self.value(value)\n",
        "\n",
        "        # 이득 함수\n",
        "        advantage = torch.relu(self.fc_advantage(x))\n",
        "        advantage = self.advantage(advantage)\n",
        "\n",
        "        # 최종 Q값: 가치 + (이득 - 평균 이득)\n",
        "        q_value = value + (advantage - advantage.mean())\n",
        "        return q_value\n",
        "\n",
        "# Hyperparameters\n",
        "state_size = 4\n",
        "action_size = 2\n",
        "batch_size = 64\n",
        "gamma = 0.99\n",
        "epsilon = 1.0\n",
        "epsilon_min = 0.01\n",
        "epsilon_decay = 0.995\n",
        "learning_rate = 0.001\n",
        "target_update = 10\n",
        "\n",
        "# Replay Memory\n",
        "memory = deque(maxlen=2000)\n",
        "\n",
        "# Double DQN: 두 개의 네트워크 사용\n",
        "q_network = DuelingQNetwork(state_size, action_size)\n",
        "target_network = DuelingQNetwork(state_size, action_size)\n",
        "target_network.load_state_dict(q_network.state_dict())\n",
        "optimizer = optim.Adam(q_network.parameters(), lr=learning_rate)\n",
        "\n",
        "# 경험 샘플링 함수\n",
        "def replay(memory, batch_size):\n",
        "    if len(memory) < batch_size:\n",
        "        return\n",
        "    minibatch = random.sample(memory, batch_size)\n",
        "    states, actions, rewards, next_states, dones = zip(*minibatch)\n",
        "\n",
        "    states = torch.tensor(states, dtype=torch.float32)\n",
        "    actions = torch.tensor(actions, dtype=torch.long)\n",
        "    rewards = torch.tensor(rewards, dtype=torch.float32)\n",
        "    next_states = torch.tensor(next_states, dtype=torch.float32)\n",
        "    dones = torch.tensor(dones, dtype=torch.float32)\n",
        "\n",
        "    # 현재 상태에서의 Q값 계산\n",
        "    q_values = q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "    # Double Q-Learning: 행동 선택은 q_network로, Q값 계산은 target_network로\n",
        "    next_actions = q_network(next_states).max(1)[1]\n",
        "    next_q_values = target_network(next_states).gather(1, next_actions.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "    expected_q_values = rewards + (gamma * next_q_values * (1 - dones))\n",
        "\n",
        "    # 손실 계산 및 역전파\n",
        "    loss = nn.MSELoss()(q_values, expected_q_values.detach())\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# 행동 선택 함수 (ε-greedy)\n",
        "def choose_action(state, epsilon):\n",
        "    if np.random.rand() <= epsilon:\n",
        "        return random.randrange(action_size)\n",
        "    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        q_values = q_network(state)\n",
        "    return np.argmax(q_values.numpy())\n",
        "\n",
        "# CartPole 환경 설정\n",
        "env = gym.make('CartPole-v1')\n",
        "episodes = 1000\n",
        "\n",
        "# 학습 루프\n",
        "for episode in range(episodes):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # 행동 선택\n",
        "        action = choose_action(state, epsilon)\n",
        "\n",
        "        # 환경에서 한 단계 진행\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        # 보상 조정\n",
        "        reward = reward if not done else -10\n",
        "\n",
        "        # 메모리에 저장\n",
        "        memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "        # 상태 업데이트\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        # 경험 리플레이\n",
        "        replay(memory, batch_size)\n",
        "\n",
        "    # 탐험률 감소\n",
        "    if epsilon > epsilon_min:\n",
        "        epsilon *= epsilon_decay\n",
        "\n",
        "    # 타겟 네트워크 업데이트\n",
        "    if episode % target_update == 0:\n",
        "        target_network.load_state_dict(q_network.state_dict())\n",
        "\n",
        "    print(f\"Episode: {episode}, Total reward: {total_reward}, Epsilon: {epsilon}\")\n",
        "\n",
        "# 학습 완료 후 에이전트 테스트\n",
        "for i in range(10):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        env.render()\n",
        "        action = choose_action(state, epsilon_min)\n",
        "        next_state, _, done, _ = env.step(action)\n",
        "        state = next_state\n",
        "env.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PU_tdBWsj_gk",
        "outputId": "8cde9f69-eb09-4c89-c32a-9dc357e5d892"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n",
            "<ipython-input-5-6cb7083c972b>:64: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  states = torch.tensor(states, dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 0, Total reward: 8.0, Epsilon: 0.995\n",
            "Episode: 1, Total reward: 13.0, Epsilon: 0.990025\n",
            "Episode: 2, Total reward: 19.0, Epsilon: 0.985074875\n",
            "Episode: 3, Total reward: 7.0, Epsilon: 0.9801495006250001\n",
            "Episode: 4, Total reward: 29.0, Epsilon: 0.9752487531218751\n",
            "Episode: 5, Total reward: 6.0, Epsilon: 0.9703725093562657\n",
            "Episode: 6, Total reward: 64.0, Epsilon: 0.9655206468094844\n",
            "Episode: 7, Total reward: 13.0, Epsilon: 0.960693043575437\n",
            "Episode: 8, Total reward: 8.0, Epsilon: 0.9558895783575597\n",
            "Episode: 9, Total reward: 16.0, Epsilon: 0.9511101304657719\n",
            "Episode: 10, Total reward: 11.0, Epsilon: 0.946354579813443\n",
            "Episode: 11, Total reward: 5.0, Epsilon: 0.9416228069143757\n",
            "Episode: 12, Total reward: 9.0, Epsilon: 0.9369146928798039\n",
            "Episode: 13, Total reward: 0.0, Epsilon: 0.9322301194154049\n",
            "Episode: 14, Total reward: 3.0, Epsilon: 0.9275689688183278\n",
            "Episode: 15, Total reward: 28.0, Epsilon: 0.9229311239742362\n",
            "Episode: 16, Total reward: -1.0, Epsilon: 0.918316468354365\n",
            "Episode: 17, Total reward: 4.0, Epsilon: 0.9137248860125932\n",
            "Episode: 18, Total reward: 16.0, Epsilon: 0.9091562615825302\n",
            "Episode: 19, Total reward: 9.0, Epsilon: 0.9046104802746175\n",
            "Episode: 20, Total reward: 32.0, Epsilon: 0.9000874278732445\n",
            "Episode: 21, Total reward: 1.0, Epsilon: 0.8955869907338783\n",
            "Episode: 22, Total reward: 25.0, Epsilon: 0.8911090557802088\n",
            "Episode: 23, Total reward: 3.0, Epsilon: 0.8866535105013078\n",
            "Episode: 24, Total reward: 17.0, Epsilon: 0.8822202429488013\n",
            "Episode: 25, Total reward: 59.0, Epsilon: 0.8778091417340573\n",
            "Episode: 26, Total reward: 8.0, Epsilon: 0.8734200960253871\n",
            "Episode: 27, Total reward: 29.0, Epsilon: 0.8690529955452602\n",
            "Episode: 28, Total reward: 1.0, Epsilon: 0.8647077305675338\n",
            "Episode: 29, Total reward: 2.0, Epsilon: 0.8603841919146962\n",
            "Episode: 30, Total reward: 10.0, Epsilon: 0.8560822709551227\n",
            "Episode: 31, Total reward: 8.0, Epsilon: 0.851801859600347\n",
            "Episode: 32, Total reward: 18.0, Epsilon: 0.8475428503023453\n",
            "Episode: 33, Total reward: 9.0, Epsilon: 0.8433051360508336\n",
            "Episode: 34, Total reward: 63.0, Epsilon: 0.8390886103705794\n",
            "Episode: 35, Total reward: 19.0, Epsilon: 0.8348931673187264\n",
            "Episode: 36, Total reward: 26.0, Epsilon: 0.8307187014821328\n",
            "Episode: 37, Total reward: 10.0, Epsilon: 0.8265651079747222\n",
            "Episode: 38, Total reward: 87.0, Epsilon: 0.8224322824348486\n",
            "Episode: 39, Total reward: 39.0, Epsilon: 0.8183201210226743\n",
            "Episode: 40, Total reward: 3.0, Epsilon: 0.8142285204175609\n",
            "Episode: 41, Total reward: 20.0, Epsilon: 0.810157377815473\n",
            "Episode: 42, Total reward: 14.0, Epsilon: 0.8061065909263957\n",
            "Episode: 43, Total reward: 19.0, Epsilon: 0.8020760579717637\n",
            "Episode: 44, Total reward: 18.0, Epsilon: 0.798065677681905\n",
            "Episode: 45, Total reward: 19.0, Epsilon: 0.7940753492934954\n",
            "Episode: 46, Total reward: 12.0, Epsilon: 0.7901049725470279\n",
            "Episode: 47, Total reward: 23.0, Epsilon: 0.7861544476842928\n",
            "Episode: 48, Total reward: 32.0, Epsilon: 0.7822236754458713\n",
            "Episode: 49, Total reward: 10.0, Epsilon: 0.778312557068642\n",
            "Episode: 50, Total reward: 56.0, Epsilon: 0.7744209942832988\n",
            "Episode: 51, Total reward: 16.0, Epsilon: 0.7705488893118823\n",
            "Episode: 52, Total reward: 5.0, Epsilon: 0.7666961448653229\n",
            "Episode: 53, Total reward: 48.0, Epsilon: 0.7628626641409962\n",
            "Episode: 54, Total reward: 14.0, Epsilon: 0.7590483508202912\n",
            "Episode: 55, Total reward: 54.0, Epsilon: 0.7552531090661897\n",
            "Episode: 56, Total reward: 83.0, Epsilon: 0.7514768435208588\n",
            "Episode: 57, Total reward: 2.0, Epsilon: 0.7477194593032545\n",
            "Episode: 58, Total reward: 5.0, Epsilon: 0.7439808620067382\n",
            "Episode: 59, Total reward: 72.0, Epsilon: 0.7402609576967045\n",
            "Episode: 60, Total reward: 29.0, Epsilon: 0.736559652908221\n",
            "Episode: 61, Total reward: 19.0, Epsilon: 0.7328768546436799\n",
            "Episode: 62, Total reward: 51.0, Epsilon: 0.7292124703704616\n",
            "Episode: 63, Total reward: 70.0, Epsilon: 0.7255664080186093\n",
            "Episode: 64, Total reward: 4.0, Epsilon: 0.7219385759785162\n",
            "Episode: 65, Total reward: 27.0, Epsilon: 0.7183288830986236\n",
            "Episode: 66, Total reward: 28.0, Epsilon: 0.7147372386831305\n",
            "Episode: 67, Total reward: 6.0, Epsilon: 0.7111635524897149\n",
            "Episode: 68, Total reward: 101.0, Epsilon: 0.7076077347272662\n",
            "Episode: 69, Total reward: 4.0, Epsilon: 0.7040696960536299\n",
            "Episode: 70, Total reward: 4.0, Epsilon: 0.7005493475733617\n",
            "Episode: 71, Total reward: 73.0, Epsilon: 0.697046600835495\n",
            "Episode: 72, Total reward: 27.0, Epsilon: 0.6935613678313175\n",
            "Episode: 73, Total reward: 52.0, Epsilon: 0.6900935609921609\n",
            "Episode: 74, Total reward: 57.0, Epsilon: 0.6866430931872001\n",
            "Episode: 75, Total reward: 94.0, Epsilon: 0.6832098777212641\n",
            "Episode: 76, Total reward: 21.0, Epsilon: 0.6797938283326578\n",
            "Episode: 77, Total reward: 3.0, Epsilon: 0.6763948591909945\n",
            "Episode: 78, Total reward: 25.0, Epsilon: 0.6730128848950395\n",
            "Episode: 79, Total reward: 42.0, Epsilon: 0.6696478204705644\n",
            "Episode: 80, Total reward: 7.0, Epsilon: 0.6662995813682115\n",
            "Episode: 81, Total reward: 95.0, Epsilon: 0.6629680834613705\n",
            "Episode: 82, Total reward: 90.0, Epsilon: 0.6596532430440636\n",
            "Episode: 83, Total reward: 25.0, Epsilon: 0.6563549768288433\n",
            "Episode: 84, Total reward: 2.0, Epsilon: 0.653073201944699\n",
            "Episode: 85, Total reward: 64.0, Epsilon: 0.6498078359349755\n",
            "Episode: 86, Total reward: 2.0, Epsilon: 0.6465587967553006\n",
            "Episode: 87, Total reward: 10.0, Epsilon: 0.6433260027715241\n",
            "Episode: 88, Total reward: 96.0, Epsilon: 0.6401093727576664\n",
            "Episode: 89, Total reward: 34.0, Epsilon: 0.6369088258938781\n",
            "Episode: 90, Total reward: 15.0, Epsilon: 0.6337242817644086\n",
            "Episode: 91, Total reward: 18.0, Epsilon: 0.6305556603555866\n",
            "Episode: 92, Total reward: 28.0, Epsilon: 0.6274028820538087\n",
            "Episode: 93, Total reward: 3.0, Epsilon: 0.6242658676435396\n",
            "Episode: 94, Total reward: 16.0, Epsilon: 0.6211445383053219\n",
            "Episode: 95, Total reward: 44.0, Epsilon: 0.6180388156137953\n",
            "Episode: 96, Total reward: 57.0, Epsilon: 0.6149486215357263\n",
            "Episode: 97, Total reward: 16.0, Epsilon: 0.6118738784280476\n",
            "Episode: 98, Total reward: 47.0, Epsilon: 0.6088145090359074\n",
            "Episode: 99, Total reward: 190.0, Epsilon: 0.6057704364907278\n",
            "Episode: 100, Total reward: 14.0, Epsilon: 0.6027415843082742\n",
            "Episode: 101, Total reward: 95.0, Epsilon: 0.5997278763867329\n",
            "Episode: 102, Total reward: 42.0, Epsilon: 0.5967292370047992\n",
            "Episode: 103, Total reward: 44.0, Epsilon: 0.5937455908197752\n",
            "Episode: 104, Total reward: 1.0, Epsilon: 0.5907768628656763\n",
            "Episode: 105, Total reward: 23.0, Epsilon: 0.5878229785513479\n",
            "Episode: 106, Total reward: 21.0, Epsilon: 0.5848838636585911\n",
            "Episode: 107, Total reward: 38.0, Epsilon: 0.5819594443402982\n",
            "Episode: 108, Total reward: 38.0, Epsilon: 0.5790496471185967\n",
            "Episode: 109, Total reward: 111.0, Epsilon: 0.5761543988830038\n",
            "Episode: 110, Total reward: 7.0, Epsilon: 0.5732736268885887\n",
            "Episode: 111, Total reward: 82.0, Epsilon: 0.5704072587541458\n",
            "Episode: 112, Total reward: 7.0, Epsilon: 0.567555222460375\n",
            "Episode: 113, Total reward: 53.0, Epsilon: 0.5647174463480732\n",
            "Episode: 114, Total reward: 89.0, Epsilon: 0.5618938591163328\n",
            "Episode: 115, Total reward: 56.0, Epsilon: 0.5590843898207511\n",
            "Episode: 116, Total reward: 4.0, Epsilon: 0.5562889678716474\n",
            "Episode: 117, Total reward: 29.0, Epsilon: 0.5535075230322891\n",
            "Episode: 118, Total reward: 111.0, Epsilon: 0.5507399854171277\n",
            "Episode: 119, Total reward: 21.0, Epsilon: 0.547986285490042\n",
            "Episode: 120, Total reward: 3.0, Epsilon: 0.5452463540625918\n",
            "Episode: 121, Total reward: 35.0, Epsilon: 0.5425201222922789\n",
            "Episode: 122, Total reward: 39.0, Epsilon: 0.5398075216808175\n",
            "Episode: 123, Total reward: 7.0, Epsilon: 0.5371084840724134\n",
            "Episode: 124, Total reward: 4.0, Epsilon: 0.5344229416520513\n",
            "Episode: 125, Total reward: 99.0, Epsilon: 0.531750826943791\n",
            "Episode: 126, Total reward: 15.0, Epsilon: 0.5290920728090721\n",
            "Episode: 127, Total reward: 98.0, Epsilon: 0.5264466124450268\n",
            "Episode: 128, Total reward: 4.0, Epsilon: 0.5238143793828016\n",
            "Episode: 129, Total reward: 35.0, Epsilon: 0.5211953074858876\n",
            "Episode: 130, Total reward: 41.0, Epsilon: 0.5185893309484582\n",
            "Episode: 131, Total reward: 36.0, Epsilon: 0.5159963842937159\n",
            "Episode: 132, Total reward: 5.0, Epsilon: 0.5134164023722473\n",
            "Episode: 133, Total reward: 50.0, Epsilon: 0.510849320360386\n",
            "Episode: 134, Total reward: 90.0, Epsilon: 0.5082950737585841\n",
            "Episode: 135, Total reward: 32.0, Epsilon: 0.5057535983897912\n",
            "Episode: 136, Total reward: 8.0, Epsilon: 0.5032248303978422\n",
            "Episode: 137, Total reward: 27.0, Epsilon: 0.500708706245853\n",
            "Episode: 138, Total reward: 35.0, Epsilon: 0.4982051627146237\n",
            "Episode: 139, Total reward: 16.0, Epsilon: 0.49571413690105054\n",
            "Episode: 140, Total reward: 4.0, Epsilon: 0.4932355662165453\n",
            "Episode: 141, Total reward: 2.0, Epsilon: 0.4907693883854626\n",
            "Episode: 142, Total reward: 7.0, Epsilon: 0.4883155414435353\n",
            "Episode: 143, Total reward: 17.0, Epsilon: 0.4858739637363176\n",
            "Episode: 144, Total reward: 1.0, Epsilon: 0.483444593917636\n",
            "Episode: 145, Total reward: 85.0, Epsilon: 0.4810273709480478\n",
            "Episode: 146, Total reward: 52.0, Epsilon: 0.47862223409330756\n",
            "Episode: 147, Total reward: 42.0, Epsilon: 0.47622912292284103\n",
            "Episode: 148, Total reward: 16.0, Epsilon: 0.4738479773082268\n",
            "Episode: 149, Total reward: 6.0, Epsilon: 0.47147873742168567\n",
            "Episode: 150, Total reward: 16.0, Epsilon: 0.46912134373457726\n",
            "Episode: 151, Total reward: 30.0, Epsilon: 0.46677573701590436\n",
            "Episode: 152, Total reward: 41.0, Epsilon: 0.46444185833082485\n",
            "Episode: 153, Total reward: 88.0, Epsilon: 0.46211964903917074\n",
            "Episode: 154, Total reward: 4.0, Epsilon: 0.4598090507939749\n",
            "Episode: 155, Total reward: 92.0, Epsilon: 0.457510005540005\n",
            "Episode: 156, Total reward: 23.0, Epsilon: 0.45522245551230495\n",
            "Episode: 157, Total reward: 89.0, Epsilon: 0.4529463432347434\n",
            "Episode: 158, Total reward: 7.0, Epsilon: 0.4506816115185697\n",
            "Episode: 159, Total reward: 92.0, Epsilon: 0.4484282034609769\n",
            "Episode: 160, Total reward: 15.0, Epsilon: 0.446186062443672\n",
            "Episode: 161, Total reward: -1.0, Epsilon: 0.4439551321314536\n",
            "Episode: 162, Total reward: 47.0, Epsilon: 0.4417353564707963\n",
            "Episode: 163, Total reward: 10.0, Epsilon: 0.43952667968844233\n",
            "Episode: 164, Total reward: 93.0, Epsilon: 0.43732904629000013\n",
            "Episode: 165, Total reward: 39.0, Epsilon: 0.4351424010585501\n",
            "Episode: 166, Total reward: 5.0, Epsilon: 0.43296668905325736\n",
            "Episode: 167, Total reward: 7.0, Epsilon: 0.43080185560799106\n",
            "Episode: 168, Total reward: 8.0, Epsilon: 0.4286478463299511\n",
            "Episode: 169, Total reward: 94.0, Epsilon: 0.42650460709830135\n",
            "Episode: 170, Total reward: 3.0, Epsilon: 0.42437208406280985\n",
            "Episode: 171, Total reward: 32.0, Epsilon: 0.4222502236424958\n",
            "Episode: 172, Total reward: 124.0, Epsilon: 0.42013897252428334\n",
            "Episode: 173, Total reward: 18.0, Epsilon: 0.4180382776616619\n",
            "Episode: 174, Total reward: 71.0, Epsilon: 0.4159480862733536\n",
            "Episode: 175, Total reward: 53.0, Epsilon: 0.41386834584198684\n",
            "Episode: 176, Total reward: 9.0, Epsilon: 0.4117990041127769\n",
            "Episode: 177, Total reward: 48.0, Epsilon: 0.40974000909221303\n",
            "Episode: 178, Total reward: 163.0, Epsilon: 0.40769130904675194\n",
            "Episode: 179, Total reward: 92.0, Epsilon: 0.40565285250151817\n",
            "Episode: 180, Total reward: 4.0, Epsilon: 0.4036245882390106\n",
            "Episode: 181, Total reward: 62.0, Epsilon: 0.4016064652978155\n",
            "Episode: 182, Total reward: 7.0, Epsilon: 0.3995984329713264\n",
            "Episode: 183, Total reward: 9.0, Epsilon: 0.3976004408064698\n",
            "Episode: 184, Total reward: 15.0, Epsilon: 0.39561243860243744\n",
            "Episode: 185, Total reward: -3.0, Epsilon: 0.3936343764094253\n",
            "Episode: 186, Total reward: -1.0, Epsilon: 0.39166620452737816\n",
            "Episode: 187, Total reward: 45.0, Epsilon: 0.3897078735047413\n",
            "Episode: 188, Total reward: 19.0, Epsilon: 0.3877593341372176\n",
            "Episode: 189, Total reward: 133.0, Epsilon: 0.3858205374665315\n",
            "Episode: 190, Total reward: 7.0, Epsilon: 0.38389143477919885\n",
            "Episode: 191, Total reward: 0.0, Epsilon: 0.3819719776053028\n",
            "Episode: 192, Total reward: 2.0, Epsilon: 0.3800621177172763\n",
            "Episode: 193, Total reward: -1.0, Epsilon: 0.37816180712868996\n",
            "Episode: 194, Total reward: 63.0, Epsilon: 0.37627099809304654\n",
            "Episode: 195, Total reward: 7.0, Epsilon: 0.3743896431025813\n",
            "Episode: 196, Total reward: 11.0, Epsilon: 0.37251769488706843\n",
            "Episode: 197, Total reward: 69.0, Epsilon: 0.3706551064126331\n",
            "Episode: 198, Total reward: 13.0, Epsilon: 0.36880183088056995\n",
            "Episode: 199, Total reward: 86.0, Epsilon: 0.3669578217261671\n",
            "Episode: 200, Total reward: 41.0, Epsilon: 0.36512303261753626\n",
            "Episode: 201, Total reward: 9.0, Epsilon: 0.3632974174544486\n",
            "Episode: 202, Total reward: 23.0, Epsilon: 0.3614809303671764\n",
            "Episode: 203, Total reward: 36.0, Epsilon: 0.3596735257153405\n",
            "Episode: 204, Total reward: 6.0, Epsilon: 0.3578751580867638\n",
            "Episode: 205, Total reward: 11.0, Epsilon: 0.35608578229633\n",
            "Episode: 206, Total reward: 3.0, Epsilon: 0.3543053533848483\n",
            "Episode: 207, Total reward: 35.0, Epsilon: 0.35253382661792404\n",
            "Episode: 208, Total reward: 6.0, Epsilon: 0.3507711574848344\n",
            "Episode: 209, Total reward: -1.0, Epsilon: 0.34901730169741024\n",
            "Episode: 210, Total reward: 3.0, Epsilon: 0.3472722151889232\n",
            "Episode: 211, Total reward: 50.0, Epsilon: 0.3455358541129786\n",
            "Episode: 212, Total reward: 45.0, Epsilon: 0.3438081748424137\n",
            "Episode: 213, Total reward: 10.0, Epsilon: 0.3420891339682016\n",
            "Episode: 214, Total reward: 13.0, Epsilon: 0.3403786882983606\n",
            "Episode: 215, Total reward: 40.0, Epsilon: 0.3386767948568688\n",
            "Episode: 216, Total reward: 64.0, Epsilon: 0.33698341088258443\n",
            "Episode: 217, Total reward: 91.0, Epsilon: 0.3352984938281715\n",
            "Episode: 218, Total reward: 3.0, Epsilon: 0.33362200135903064\n",
            "Episode: 219, Total reward: 3.0, Epsilon: 0.33195389135223546\n",
            "Episode: 220, Total reward: 22.0, Epsilon: 0.3302941218954743\n",
            "Episode: 221, Total reward: 107.0, Epsilon: 0.32864265128599696\n",
            "Episode: 222, Total reward: 116.0, Epsilon: 0.326999438029567\n",
            "Episode: 223, Total reward: 107.0, Epsilon: 0.3253644408394192\n",
            "Episode: 224, Total reward: 23.0, Epsilon: 0.3237376186352221\n",
            "Episode: 225, Total reward: 10.0, Epsilon: 0.322118930542046\n",
            "Episode: 226, Total reward: 95.0, Epsilon: 0.32050833588933575\n",
            "Episode: 227, Total reward: 50.0, Epsilon: 0.31890579420988907\n",
            "Episode: 228, Total reward: 7.0, Epsilon: 0.3173112652388396\n",
            "Episode: 229, Total reward: 112.0, Epsilon: 0.3157247089126454\n",
            "Episode: 230, Total reward: 94.0, Epsilon: 0.3141460853680822\n",
            "Episode: 231, Total reward: 129.0, Epsilon: 0.3125753549412418\n",
            "Episode: 232, Total reward: 5.0, Epsilon: 0.31101247816653554\n",
            "Episode: 233, Total reward: 67.0, Epsilon: 0.30945741577570285\n",
            "Episode: 234, Total reward: 2.0, Epsilon: 0.3079101286968243\n",
            "Episode: 235, Total reward: 147.0, Epsilon: 0.3063705780533402\n",
            "Episode: 236, Total reward: 1.0, Epsilon: 0.30483872516307353\n",
            "Episode: 237, Total reward: 57.0, Epsilon: 0.3033145315372582\n",
            "Episode: 238, Total reward: 7.0, Epsilon: 0.3017979588795719\n",
            "Episode: 239, Total reward: 187.0, Epsilon: 0.30028896908517405\n",
            "Episode: 240, Total reward: 280.0, Epsilon: 0.2987875242397482\n",
            "Episode: 241, Total reward: 35.0, Epsilon: 0.29729358661854943\n",
            "Episode: 242, Total reward: 265.0, Epsilon: 0.29580711868545667\n",
            "Episode: 243, Total reward: 5.0, Epsilon: 0.2943280830920294\n",
            "Episode: 244, Total reward: 33.0, Epsilon: 0.29285644267656924\n",
            "Episode: 245, Total reward: 10.0, Epsilon: 0.2913921604631864\n",
            "Episode: 246, Total reward: 222.0, Epsilon: 0.28993519966087045\n",
            "Episode: 247, Total reward: 382.0, Epsilon: 0.2884855236625661\n",
            "Episode: 248, Total reward: 160.0, Epsilon: 0.28704309604425327\n",
            "Episode: 249, Total reward: 263.0, Epsilon: 0.285607880564032\n",
            "Episode: 250, Total reward: 159.0, Epsilon: 0.28417984116121187\n",
            "Episode: 251, Total reward: 332.0, Epsilon: 0.2827589419554058\n",
            "Episode: 252, Total reward: 334.0, Epsilon: 0.28134514724562876\n",
            "Episode: 253, Total reward: 300.0, Epsilon: 0.2799384215094006\n",
            "Episode: 254, Total reward: 1.0, Epsilon: 0.27853872940185365\n",
            "Episode: 255, Total reward: 279.0, Epsilon: 0.27714603575484437\n",
            "Episode: 256, Total reward: 334.0, Epsilon: 0.2757603055760701\n",
            "Episode: 257, Total reward: 175.0, Epsilon: 0.2743815040481898\n",
            "Episode: 258, Total reward: 286.0, Epsilon: 0.2730095965279488\n",
            "Episode: 259, Total reward: 320.0, Epsilon: 0.27164454854530906\n",
            "Episode: 260, Total reward: 264.0, Epsilon: 0.2702863258025825\n",
            "Episode: 261, Total reward: 173.0, Epsilon: 0.2689348941735696\n",
            "Episode: 262, Total reward: 95.0, Epsilon: 0.26759021970270175\n",
            "Episode: 263, Total reward: 67.0, Epsilon: 0.2662522686041882\n",
            "Episode: 264, Total reward: 117.0, Epsilon: 0.2649210072611673\n",
            "Episode: 265, Total reward: 9.0, Epsilon: 0.26359640222486147\n",
            "Episode: 266, Total reward: 0.0, Epsilon: 0.26227842021373715\n",
            "Episode: 267, Total reward: 8.0, Epsilon: 0.2609670281126685\n",
            "Episode: 268, Total reward: -1.0, Epsilon: 0.25966219297210513\n",
            "Episode: 269, Total reward: 255.0, Epsilon: 0.2583638820072446\n",
            "Episode: 270, Total reward: 294.0, Epsilon: 0.2570720625972084\n",
            "Episode: 271, Total reward: 3.0, Epsilon: 0.25578670228422234\n",
            "Episode: 272, Total reward: 0.0, Epsilon: 0.25450776877280124\n",
            "Episode: 273, Total reward: 444.0, Epsilon: 0.2532352299289372\n",
            "Episode: 274, Total reward: 9.0, Epsilon: 0.2519690537792925\n",
            "Episode: 275, Total reward: 1.0, Epsilon: 0.2507092085103961\n",
            "Episode: 276, Total reward: 2.0, Epsilon: 0.2494556624678441\n",
            "Episode: 277, Total reward: 204.0, Epsilon: 0.24820838415550486\n",
            "Episode: 278, Total reward: 489.0, Epsilon: 0.24696734223472733\n",
            "Episode: 279, Total reward: 7.0, Epsilon: 0.2457325055235537\n",
            "Episode: 280, Total reward: 380.0, Epsilon: 0.24450384299593592\n",
            "Episode: 281, Total reward: 3.0, Epsilon: 0.24328132378095624\n",
            "Episode: 282, Total reward: 16.0, Epsilon: 0.24206491716205145\n",
            "Episode: 283, Total reward: 489.0, Epsilon: 0.2408545925762412\n",
            "Episode: 284, Total reward: 305.0, Epsilon: 0.23965031961336\n",
            "Episode: 285, Total reward: 283.0, Epsilon: 0.2384520680152932\n",
            "Episode: 286, Total reward: 131.0, Epsilon: 0.23725980767521673\n",
            "Episode: 287, Total reward: 125.0, Epsilon: 0.23607350863684065\n",
            "Episode: 288, Total reward: 12.0, Epsilon: 0.23489314109365644\n",
            "Episode: 289, Total reward: 281.0, Epsilon: 0.23371867538818816\n",
            "Episode: 290, Total reward: 166.0, Epsilon: 0.23255008201124722\n",
            "Episode: 291, Total reward: 14.0, Epsilon: 0.231387331601191\n",
            "Episode: 292, Total reward: 348.0, Epsilon: 0.23023039494318503\n",
            "Episode: 293, Total reward: 243.0, Epsilon: 0.2290792429684691\n",
            "Episode: 294, Total reward: 3.0, Epsilon: 0.22793384675362674\n",
            "Episode: 295, Total reward: 101.0, Epsilon: 0.22679417751985861\n",
            "Episode: 296, Total reward: 70.0, Epsilon: 0.22566020663225933\n",
            "Episode: 297, Total reward: 122.0, Epsilon: 0.22453190559909803\n",
            "Episode: 298, Total reward: 193.0, Epsilon: 0.22340924607110255\n",
            "Episode: 299, Total reward: 128.0, Epsilon: 0.22229219984074702\n",
            "Episode: 300, Total reward: 134.0, Epsilon: 0.2211807388415433\n",
            "Episode: 301, Total reward: 116.0, Epsilon: 0.22007483514733558\n",
            "Episode: 302, Total reward: 4.0, Epsilon: 0.2189744609715989\n",
            "Episode: 303, Total reward: 180.0, Epsilon: 0.2178795886667409\n",
            "Episode: 304, Total reward: 6.0, Epsilon: 0.2167901907234072\n",
            "Episode: 305, Total reward: 325.0, Epsilon: 0.21570623976979014\n",
            "Episode: 306, Total reward: 5.0, Epsilon: 0.21462770857094118\n",
            "Episode: 307, Total reward: 22.0, Epsilon: 0.21355457002808648\n",
            "Episode: 308, Total reward: 167.0, Epsilon: 0.21248679717794605\n",
            "Episode: 309, Total reward: 16.0, Epsilon: 0.21142436319205632\n",
            "Episode: 310, Total reward: 8.0, Epsilon: 0.21036724137609603\n",
            "Episode: 311, Total reward: 273.0, Epsilon: 0.20931540516921554\n",
            "Episode: 312, Total reward: 216.0, Epsilon: 0.20826882814336947\n",
            "Episode: 313, Total reward: 5.0, Epsilon: 0.20722748400265262\n",
            "Episode: 314, Total reward: 291.0, Epsilon: 0.20619134658263935\n",
            "Episode: 315, Total reward: 314.0, Epsilon: 0.20516038984972615\n",
            "Episode: 316, Total reward: 5.0, Epsilon: 0.2041345879004775\n",
            "Episode: 317, Total reward: 6.0, Epsilon: 0.2031139149609751\n",
            "Episode: 318, Total reward: 321.0, Epsilon: 0.20209834538617025\n"
          ]
        }
      ]
    }
  ]
}