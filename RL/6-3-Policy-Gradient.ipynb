{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMi9Fi+MpQ7qdNlU0n3bdC7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSJeong-me/AI-Innovation-2024/blob/main/RL/6-3-Policy-Gradient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "omTxvvcupo1N",
        "outputId": "375054ba-f34e-40c3-998c-1d1173ba3099"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "zero-dimensional tensor (at position 0) cannot be concatenated",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1fd1b0f74fb5>\u001b[0m in \u001b[0;36m<cell line: 51>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# 역전파 및 최적화\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mpolicy_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_gradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 모든 손실을 합산\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0mpolicy_gradient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 손실을 기준으로 역전파 수행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 정책 네트워크의 가중치 업데이트\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: zero-dimensional tensor (at position 0) cannot be concatenated"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Policy Network 정의 (정책 신경망)\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, action_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.fc1(state))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        action_probs = torch.softmax(self.fc3(x), dim=-1)  # 각 행동의 확률\n",
        "        return action_probs\n",
        "\n",
        "# 에이전트의 행동을 선택하는 함수 (정책을 통해 행동 선택)\n",
        "def select_action(state, policy_network):\n",
        "    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)  # 배치 차원을 맞추기 위해 추가\n",
        "    action_probs = policy_network(state)\n",
        "    action = np.random.choice(np.arange(action_size), p=action_probs.detach().numpy().squeeze())\n",
        "    return action, action_probs[0, action]\n",
        "\n",
        "# 학습을 위한 하이퍼파라미터\n",
        "learning_rate = 0.001\n",
        "gamma = 0.99  # 할인율\n",
        "state_size = 4  # CartPole의 상태 크기\n",
        "action_size = 2  # CartPole의 행동 공간 크기\n",
        "\n",
        "# CartPole 환경 설정\n",
        "env = gym.make('CartPole-v1')\n",
        "policy_network = PolicyNetwork(state_size, action_size)\n",
        "optimizer = optim.Adam(policy_network.parameters(), lr=learning_rate)\n",
        "\n",
        "# 에피소드에서 수집한 보상을 할인해서 반환하는 함수\n",
        "def discount_rewards(rewards, gamma):\n",
        "    discounted_rewards = np.zeros_like(rewards)\n",
        "    cumulative = 0\n",
        "    for i in reversed(range(len(rewards))):\n",
        "        cumulative = cumulative * gamma + rewards[i]\n",
        "        discounted_rewards[i] = cumulative\n",
        "    return discounted_rewards\n",
        "\n",
        "# REINFORCE 알고리즘을 사용한 학습 루프\n",
        "episodes = 1000\n",
        "\n",
        "for episode in range(episodes):\n",
        "    state = env.reset()\n",
        "    log_probs = []\n",
        "    rewards = []\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # 정책에 따라 행동 선택\n",
        "        action, log_prob = select_action(state, policy_network)\n",
        "\n",
        "        # 선택된 행동을 실행하여 다음 상태, 보상 등을 얻음\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        # 로그 확률과 보상을 저장\n",
        "        log_probs.append(torch.log(log_prob))\n",
        "        rewards.append(reward)\n",
        "        total_reward += reward\n",
        "\n",
        "        # 상태 업데이트\n",
        "        state = next_state\n",
        "\n",
        "    # 에피소드가 끝난 후 보상을 할인하여 얻음\n",
        "    discounted_rewards = discount_rewards(rewards, gamma)\n",
        "    discounted_rewards = torch.tensor(discounted_rewards, dtype=torch.float32)\n",
        "\n",
        "    # 보상의 표준화를 통해 학습의 안정성 개선\n",
        "    discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-9)\n",
        "\n",
        "    # 정책 네트워크 업데이트 (정책 경사)\n",
        "    policy_gradient = []\n",
        "    for log_prob, reward in zip(log_probs, discounted_rewards):\n",
        "        policy_gradient.append(-log_prob * reward)\n",
        "\n",
        "    # 역전파 및 최적화\n",
        "    optimizer.zero_grad()\n",
        "    policy_gradient = torch.cat(policy_gradient).sum()  # 모든 손실을 합산\n",
        "    policy_gradient.backward()  # 손실을 기준으로 역전파 수행\n",
        "    optimizer.step()  # 정책 네트워크의 가중치 업데이트\n",
        "\n",
        "    # 에피소드 결과 출력\n",
        "    print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
        "\n",
        "# 학습 완료 후 에이전트 테스트\n",
        "for i in range(10):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        env.render()\n",
        "        action, _ = select_action(state, policy_network)\n",
        "        next_state, _, done, _ = env.step(action)\n",
        "        state = next_state\n",
        "env.close()\n"
      ]
    }
  ]
}