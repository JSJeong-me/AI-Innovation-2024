{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO38Nz5qDZ8bcx8k9iGLDYl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSJeong-me/AI-Innovation-2024/blob/main/RL/6-3-Actor-Critic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Vk3Ctanmdt8",
        "outputId": "95b86bb7-a412-4cab-f8bd-58c5b7e3fbfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0, Total Reward: 21.0\n",
            "Episode 1, Total Reward: 25.0\n",
            "Episode 2, Total Reward: 24.0\n",
            "Episode 3, Total Reward: 65.0\n",
            "Episode 4, Total Reward: 39.0\n",
            "Episode 5, Total Reward: 34.0\n",
            "Episode 6, Total Reward: 16.0\n",
            "Episode 7, Total Reward: 52.0\n",
            "Episode 8, Total Reward: 14.0\n",
            "Episode 9, Total Reward: 22.0\n",
            "Episode 10, Total Reward: 58.0\n",
            "Episode 11, Total Reward: 17.0\n",
            "Episode 12, Total Reward: 92.0\n",
            "Episode 13, Total Reward: 44.0\n",
            "Episode 14, Total Reward: 87.0\n",
            "Episode 15, Total Reward: 20.0\n",
            "Episode 16, Total Reward: 30.0\n",
            "Episode 17, Total Reward: 50.0\n",
            "Episode 18, Total Reward: 65.0\n",
            "Episode 19, Total Reward: 93.0\n",
            "Episode 20, Total Reward: 32.0\n",
            "Episode 21, Total Reward: 23.0\n",
            "Episode 22, Total Reward: 25.0\n",
            "Episode 23, Total Reward: 23.0\n",
            "Episode 24, Total Reward: 24.0\n",
            "Episode 25, Total Reward: 31.0\n",
            "Episode 26, Total Reward: 35.0\n",
            "Episode 27, Total Reward: 106.0\n",
            "Episode 28, Total Reward: 63.0\n",
            "Episode 29, Total Reward: 40.0\n",
            "Episode 30, Total Reward: 25.0\n",
            "Episode 31, Total Reward: 40.0\n",
            "Episode 32, Total Reward: 51.0\n",
            "Episode 33, Total Reward: 73.0\n",
            "Episode 34, Total Reward: 34.0\n",
            "Episode 35, Total Reward: 45.0\n",
            "Episode 36, Total Reward: 76.0\n",
            "Episode 37, Total Reward: 40.0\n",
            "Episode 38, Total Reward: 89.0\n",
            "Episode 39, Total Reward: 136.0\n",
            "Episode 40, Total Reward: 63.0\n",
            "Episode 41, Total Reward: 86.0\n",
            "Episode 42, Total Reward: 54.0\n",
            "Episode 43, Total Reward: 66.0\n",
            "Episode 44, Total Reward: 86.0\n",
            "Episode 45, Total Reward: 50.0\n",
            "Episode 46, Total Reward: 62.0\n",
            "Episode 47, Total Reward: 46.0\n",
            "Episode 48, Total Reward: 42.0\n",
            "Episode 49, Total Reward: 48.0\n",
            "Episode 50, Total Reward: 31.0\n",
            "Episode 51, Total Reward: 27.0\n",
            "Episode 52, Total Reward: 37.0\n",
            "Episode 53, Total Reward: 38.0\n",
            "Episode 54, Total Reward: 45.0\n",
            "Episode 55, Total Reward: 46.0\n",
            "Episode 56, Total Reward: 36.0\n",
            "Episode 57, Total Reward: 61.0\n",
            "Episode 58, Total Reward: 82.0\n",
            "Episode 59, Total Reward: 65.0\n",
            "Episode 60, Total Reward: 34.0\n",
            "Episode 61, Total Reward: 55.0\n",
            "Episode 62, Total Reward: 58.0\n",
            "Episode 63, Total Reward: 33.0\n",
            "Episode 64, Total Reward: 110.0\n",
            "Episode 65, Total Reward: 55.0\n",
            "Episode 66, Total Reward: 65.0\n",
            "Episode 67, Total Reward: 78.0\n",
            "Episode 68, Total Reward: 74.0\n",
            "Episode 69, Total Reward: 55.0\n",
            "Episode 70, Total Reward: 63.0\n",
            "Episode 71, Total Reward: 59.0\n",
            "Episode 72, Total Reward: 41.0\n",
            "Episode 73, Total Reward: 47.0\n",
            "Episode 74, Total Reward: 68.0\n",
            "Episode 75, Total Reward: 73.0\n",
            "Episode 76, Total Reward: 51.0\n",
            "Episode 77, Total Reward: 41.0\n",
            "Episode 78, Total Reward: 68.0\n",
            "Episode 79, Total Reward: 40.0\n",
            "Episode 80, Total Reward: 46.0\n",
            "Episode 81, Total Reward: 65.0\n",
            "Episode 82, Total Reward: 51.0\n",
            "Episode 83, Total Reward: 42.0\n",
            "Episode 84, Total Reward: 62.0\n",
            "Episode 85, Total Reward: 51.0\n",
            "Episode 86, Total Reward: 31.0\n",
            "Episode 87, Total Reward: 35.0\n",
            "Episode 88, Total Reward: 54.0\n",
            "Episode 89, Total Reward: 45.0\n",
            "Episode 90, Total Reward: 114.0\n",
            "Episode 91, Total Reward: 37.0\n",
            "Episode 92, Total Reward: 57.0\n",
            "Episode 93, Total Reward: 41.0\n",
            "Episode 94, Total Reward: 32.0\n",
            "Episode 95, Total Reward: 39.0\n",
            "Episode 96, Total Reward: 32.0\n",
            "Episode 97, Total Reward: 28.0\n",
            "Episode 98, Total Reward: 47.0\n",
            "Episode 99, Total Reward: 46.0\n",
            "Episode 100, Total Reward: 34.0\n",
            "Episode 101, Total Reward: 40.0\n",
            "Episode 102, Total Reward: 29.0\n",
            "Episode 103, Total Reward: 38.0\n",
            "Episode 104, Total Reward: 48.0\n",
            "Episode 105, Total Reward: 28.0\n",
            "Episode 106, Total Reward: 72.0\n",
            "Episode 107, Total Reward: 35.0\n",
            "Episode 108, Total Reward: 55.0\n",
            "Episode 109, Total Reward: 38.0\n",
            "Episode 110, Total Reward: 74.0\n",
            "Episode 111, Total Reward: 32.0\n",
            "Episode 112, Total Reward: 47.0\n",
            "Episode 113, Total Reward: 45.0\n",
            "Episode 114, Total Reward: 60.0\n",
            "Episode 115, Total Reward: 47.0\n",
            "Episode 116, Total Reward: 67.0\n",
            "Episode 117, Total Reward: 28.0\n",
            "Episode 118, Total Reward: 65.0\n",
            "Episode 119, Total Reward: 31.0\n",
            "Episode 120, Total Reward: 55.0\n",
            "Episode 121, Total Reward: 47.0\n",
            "Episode 122, Total Reward: 57.0\n",
            "Episode 123, Total Reward: 38.0\n",
            "Episode 124, Total Reward: 61.0\n",
            "Episode 125, Total Reward: 50.0\n",
            "Episode 126, Total Reward: 179.0\n",
            "Episode 127, Total Reward: 48.0\n",
            "Episode 128, Total Reward: 78.0\n",
            "Episode 129, Total Reward: 44.0\n",
            "Episode 130, Total Reward: 27.0\n",
            "Episode 131, Total Reward: 46.0\n",
            "Episode 132, Total Reward: 33.0\n",
            "Episode 133, Total Reward: 37.0\n",
            "Episode 134, Total Reward: 28.0\n",
            "Episode 135, Total Reward: 26.0\n",
            "Episode 136, Total Reward: 27.0\n",
            "Episode 137, Total Reward: 38.0\n",
            "Episode 138, Total Reward: 35.0\n",
            "Episode 139, Total Reward: 42.0\n",
            "Episode 140, Total Reward: 32.0\n",
            "Episode 141, Total Reward: 45.0\n",
            "Episode 142, Total Reward: 39.0\n",
            "Episode 143, Total Reward: 75.0\n",
            "Episode 144, Total Reward: 78.0\n",
            "Episode 145, Total Reward: 162.0\n",
            "Episode 146, Total Reward: 71.0\n",
            "Episode 147, Total Reward: 65.0\n",
            "Episode 148, Total Reward: 54.0\n",
            "Episode 149, Total Reward: 53.0\n",
            "Episode 150, Total Reward: 53.0\n",
            "Episode 151, Total Reward: 49.0\n",
            "Episode 152, Total Reward: 81.0\n",
            "Episode 153, Total Reward: 63.0\n",
            "Episode 154, Total Reward: 60.0\n",
            "Episode 155, Total Reward: 42.0\n",
            "Episode 156, Total Reward: 86.0\n",
            "Episode 157, Total Reward: 61.0\n",
            "Episode 158, Total Reward: 57.0\n",
            "Episode 159, Total Reward: 65.0\n",
            "Episode 160, Total Reward: 47.0\n",
            "Episode 161, Total Reward: 103.0\n",
            "Episode 162, Total Reward: 91.0\n",
            "Episode 163, Total Reward: 74.0\n",
            "Episode 164, Total Reward: 85.0\n",
            "Episode 165, Total Reward: 56.0\n",
            "Episode 166, Total Reward: 70.0\n",
            "Episode 167, Total Reward: 183.0\n",
            "Episode 168, Total Reward: 121.0\n",
            "Episode 169, Total Reward: 200.0\n",
            "Episode 170, Total Reward: 169.0\n",
            "Episode 171, Total Reward: 83.0\n",
            "Episode 172, Total Reward: 100.0\n",
            "Episode 173, Total Reward: 393.0\n",
            "Episode 174, Total Reward: 212.0\n",
            "Episode 175, Total Reward: 106.0\n",
            "Episode 176, Total Reward: 113.0\n",
            "Episode 177, Total Reward: 87.0\n",
            "Episode 178, Total Reward: 100.0\n",
            "Episode 179, Total Reward: 36.0\n",
            "Episode 180, Total Reward: 37.0\n",
            "Episode 181, Total Reward: 50.0\n",
            "Episode 182, Total Reward: 78.0\n",
            "Episode 183, Total Reward: 60.0\n",
            "Episode 184, Total Reward: 60.0\n",
            "Episode 185, Total Reward: 79.0\n",
            "Episode 186, Total Reward: 61.0\n",
            "Episode 187, Total Reward: 78.0\n",
            "Episode 188, Total Reward: 67.0\n",
            "Episode 189, Total Reward: 69.0\n",
            "Episode 190, Total Reward: 75.0\n",
            "Episode 191, Total Reward: 60.0\n",
            "Episode 192, Total Reward: 72.0\n",
            "Episode 193, Total Reward: 84.0\n",
            "Episode 194, Total Reward: 77.0\n",
            "Episode 195, Total Reward: 65.0\n",
            "Episode 196, Total Reward: 80.0\n",
            "Episode 197, Total Reward: 120.0\n",
            "Episode 198, Total Reward: 82.0\n",
            "Episode 199, Total Reward: 108.0\n",
            "Episode 200, Total Reward: 123.0\n",
            "Episode 201, Total Reward: 120.0\n",
            "Episode 202, Total Reward: 120.0\n",
            "Episode 203, Total Reward: 159.0\n",
            "Episode 204, Total Reward: 133.0\n",
            "Episode 205, Total Reward: 263.0\n",
            "Episode 206, Total Reward: 121.0\n",
            "Episode 207, Total Reward: 154.0\n",
            "Episode 208, Total Reward: 211.0\n",
            "Episode 209, Total Reward: 130.0\n",
            "Episode 210, Total Reward: 307.0\n",
            "Episode 211, Total Reward: 128.0\n",
            "Episode 212, Total Reward: 171.0\n",
            "Episode 213, Total Reward: 135.0\n",
            "Episode 214, Total Reward: 126.0\n",
            "Episode 215, Total Reward: 151.0\n",
            "Episode 216, Total Reward: 116.0\n",
            "Episode 217, Total Reward: 119.0\n",
            "Episode 218, Total Reward: 150.0\n",
            "Episode 219, Total Reward: 100.0\n",
            "Episode 220, Total Reward: 95.0\n",
            "Episode 221, Total Reward: 88.0\n",
            "Episode 222, Total Reward: 102.0\n",
            "Episode 223, Total Reward: 117.0\n",
            "Episode 224, Total Reward: 109.0\n",
            "Episode 225, Total Reward: 114.0\n",
            "Episode 226, Total Reward: 268.0\n",
            "Episode 227, Total Reward: 155.0\n",
            "Episode 228, Total Reward: 242.0\n",
            "Episode 229, Total Reward: 208.0\n",
            "Episode 230, Total Reward: 375.0\n",
            "Episode 231, Total Reward: 156.0\n",
            "Episode 232, Total Reward: 277.0\n",
            "Episode 233, Total Reward: 228.0\n",
            "Episode 234, Total Reward: 218.0\n",
            "Episode 235, Total Reward: 191.0\n",
            "Episode 236, Total Reward: 256.0\n",
            "Episode 237, Total Reward: 318.0\n",
            "Episode 238, Total Reward: 201.0\n",
            "Episode 239, Total Reward: 317.0\n",
            "Episode 240, Total Reward: 213.0\n",
            "Episode 241, Total Reward: 188.0\n",
            "Episode 242, Total Reward: 202.0\n",
            "Episode 243, Total Reward: 258.0\n",
            "Episode 244, Total Reward: 402.0\n",
            "Episode 245, Total Reward: 430.0\n",
            "Episode 246, Total Reward: 500.0\n",
            "Episode 247, Total Reward: 381.0\n",
            "Episode 248, Total Reward: 500.0\n",
            "Episode 249, Total Reward: 500.0\n",
            "Episode 250, Total Reward: 500.0\n",
            "Episode 251, Total Reward: 500.0\n",
            "Episode 252, Total Reward: 500.0\n",
            "Episode 253, Total Reward: 92.0\n",
            "Episode 254, Total Reward: 75.0\n",
            "Episode 255, Total Reward: 53.0\n",
            "Episode 256, Total Reward: 72.0\n",
            "Episode 257, Total Reward: 53.0\n",
            "Episode 258, Total Reward: 71.0\n",
            "Episode 259, Total Reward: 48.0\n",
            "Episode 260, Total Reward: 59.0\n",
            "Episode 261, Total Reward: 54.0\n",
            "Episode 262, Total Reward: 61.0\n",
            "Episode 263, Total Reward: 69.0\n",
            "Episode 264, Total Reward: 74.0\n",
            "Episode 265, Total Reward: 61.0\n",
            "Episode 266, Total Reward: 60.0\n",
            "Episode 267, Total Reward: 70.0\n",
            "Episode 268, Total Reward: 51.0\n",
            "Episode 269, Total Reward: 59.0\n",
            "Episode 270, Total Reward: 83.0\n",
            "Episode 271, Total Reward: 66.0\n",
            "Episode 272, Total Reward: 63.0\n",
            "Episode 273, Total Reward: 84.0\n",
            "Episode 274, Total Reward: 92.0\n",
            "Episode 275, Total Reward: 77.0\n",
            "Episode 276, Total Reward: 86.0\n",
            "Episode 277, Total Reward: 54.0\n",
            "Episode 278, Total Reward: 61.0\n",
            "Episode 279, Total Reward: 58.0\n",
            "Episode 280, Total Reward: 64.0\n",
            "Episode 281, Total Reward: 51.0\n",
            "Episode 282, Total Reward: 88.0\n",
            "Episode 283, Total Reward: 50.0\n",
            "Episode 284, Total Reward: 64.0\n",
            "Episode 285, Total Reward: 62.0\n",
            "Episode 286, Total Reward: 60.0\n",
            "Episode 287, Total Reward: 68.0\n",
            "Episode 288, Total Reward: 72.0\n",
            "Episode 289, Total Reward: 49.0\n",
            "Episode 290, Total Reward: 58.0\n",
            "Episode 291, Total Reward: 116.0\n",
            "Episode 292, Total Reward: 86.0\n",
            "Episode 293, Total Reward: 105.0\n",
            "Episode 294, Total Reward: 69.0\n",
            "Episode 295, Total Reward: 70.0\n",
            "Episode 296, Total Reward: 74.0\n",
            "Episode 297, Total Reward: 67.0\n",
            "Episode 298, Total Reward: 68.0\n",
            "Episode 299, Total Reward: 48.0\n",
            "Episode 300, Total Reward: 58.0\n",
            "Episode 301, Total Reward: 92.0\n",
            "Episode 302, Total Reward: 59.0\n",
            "Episode 303, Total Reward: 120.0\n",
            "Episode 304, Total Reward: 62.0\n",
            "Episode 305, Total Reward: 56.0\n",
            "Episode 306, Total Reward: 62.0\n",
            "Episode 307, Total Reward: 85.0\n",
            "Episode 308, Total Reward: 80.0\n",
            "Episode 309, Total Reward: 98.0\n",
            "Episode 310, Total Reward: 70.0\n",
            "Episode 311, Total Reward: 79.0\n",
            "Episode 312, Total Reward: 119.0\n",
            "Episode 313, Total Reward: 71.0\n",
            "Episode 314, Total Reward: 94.0\n",
            "Episode 315, Total Reward: 100.0\n",
            "Episode 316, Total Reward: 86.0\n",
            "Episode 317, Total Reward: 142.0\n",
            "Episode 318, Total Reward: 92.0\n",
            "Episode 319, Total Reward: 84.0\n",
            "Episode 320, Total Reward: 327.0\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Actor 네트워크 정의\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(Actor, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, action_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.fc1(state))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        action_probs = torch.softmax(self.fc3(x), dim=-1)\n",
        "        return action_probs\n",
        "\n",
        "# Critic 네트워크 정의\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_size):\n",
        "        super(Critic, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.fc1(state))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        state_value = self.fc3(x)\n",
        "        return state_value\n",
        "\n",
        "# 환경 설정 및 하이퍼파라미터 정의\n",
        "env = gym.make('CartPole-v1')\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "\n",
        "actor = Actor(state_size, action_size)\n",
        "critic = Critic(state_size)\n",
        "\n",
        "# 학습 하이퍼파라미터 설정\n",
        "learning_rate = 0.001\n",
        "gamma = 0.99\n",
        "optimizer_actor = optim.Adam(actor.parameters(), lr=learning_rate)\n",
        "optimizer_critic = optim.Adam(critic.parameters(), lr=learning_rate)\n",
        "\n",
        "# 정책에 따라 행동을 선택하는 함수\n",
        "def choose_action(state):\n",
        "    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "    action_probs = actor(state)\n",
        "    action = np.random.choice(np.arange(action_size), p=action_probs.detach().numpy().squeeze())\n",
        "    return action, action_probs[0, action]\n",
        "\n",
        "# 학습 루프\n",
        "episodes = 1000\n",
        "\n",
        "for episode in range(episodes):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # 현재 상태에서 행동 선택\n",
        "        action, action_prob = choose_action(state)\n",
        "\n",
        "        # 환경에서 한 단계 진행\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "\n",
        "        # 상태 평가 (Critic)\n",
        "        state_value = critic(torch.tensor(state, dtype=torch.float32).unsqueeze(0))\n",
        "        next_state_value = critic(torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)) if not done else torch.tensor([[0.0]])\n",
        "\n",
        "        # 타겟 값 및 TD 에러 계산\n",
        "        target_value = reward + gamma * next_state_value\n",
        "        td_error = target_value - state_value\n",
        "\n",
        "        # Critic 업데이트 (MSE Loss)\n",
        "        critic_loss = td_error.pow(2).mean()\n",
        "        optimizer_critic.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        optimizer_critic.step()\n",
        "\n",
        "        # Actor 업데이트 (Policy Gradient)\n",
        "        actor_loss = -torch.log(action_prob) * td_error.detach()\n",
        "        optimizer_actor.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        optimizer_actor.step()\n",
        "\n",
        "        # 상태 업데이트\n",
        "        state = next_state\n",
        "\n",
        "    print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
        "\n",
        "# 학습 완료 후 에이전트 테스트\n",
        "for i in range(10):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        env.render()\n",
        "        action, _ = choose_action(state)\n",
        "        next_state, _, done, _ = env.step(action)\n",
        "        state = next_state\n",
        "env.close()\n"
      ]
    }
  ]
}