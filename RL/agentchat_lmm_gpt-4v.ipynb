{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSJeong-me/AI-Innovation-2024/blob/main/RL/agentchat_lmm_gpt-4v.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c75da30",
      "metadata": {
        "id": "2c75da30"
      },
      "source": [
        "# Engaging with Multimodal Models: GPT-4V in AutoGen\n",
        "\n",
        "In AutoGen, leveraging multimodal models can be done through two different methodologies:\n",
        "1. **MultimodalAgent**: Supported by GPT-4V and other LMMs, this agent is endowed with visual cognitive abilities, allowing it to engage in interactions comparable to those of other ConversableAgents.\n",
        "2. **VisionCapability**: For LLM-based agents lacking inherent visual comprehension, we introduce vision capabilities by converting images into descriptive captions.\n",
        "\n",
        "This guide will delve into each approach, providing insights into their application and integration."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f51914c",
      "metadata": {
        "id": "5f51914c"
      },
      "source": [
        "### Before everything starts, install AutoGen with the `lmm` option\n",
        "\n",
        "Install `autogen-agentchat`:\n",
        "```bash\n",
        "pip install \"autogen-agentchat[lmm]~=0.2\"\n",
        "```\n",
        "\n",
        "For more information, please refer to the [installation guide](/docs/installation/).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install autogen"
      ],
      "metadata": {
        "id": "c-ZizUNONMBu",
        "outputId": "07943507-945a-49bc-9ad4-341783f31dbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "c-ZizUNONMBu",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autogen\n",
            "  Downloading autogen-0.3.2-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting diskcache (from autogen)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting docker (from autogen)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting flaml (from autogen)\n",
            "  Downloading FLAML-2.3.2-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from autogen) (1.26.4)\n",
            "Requirement already satisfied: openai>=1.3 in /usr/local/lib/python3.10/dist-packages (from autogen) (1.54.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from autogen) (24.2)\n",
            "Requirement already satisfied: pydantic!=2.6.0,<3,>=1.10 in /usr/local/lib/python3.10/dist-packages (from autogen) (2.9.2)\n",
            "Collecting python-dotenv (from autogen)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from autogen) (2.5.0)\n",
            "Collecting tiktoken (from autogen)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->autogen) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->autogen) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->autogen) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->autogen) (0.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->autogen) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->autogen) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->autogen) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.6.0,<3,>=1.10->autogen) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.6.0,<3,>=1.10->autogen) (2.23.4)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from docker->autogen) (2.32.3)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from docker->autogen) (2.2.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->autogen) (2024.9.11)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.3->autogen) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.3->autogen) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.3->autogen) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.3->autogen) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.3->autogen) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->docker->autogen) (3.4.0)\n",
            "Downloading autogen-0.3.2-py3-none-any.whl (351 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.0/352.0 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading FLAML-2.3.2-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.9/313.9 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-dotenv, flaml, diskcache, tiktoken, docker, autogen\n",
            "Successfully installed autogen-0.3.2 diskcache-5.6.3 docker-7.1.0 flaml-2.3.2 python-dotenv-1.0.1 tiktoken-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import openai\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "openai.api_key  = os.environ[\"OPENAI_API_KEY\"]"
      ],
      "metadata": {
        "id": "jxkMy8JANQB_"
      },
      "id": "jxkMy8JANQB_",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "67d45964",
      "metadata": {
        "id": "67d45964",
        "outputId": "faac64da-4ace-43b7-dab7-448ae0c57dd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import requests\n",
        "from PIL import Image\n",
        "from termcolor import colored\n",
        "\n",
        "import autogen\n",
        "from autogen import Agent, AssistantAgent, ConversableAgent, UserProxyAgent\n",
        "from autogen.agentchat.contrib.capabilities.vision_capability import VisionCapability\n",
        "from autogen.agentchat.contrib.img_utils import get_pil_image, pil_to_data_uri\n",
        "from autogen.agentchat.contrib.multimodal_conversable_agent import MultimodalConversableAgent\n",
        "from autogen.code_utils import content_str"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e4faf59",
      "metadata": {
        "id": "7e4faf59"
      },
      "source": [
        "<a id=\"app-1\"></a>\n",
        "## Application 1: Image Chat\n",
        "\n",
        "In this section, we present a straightforward dual-agent architecture to enable user to chat with a multimodal agent.\n",
        "\n",
        "\n",
        "First, we show this image and ask a question.\n",
        "![](https://th.bing.com/th/id/R.422068ce8af4e15b0634fe2540adea7a?rik=y4OcXBE%2fqutDOw&pid=ImgRaw&r=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3d5580e",
      "metadata": {
        "id": "e3d5580e"
      },
      "source": [
        "Within the user proxy agent, we can decide to activate the human input mode or not (for here, we use human_input_mode=\"NEVER\" for conciseness). This allows you to interact with LMM in a multi-round dialogue, enabling you to provide feedback as the conversation unfolds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b1db6f5d",
      "metadata": {
        "id": "b1db6f5d"
      },
      "outputs": [],
      "source": [
        "config_list_4v = autogen.config_list_from_json(\n",
        "    \"OAI_CONFIG_LIST\",\n",
        "    filter_dict={\n",
        "        \"model\": [\"gpt-4-vision-system\"],\n",
        "    },\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "config_list_gpt4 = autogen.config_list_from_json(\n",
        "    \"OAI_CONFIG_LIST\",\n",
        "    filter_dict={\n",
        "        \"model\": [\"gpt-4\", \"gpt-4-0314\", \"gpt4\", \"gpt-4-32k\", \"gpt-4-32k-0314\", \"gpt-4-32k-v0314\"],\n",
        "    },\n",
        ")\n",
        "\n",
        "gpt4_llm_config = {\"config_list\": config_list_gpt4, \"cache_seed\": 42}"
      ],
      "metadata": {
        "id": "NENOb1LzOj3e"
      },
      "id": "NENOb1LzOj3e",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e9c628db",
      "metadata": {
        "id": "e9c628db"
      },
      "source": [
        "Learn more about configuring LLMs for agents [here](/docs/topics/llm_configuration)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "67157629",
      "metadata": {
        "scrolled": false,
        "id": "67157629",
        "outputId": "28155563-e270-4545-cba4-9752ed0bc2dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User_proxy (to image-explainer):\n",
            "\n",
            "What's the breed of this dog?\n",
            "<image>.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "image-explainer (to User_proxy):\n",
            "\n",
            "This looks like a Goldendoodle, which is a cross between a Golden Retriever and a Poodle. They are known for their curly coats and friendly demeanor.\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatResult(chat_id=None, chat_history=[{'content': \"What's the breed of this dog?\\n<img https://th.bing.com/th/id/R.422068ce8af4e15b0634fe2540adea7a?rik=y4OcXBE%2fqutDOw&pid=ImgRaw&r=0>.\", 'role': 'assistant', 'name': 'User_proxy'}, {'content': 'This looks like a Goldendoodle, which is a cross between a Golden Retriever and a Poodle. They are known for their curly coats and friendly demeanor.', 'role': 'user', 'name': 'image-explainer'}], summary='This looks like a Goldendoodle, which is a cross between a Golden Retriever and a Poodle. They are known for their curly coats and friendly demeanor.', cost={'usage_including_cached_inference': {'total_cost': 0.003155, 'gpt-4o-2024-08-06': {'cost': 0.003155, 'prompt_tokens': 1134, 'completion_tokens': 32, 'total_tokens': 1166}}, 'usage_excluding_cached_inference': {'total_cost': 0.003155, 'gpt-4o-2024-08-06': {'cost': 0.003155, 'prompt_tokens': 1134, 'completion_tokens': 32, 'total_tokens': 1166}}}, human_input=[])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "image_agent = MultimodalConversableAgent(\n",
        "    name=\"image-explainer\",\n",
        "    max_consecutive_auto_reply=10,\n",
        "    llm_config={\"config_list\": config_list_4v, \"temperature\": 0.5, \"max_tokens\": 300, \"model\": \"gpt-4o\" },\n",
        ")\n",
        "\n",
        "user_proxy = autogen.UserProxyAgent(\n",
        "    name=\"User_proxy\",\n",
        "    system_message=\"A human admin.\",\n",
        "    human_input_mode=\"NEVER\",  # Try between ALWAYS or NEVER\n",
        "    max_consecutive_auto_reply=0,\n",
        "    code_execution_config={\n",
        "        \"use_docker\": False\n",
        "    },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n",
        ")\n",
        "\n",
        "# Ask the question with an image\n",
        "user_proxy.initiate_chat(\n",
        "    image_agent,\n",
        "    message=\"\"\"What's the breed of this dog?\n",
        "<img https://th.bing.com/th/id/R.422068ce8af4e15b0634fe2540adea7a?rik=y4OcXBE%2fqutDOw&pid=ImgRaw&r=0>.\"\"\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f60521d",
      "metadata": {
        "id": "3f60521d"
      },
      "source": [
        "Now, input another image, and ask a followup question.\n",
        "\n",
        "![](https://th.bing.com/th/id/OIP.29Mi2kJmcHHyQVGe_0NG7QHaEo?pid=ImgDet&rs=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "73a2b234",
      "metadata": {
        "scrolled": false,
        "id": "73a2b234",
        "outputId": "e8ef02ad-9c4f-4e9d-bdf1-a2b1cef26c7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User_proxy (to image-explainer):\n",
            "\n",
            "What is this breed?\n",
            "<image>\n",
            "\n",
            "Among the breeds, which one barks less?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "image-explainer (to User_proxy):\n",
            "\n",
            "This dog looks like a Siberian Husky. \n",
            "\n",
            "When it comes to barking, Siberian Huskies are known to bark less than many other breeds, but they are quite vocal and may howl or \"talk\" more. Goldendoodles, on the other hand, can vary depending on their genetic makeup, but they are generally not excessive barkers. Both breeds have unique vocal behaviors.\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Ask the question with an image\n",
        "user_proxy.send(\n",
        "    message=\"\"\"What is this breed?\n",
        "<img https://th.bing.com/th/id/OIP.29Mi2kJmcHHyQVGe_0NG7QHaEo?pid=ImgDet&rs=1>\n",
        "\n",
        "Among the breeds, which one barks less?\"\"\",\n",
        "    recipient=image_agent,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c40d0eb",
      "metadata": {
        "id": "0c40d0eb"
      },
      "source": [
        "<a id=\"app-2\"></a>\n",
        "## Application 2: Figure Creator\n",
        "\n",
        "Here, we define a `FigureCreator` agent, which contains three child agents: commander, coder, and critics.\n",
        "\n",
        "- Commander: interacts with users, runs code, and coordinates the flow between the coder and critics.\n",
        "- Coder: writes code for visualization.\n",
        "- Critics: LMM-based agent that provides comments and feedback on the generated image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "1e057758",
      "metadata": {
        "id": "1e057758"
      },
      "outputs": [],
      "source": [
        "working_dir = \"tmp/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "e8eca993",
      "metadata": {
        "id": "e8eca993"
      },
      "outputs": [],
      "source": [
        "class FigureCreator(ConversableAgent):\n",
        "    def __init__(self, n_iters=2, **kwargs):\n",
        "        \"\"\"\n",
        "        Initializes a FigureCreator instance.\n",
        "\n",
        "        This agent facilitates the creation of visualizations through a collaborative effort among its child agents: commander, coder, and critics.\n",
        "\n",
        "        Parameters:\n",
        "            - n_iters (int, optional): The number of \"improvement\" iterations to run. Defaults to 2.\n",
        "            - **kwargs: keyword arguments for the parent AssistantAgent.\n",
        "        \"\"\"\n",
        "        super().__init__(**kwargs)\n",
        "        self.register_reply([Agent, None], reply_func=FigureCreator._reply_user, position=0)\n",
        "        self._n_iters = n_iters\n",
        "\n",
        "    def _reply_user(self, messages=None, sender=None, config=None):\n",
        "        if all((messages is None, sender is None)):\n",
        "            error_msg = f\"Either {messages=} or {sender=} must be provided.\"\n",
        "            logger.error(error_msg)  # noqa: F821\n",
        "            raise AssertionError(error_msg)\n",
        "        if messages is None:\n",
        "            messages = self._oai_messages[sender]\n",
        "\n",
        "        user_question = messages[-1][\"content\"]\n",
        "\n",
        "        ### Define the agents\n",
        "        commander = AssistantAgent(\n",
        "            name=\"Commander\",\n",
        "            human_input_mode=\"NEVER\",\n",
        "            max_consecutive_auto_reply=10,\n",
        "            system_message=\"Help me run the code, and tell other agents it is in the <img result.jpg> file location.\",\n",
        "            is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
        "            code_execution_config={\"last_n_messages\": 3, \"work_dir\": working_dir, \"use_docker\": False},\n",
        "            llm_config=self.llm_config,\n",
        "        )\n",
        "\n",
        "        critics = MultimodalConversableAgent(\n",
        "            name=\"Critics\",\n",
        "            system_message=\"\"\"Criticize the input figure. How to replot the figure so it will be better? Find bugs and issues for the figure.\n",
        "            Pay attention to the color, format, and presentation. Keep in mind of the reader-friendliness.\n",
        "            If you think the figures is good enough, then simply say NO_ISSUES\"\"\",\n",
        "            llm_config={\"config_list\": config_list_4v, \"max_tokens\": 300},\n",
        "            human_input_mode=\"NEVER\",\n",
        "            max_consecutive_auto_reply=1,\n",
        "            #     use_docker=False,\n",
        "        )\n",
        "\n",
        "        coder = AssistantAgent(\n",
        "            name=\"Coder\",\n",
        "            llm_config=self.llm_config,\n",
        "        )\n",
        "\n",
        "        coder.update_system_message(\n",
        "            coder.system_message\n",
        "            + \"ALWAYS save the figure in `result.jpg` file. Tell other agents it is in the <img result.jpg> file location.\"\n",
        "        )\n",
        "\n",
        "        # Data flow begins\n",
        "        commander.initiate_chat(coder, message=user_question)\n",
        "        img = Image.open(os.path.join(working_dir, \"result.jpg\"))\n",
        "        plt.imshow(img)\n",
        "        plt.axis(\"off\")  # Hide the axes\n",
        "        plt.show()\n",
        "\n",
        "        for i in range(self._n_iters):\n",
        "            commander.send(\n",
        "                message=f\"Improve <img {os.path.join(working_dir, 'result.jpg')}>\",\n",
        "                recipient=critics,\n",
        "                request_reply=True,\n",
        "            )\n",
        "\n",
        "            feedback = commander._oai_messages[critics][-1][\"content\"]\n",
        "            if feedback.find(\"NO_ISSUES\") >= 0:\n",
        "                break\n",
        "            commander.send(\n",
        "                message=\"Here is the feedback to your figure. Please improve! Save the result to `result.jpg`\\n\"\n",
        "                + feedback,\n",
        "                recipient=coder,\n",
        "                request_reply=True,\n",
        "            )\n",
        "            img = Image.open(os.path.join(working_dir, \"result.jpg\"))\n",
        "            plt.imshow(img)\n",
        "            plt.axis(\"off\")  # Hide the axes\n",
        "            plt.show()\n",
        "\n",
        "        return True, os.path.join(working_dir, \"result.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "977b9017",
      "metadata": {
        "scrolled": false,
        "id": "977b9017"
      },
      "outputs": [],
      "source": [
        "creator = FigureCreator(name=\"Figure Creator~\", llm_config=gpt4_llm_config)\n",
        "\n",
        "user_proxy = autogen.UserProxyAgent(\n",
        "    name=\"User\", human_input_mode=\"NEVER\", max_consecutive_auto_reply=0, code_execution_config={\"use_docker\": False}\n",
        ")\n",
        "\n",
        "user_proxy.initiate_chat(\n",
        "    creator,\n",
        "    message=\"\"\"\n",
        "Plot a figure by using the data from:\n",
        "https://raw.githubusercontent.com/vega/vega/main/docs/data/seattle-weather.csv\n",
        "\n",
        "I want to show both temperature high and low.\n",
        "\"\"\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a95d87c2",
      "metadata": {
        "id": "a95d87c2"
      },
      "source": [
        "## Vision Capability: Group Chat Example with Multimodal Agent\n",
        "\n",
        "We recommend using VisionCapability for group chat managers so that it can organize and understand images better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56bd5742",
      "metadata": {
        "scrolled": false,
        "id": "56bd5742"
      },
      "outputs": [],
      "source": [
        "agent1 = MultimodalConversableAgent(\n",
        "    name=\"image-explainer-1\",\n",
        "    max_consecutive_auto_reply=10,\n",
        "    llm_config={\"config_list\": config_list_4v, \"temperature\": 0.5, \"max_tokens\": 300},\n",
        "    system_message=\"Your image description is poetic and engaging.\",\n",
        ")\n",
        "agent2 = MultimodalConversableAgent(\n",
        "    name=\"image-explainer-2\",\n",
        "    max_consecutive_auto_reply=10,\n",
        "    llm_config={\"config_list\": config_list_4v, \"temperature\": 0.5, \"max_tokens\": 300},\n",
        "    system_message=\"Your image description is factual and to the point.\",\n",
        ")\n",
        "\n",
        "user_proxy = autogen.UserProxyAgent(\n",
        "    name=\"User_proxy\",\n",
        "    system_message=\"Desribe image for me.\",\n",
        "    human_input_mode=\"TERMINATE\",  # Try between ALWAYS, NEVER, and TERMINATE\n",
        "    max_consecutive_auto_reply=10,\n",
        "    code_execution_config={\n",
        "        \"use_docker\": False\n",
        "    },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n",
        ")\n",
        "\n",
        "# We set max_round to 5\n",
        "groupchat = autogen.GroupChat(agents=[agent1, agent2, user_proxy], messages=[], max_round=5)\n",
        "\n",
        "vision_capability = VisionCapability(lmm_config={\"config_list\": config_list_4v, \"temperature\": 0.5, \"max_tokens\": 300})\n",
        "group_chat_manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=gpt4_llm_config)\n",
        "vision_capability.add_to_agent(group_chat_manager)\n",
        "\n",
        "rst = user_proxy.initiate_chat(\n",
        "    group_chat_manager,\n",
        "    message=\"\"\"Write a poet for my image:\n",
        "                        <img https://th.bing.com/th/id/R.422068ce8af4e15b0634fe2540adea7a?rik=y4OcXBE%2fqutDOw&pid=ImgRaw&r=0>.\"\"\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d69b6c81",
      "metadata": {
        "id": "d69b6c81"
      },
      "source": [
        "## Behavior with and without VisionCapability for Agents\n",
        "\n",
        "\n",
        "Here, we show the behavior of an agent with and without VisionCapability. We use the same image and question as in the previous example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "aae1167f",
      "metadata": {
        "id": "aae1167f"
      },
      "outputs": [],
      "source": [
        "agent_no_vision = AssistantAgent(name=\"Regular LLM Agent\", llm_config=gpt4_llm_config)\n",
        "\n",
        "agent_with_vision = AssistantAgent(name=\"Regular LLM Agent with Vision Capability\", llm_config=gpt4_llm_config)\n",
        "vision_capability = VisionCapability(lmm_config={\"config_list\": config_list_4v, \"temperature\": 0.5, \"max_tokens\": 300})\n",
        "vision_capability.add_to_agent(agent_with_vision)\n",
        "\n",
        "\n",
        "user = UserProxyAgent(\n",
        "    name=\"User\",\n",
        "    human_input_mode=\"NEVER\",\n",
        "    max_consecutive_auto_reply=0,\n",
        "    code_execution_config={\"use_docker\": False},\n",
        ")\n",
        "\n",
        "message = \"\"\"Write a poet for my image:\n",
        "                        <img https://th.bing.com/th/id/R.422068ce8af4e15b0634fe2540adea7a?rik=y4OcXBE%2fqutDOw&pid=ImgRaw&r=0>.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9e81b7e",
      "metadata": {
        "id": "c9e81b7e"
      },
      "outputs": [],
      "source": [
        "user.send(message=message, recipient=agent_no_vision, request_reply=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8720820f",
      "metadata": {
        "id": "8720820f"
      },
      "outputs": [],
      "source": [
        "user.send(message=message, recipient=agent_with_vision, request_reply=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba4d3753",
      "metadata": {
        "id": "ba4d3753"
      },
      "source": [
        "## Custom Caption Function for Vision Capability\n",
        "\n",
        "In many use cases, we can use a custom function within the Vision Capability to transcribe an image into a caption.\n",
        "\n",
        "For instance, we can use rule-based algorithm or other models to detect the color, box, and other components inside the image.\n",
        "\n",
        "The custom model should take a path to the image and return a string caption.\n",
        "\n",
        "In the example below, the Vision Capability will call LMM to get caption and also call the custom function to get more information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bff55c81",
      "metadata": {
        "id": "bff55c81"
      },
      "outputs": [],
      "source": [
        "def my_description(image_url: str, image_data: Image = None, lmm_client: object = None) -> str:\n",
        "    \"\"\"\n",
        "    This function takes an image URL and returns the description.\n",
        "\n",
        "    Parameters:\n",
        "        - image_url (str): The URL of the image.\n",
        "        - image_data (PIL.Image): The image data.\n",
        "        - lmm_client (object): The LLM client object.\n",
        "\n",
        "    Returns:\n",
        "        - str: A description of the color of the image.\n",
        "    \"\"\"\n",
        "    # Print the arguments for illustration purpose\n",
        "    print(\"image_url\", image_url)\n",
        "    print(\"image_data\", image_data)\n",
        "    print(\"lmm_client\", lmm_client)\n",
        "\n",
        "    img_uri = pil_to_data_uri(image_data)  # cast data into URI (str) format for API call\n",
        "    lmm_out = lmm_client.create(\n",
        "        context=None,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": \"Describe this image in 10 words.\"},\n",
        "                    {\n",
        "                        \"type\": \"image_url\",\n",
        "                        \"image_url\": {\n",
        "                            \"url\": img_uri,\n",
        "                        },\n",
        "                    },\n",
        "                ],\n",
        "            }\n",
        "        ],\n",
        "    )\n",
        "    description = lmm_out.choices[0].message.content\n",
        "    description = content_str(description)\n",
        "\n",
        "    # Convert the image into an array of pixels.\n",
        "    pixels = np.array(image_data)\n",
        "\n",
        "    # Calculate the average color.\n",
        "    avg_color_per_row = np.mean(pixels, axis=0)\n",
        "    avg_color = np.mean(avg_color_per_row, axis=0)\n",
        "    avg_color = avg_color.astype(int)  # Convert to integer for color values\n",
        "\n",
        "    # Format the average color as a string description.\n",
        "    caption = f\"\"\"The image is from {image_url}\n",
        "    It is about: {description}\n",
        "    The average color of the image is RGB:\n",
        "        ({avg_color[0]}, {avg_color[1]}, {avg_color[2]})\"\"\"\n",
        "\n",
        "    print(caption)  # For illustration purpose\n",
        "\n",
        "    return caption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbafea62",
      "metadata": {
        "id": "cbafea62"
      },
      "outputs": [],
      "source": [
        "agent_with_vision_and_func = AssistantAgent(\n",
        "    name=\"Regular LLM Agent with Custom Func and LMM\", llm_config=gpt4_llm_config\n",
        ")\n",
        "\n",
        "vision_capability_with_func = VisionCapability(\n",
        "    lmm_config={\"config_list\": config_list_4v, \"temperature\": 0.5, \"max_tokens\": 300},\n",
        "    custom_caption_func=my_description,\n",
        ")\n",
        "vision_capability_with_func.add_to_agent(agent_with_vision_and_func)\n",
        "\n",
        "user.send(message=message, recipient=agent_with_vision_and_func, request_reply=True)"
      ]
    }
  ],
  "metadata": {
    "front_matter": {
      "description": "In AutoGen, leveraging multimodal models can be done through two different methodologies: MultimodalConversableAgent and VisionCapability.",
      "tags": [
        "multimodal",
        "gpt-4v"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}