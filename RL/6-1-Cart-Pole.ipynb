{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMalydDht6hTWoltC/S+P8f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSJeong-me/AI-Innovation-2024/blob/main/RL/6-1-Cart-Pole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZGyFeo4WWGJ",
        "outputId": "a8177b1c-7a77-44c4-ef38-1d3d756bb501"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "1BNUL5eTWXfF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Create the CartPole environment\n",
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "\n",
        "# Reset the environment with a custom seed and custom initial position range\n",
        "obs, info = env.reset(seed=123, options={\"low\": -0.1, \"high\": 0.1})\n",
        "\n",
        "# Extract the initial observation (state)\n",
        "state = obs\n",
        "print(f\"Initial state: {state}\")\n",
        "\n",
        "# Run the environment for a few steps to see how it evolves\n",
        "n_steps = 100  # Number of steps to simulate\n",
        "frames = []  # To store frames for rendering\n",
        "\n",
        "for step in range(n_steps):\n",
        "    frames.append(env.render())  # Capture the frame of the environment\n",
        "\n",
        "    action = env.action_space.sample()  # Randomly select an action (left or right)\n",
        "    state, reward, done, truncated, info = env.step(action)  # Take a step\n",
        "\n",
        "    # Print the state, action, and reward for each step\n",
        "    print(f\"Step: {step}, Action: {action}, State: {state}, Reward: {reward}\")\n",
        "\n",
        "    if done:\n",
        "        print(\"Pole fell! Resetting environment.\")\n",
        "        obs, info = env.reset(seed=123, options={\"low\": -0.1, \"high\": 0.1})\n",
        "        state = obs\n",
        "\n",
        "# Render the environment frames to visualize the CartPole problem\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejtXfLNnwWhR",
        "outputId": "db4d72a8-23ab-4971-9597-47dba222d662"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial state: [ 0.03647037 -0.0892358  -0.05592803 -0.06312564]\n",
            "Step: 0, Action: 0, State: [ 0.03468566 -0.28351313 -0.05719054  0.21140045], Reward: 1.0\n",
            "Step: 1, Action: 1, State: [ 0.02901539 -0.08762209 -0.05296253 -0.09876057], Reward: 1.0\n",
            "Step: 2, Action: 1, State: [ 0.02726295  0.10821734 -0.05493774 -0.4076715 ], Reward: 1.0\n",
            "Step: 3, Action: 1, State: [ 0.0294273   0.30407348 -0.06309117 -0.71715605], Reward: 1.0\n",
            "Step: 4, Action: 1, State: [ 0.03550877  0.50000924 -0.07743429 -1.0290115 ], Reward: 1.0\n",
            "Step: 5, Action: 1, State: [ 0.04550895  0.6960715  -0.09801452 -1.3449665 ], Reward: 1.0\n",
            "Step: 6, Action: 0, State: [ 0.05943038  0.5023093  -0.12491385 -1.0844884 ], Reward: 1.0\n",
            "Step: 7, Action: 1, State: [ 0.06947657  0.69883794 -0.14660361 -1.4136137 ], Reward: 1.0\n",
            "Step: 8, Action: 1, State: [ 0.08345333  0.89544064 -0.17487589 -1.7483015 ], Reward: 1.0\n",
            "Step: 9, Action: 0, State: [ 0.10136214  0.702684   -0.20984192 -1.5147282 ], Reward: 1.0\n",
            "Pole fell! Resetting environment.\n",
            "Step: 10, Action: 0, State: [ 0.03468566 -0.28351313 -0.05719054  0.21140045], Reward: 1.0\n",
            "Step: 11, Action: 0, State: [ 0.02901539 -0.4777727  -0.05296253  0.4855086 ], Reward: 1.0\n",
            "Step: 12, Action: 0, State: [ 0.01945994 -0.6721088  -0.04325236  0.76104033], Reward: 1.0\n",
            "Step: 13, Action: 0, State: [ 0.00601776 -0.8666091  -0.02803155  1.0398057 ], Reward: 1.0\n",
            "Step: 14, Action: 1, State: [-0.01131442 -0.6711261  -0.00723544  0.7384562 ], Reward: 1.0\n",
            "Step: 15, Action: 0, State: [-0.02473694 -0.86614746  0.00753368  1.0288533 ], Reward: 1.0\n",
            "Step: 16, Action: 0, State: [-0.04205989 -1.0613688   0.02811075  1.323892  ], Reward: 1.0\n",
            "Step: 17, Action: 0, State: [-0.06328727 -1.2568344   0.05458859  1.6252378 ], Reward: 1.0\n",
            "Step: 18, Action: 1, State: [-0.08842395 -1.0623955   0.08709335  1.3500551 ], Reward: 1.0\n",
            "Step: 19, Action: 0, State: [-0.10967187 -1.258497    0.11409445  1.6686654 ], Reward: 1.0\n",
            "Step: 20, Action: 0, State: [-0.1348418  -1.4547457   0.14746776  1.9945955 ], Reward: 1.0\n",
            "Step: 21, Action: 0, State: [-0.16393672 -1.6510715   0.18735966  2.3290865 ], Reward: 1.0\n",
            "Step: 22, Action: 0, State: [-0.19695815 -1.847334    0.2339414   2.6730905 ], Reward: 1.0\n",
            "Pole fell! Resetting environment.\n",
            "Step: 23, Action: 1, State: [ 0.03468566  0.10664157 -0.05719054 -0.37291655], Reward: 1.0\n",
            "Step: 24, Action: 1, State: [ 0.03681849  0.30252734 -0.06464887 -0.6830697 ], Reward: 1.0\n",
            "Step: 25, Action: 0, State: [ 0.04286904  0.10835987 -0.07831027 -0.4114204 ], Reward: 1.0\n",
            "Step: 26, Action: 1, State: [ 0.04503623  0.30449957 -0.08653867 -0.727728  ], Reward: 1.0\n",
            "Step: 27, Action: 1, State: [ 0.05112622  0.5007045  -0.10109323 -1.0463448 ], Reward: 1.0\n",
            "Step: 28, Action: 0, State: [ 0.06114031  0.30705905 -0.12202013 -0.78703034], Reward: 1.0\n",
            "Step: 29, Action: 1, State: [ 0.06728149  0.503627   -0.13776073 -1.1154749 ], Reward: 1.0\n",
            "Step: 30, Action: 0, State: [ 0.07735404  0.31055552 -0.16007023 -0.8689851 ], Reward: 1.0\n",
            "Step: 31, Action: 0, State: [ 0.08356515  0.11793084 -0.17744993 -0.6306018 ], Reward: 1.0\n",
            "Step: 32, Action: 1, State: [ 0.08592376  0.31502676 -0.19006197 -0.9735001 ], Reward: 1.0\n",
            "Step: 33, Action: 0, State: [ 0.0922243   0.12289248 -0.20953198 -0.74603087], Reward: 1.0\n",
            "Pole fell! Resetting environment.\n",
            "Step: 34, Action: 0, State: [ 0.03468566 -0.28351313 -0.05719054  0.21140045], Reward: 1.0\n",
            "Step: 35, Action: 1, State: [ 0.02901539 -0.08762209 -0.05296253 -0.09876057], Reward: 1.0\n",
            "Step: 36, Action: 1, State: [ 0.02726295  0.10821734 -0.05493774 -0.4076715 ], Reward: 1.0\n",
            "Step: 37, Action: 0, State: [ 0.0294273  -0.08608434 -0.06309117 -0.13280226], Reward: 1.0\n",
            "Step: 38, Action: 1, State: [ 0.02770561  0.10988194 -0.06574722 -0.44470337], Reward: 1.0\n",
            "Step: 39, Action: 0, State: [ 0.02990325 -0.08425117 -0.07464128 -0.1734486 ], Reward: 1.0\n",
            "Step: 40, Action: 1, State: [ 0.02821823  0.11185528 -0.07811026 -0.4887134 ], Reward: 1.0\n",
            "Step: 41, Action: 1, State: [ 0.03045533  0.30798733 -0.08788452 -0.80495554], Reward: 1.0\n",
            "Step: 42, Action: 0, State: [ 0.03661508  0.11417307 -0.10398363 -0.54116094], Reward: 1.0\n",
            "Step: 43, Action: 1, State: [ 0.03889854  0.31059107 -0.11480685 -0.86471266], Reward: 1.0\n",
            "Step: 44, Action: 0, State: [ 0.04511036  0.11720332 -0.1321011  -0.6102198 ], Reward: 1.0\n",
            "Step: 45, Action: 1, State: [ 0.04745443  0.31390026 -0.1443055  -0.9414194 ], Reward: 1.0\n",
            "Step: 46, Action: 0, State: [ 0.05373244  0.12098683 -0.16313389 -0.6973357 ], Reward: 1.0\n",
            "Step: 47, Action: 0, State: [ 0.05615217 -0.07154229 -0.1770806  -0.4601252 ], Reward: 1.0\n",
            "Step: 48, Action: 1, State: [ 0.05472133  0.12558253 -0.18628311 -0.8029786 ], Reward: 1.0\n",
            "Step: 49, Action: 0, State: [ 0.05723298 -0.06656346 -0.20234267 -0.57419693], Reward: 1.0\n",
            "Step: 50, Action: 1, State: [ 0.05590171  0.13073413 -0.21382661 -0.9231892 ], Reward: 1.0\n",
            "Pole fell! Resetting environment.\n",
            "Step: 51, Action: 1, State: [ 0.03468566  0.10664157 -0.05719054 -0.37291655], Reward: 1.0\n",
            "Step: 52, Action: 1, State: [ 0.03681849  0.30252734 -0.06464887 -0.6830697 ], Reward: 1.0\n",
            "Step: 53, Action: 1, State: [ 0.04286904  0.4984846  -0.07831027 -0.9953851 ], Reward: 1.0\n",
            "Step: 54, Action: 0, State: [ 0.05283873  0.3044924  -0.09821796 -0.7282883 ], Reward: 1.0\n",
            "Step: 55, Action: 0, State: [ 0.05892858  0.11085552 -0.11278373 -0.46806246], Reward: 1.0\n",
            "Step: 56, Action: 0, State: [ 0.06114569 -0.08250748 -0.12214498 -0.21294887], Reward: 1.0\n",
            "Step: 57, Action: 1, State: [ 0.05949554  0.11412986 -0.12640396 -0.5415287 ], Reward: 1.0\n",
            "Step: 58, Action: 1, State: [ 0.06177814  0.3107806  -0.13723452 -0.87121534], Reward: 1.0\n",
            "Step: 59, Action: 1, State: [ 0.06799375  0.50747544 -0.15465884 -1.203704  ], Reward: 1.0\n",
            "Step: 60, Action: 1, State: [ 0.07814325  0.7042209  -0.17873292 -1.5405884 ], Reward: 1.0\n",
            "Step: 61, Action: 0, State: [ 0.09222768  0.5116417  -0.20954469 -1.3085895 ], Reward: 1.0\n",
            "Pole fell! Resetting environment.\n",
            "Step: 62, Action: 0, State: [ 0.03468566 -0.28351313 -0.05719054  0.21140045], Reward: 1.0\n",
            "Step: 63, Action: 0, State: [ 0.02901539 -0.4777727  -0.05296253  0.4855086 ], Reward: 1.0\n",
            "Step: 64, Action: 1, State: [ 0.01945994 -0.28194496 -0.04325236  0.1766151 ], Reward: 1.0\n",
            "Step: 65, Action: 1, State: [ 0.01382104 -0.08623157 -0.03972005 -0.12939265], Reward: 1.0\n",
            "Step: 66, Action: 0, State: [ 0.01209641 -0.28076267 -0.04230791  0.15049924], Reward: 1.0\n",
            "Step: 67, Action: 1, State: [ 0.00648116 -0.08506124 -0.03929792 -0.15522504], Reward: 1.0\n",
            "Step: 68, Action: 0, State: [ 0.00477993 -0.27959913 -0.04240242  0.12480589], Reward: 1.0\n",
            "Step: 69, Action: 1, State: [-0.00081205 -0.08389618 -0.03990631 -0.18094726], Reward: 1.0\n",
            "Step: 70, Action: 1, State: [-0.00248998  0.11177342 -0.04352525 -0.48594734], Reward: 1.0\n",
            "Step: 71, Action: 1, State: [-2.5450741e-04  3.0748165e-01 -5.3244200e-02 -7.9202408e-01], Reward: 1.0\n",
            "Step: 72, Action: 1, State: [ 0.00589513  0.5032926  -0.06908468 -1.1009706 ], Reward: 1.0\n",
            "Step: 73, Action: 1, State: [ 0.01596098  0.69925225 -0.09110409 -1.4145037 ], Reward: 1.0\n",
            "Step: 74, Action: 1, State: [ 0.02994602  0.89537716 -0.11939417 -1.7342186 ], Reward: 1.0\n",
            "Step: 75, Action: 0, State: [ 0.04785357  0.70180273 -0.15407854 -1.4809426 ], Reward: 1.0\n",
            "Step: 76, Action: 1, State: [ 0.06188962  0.8984324  -0.18369739 -1.817513  ], Reward: 1.0\n",
            "Step: 77, Action: 0, State: [ 0.07985827  0.7057677  -0.22004765 -1.5870821 ], Reward: 1.0\n",
            "Pole fell! Resetting environment.\n",
            "Step: 78, Action: 0, State: [ 0.03468566 -0.28351313 -0.05719054  0.21140045], Reward: 1.0\n",
            "Step: 79, Action: 0, State: [ 0.02901539 -0.4777727  -0.05296253  0.4855086 ], Reward: 1.0\n",
            "Step: 80, Action: 1, State: [ 0.01945994 -0.28194496 -0.04325236  0.1766151 ], Reward: 1.0\n",
            "Step: 81, Action: 0, State: [ 0.01382104 -0.4764221  -0.03972005  0.45534575], Reward: 1.0\n",
            "Step: 82, Action: 1, State: [ 0.0042926  -0.28076172 -0.03061314  0.15041204], Reward: 1.0\n",
            "Step: 83, Action: 0, State: [-0.00132264 -0.47543222 -0.0276049   0.43328214], Reward: 1.0\n",
            "Step: 84, Action: 1, State: [-0.01083128 -0.27993053 -0.01893926  0.13202652], Reward: 1.0\n",
            "Step: 85, Action: 1, State: [-0.01642989 -0.08454248 -0.01629873 -0.16657081], Reward: 1.0\n",
            "Step: 86, Action: 1, State: [-0.01812074  0.11080894 -0.01963014 -0.46435064], Reward: 1.0\n",
            "Step: 87, Action: 1, State: [-0.01590456  0.3062027  -0.02891715 -0.7631557 ], Reward: 1.0\n",
            "Step: 88, Action: 0, State: [-0.00978051  0.11149072 -0.04418027 -0.47971028], Reward: 1.0\n",
            "Step: 89, Action: 1, State: [-0.00755069  0.3072076  -0.05377448 -0.7859839 ], Reward: 1.0\n",
            "Step: 90, Action: 1, State: [-0.00140654  0.5030255  -0.06949415 -1.0950882 ], Reward: 1.0\n",
            "Step: 91, Action: 1, State: [ 0.00865397  0.6989905  -0.09139591 -1.4087411 ], Reward: 1.0\n",
            "Step: 92, Action: 0, State: [ 0.02263378  0.5051135  -0.11957074 -1.1459724 ], Reward: 1.0\n",
            "Step: 93, Action: 0, State: [ 0.03273605  0.31173855 -0.1424902  -0.8930511 ], Reward: 1.0\n",
            "Step: 94, Action: 1, State: [ 0.03897082  0.50847566 -0.1603512  -1.2269166 ], Reward: 1.0\n",
            "Step: 95, Action: 1, State: [ 0.04914033  0.7052565  -0.18488954 -1.5652426 ], Reward: 1.0\n",
            "Step: 96, Action: 1, State: [ 0.06324546  0.902044   -0.21619439 -1.9094412 ], Reward: 1.0\n",
            "Pole fell! Resetting environment.\n",
            "Step: 97, Action: 0, State: [ 0.03468566 -0.28351313 -0.05719054  0.21140045], Reward: 1.0\n",
            "Step: 98, Action: 0, State: [ 0.02901539 -0.4777727  -0.05296253  0.4855086 ], Reward: 1.0\n",
            "Step: 99, Action: 1, State: [ 0.01945994 -0.28194496 -0.04325236  0.1766151 ], Reward: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Visualize a sample frame\n",
        "plt.imshow(frames[0])\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "NJTXjezQwY1u",
        "outputId": "a4c3b444-2647-41c3-b3fc-72a8a260d37f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "Environment Creation: The code initializes the CartPole-v1 environment.\n",
        "Reset with Custom Options: The environment is reset with a custom seed and initial position bounds for the state.\n",
        "Simulation Loop: The environment is run for 100 steps, with actions randomly chosen using env.action_space.sample().\n",
        "Frame Capture: Each frame is captured for rendering purposes.\n",
        "Rendering and Plotting: At the end, a sample frame is displayed using matplotlib.\n",
        "This code allows you to visualize the behavior of the CartPole over time."
      ],
      "metadata": {
        "id": "mDlb8HIsWnIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import imageio\n",
        "import os\n"
      ],
      "metadata": {
        "id": "xICBgt0pWy1H"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create the CartPole environment\n",
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "\n",
        "# Reset the environment with a custom seed and custom initial position range\n",
        "obs, info = env.reset(seed=123, options={\"low\": -0.1, \"high\": 0.1})\n",
        "\n",
        "# Extract the initial observation (state)\n",
        "state = obs\n",
        "print(f\"Initial state: {state}\")\n",
        "\n",
        "# Run the environment for a few steps to see how it evolves\n",
        "n_steps = 100  # Number of steps to simulate\n",
        "frames = []  # To store frames for rendering\n",
        "\n",
        "# Directory to save the video\n",
        "video_dir = \"cartpole_video\"\n",
        "if not os.path.exists(video_dir):\n",
        "    os.makedirs(video_dir)\n",
        "\n",
        "# Loop to simulate the environment and collect frames\n",
        "for step in range(n_steps):\n",
        "    frames.append(env.render())  # Capture the frame of the environment\n",
        "\n",
        "    action = env.action_space.sample()  # Randomly select an action (left or right)\n",
        "    state, reward, done, truncated, info = env.step(action)  # Take a step\n",
        "\n",
        "    # Print the state, action, and reward for each step\n",
        "    print(f\"Step: {step}, Action: {action}, State: {state}, Reward: {reward}\")\n",
        "\n",
        "    if done:\n",
        "        print(\"Pole fell! Resetting environment.\")\n",
        "        obs, info = env.reset(seed=123, options={\"low\": -0.1, \"high\": 0.1})\n",
        "        state = obs\n",
        "\n",
        "# Save frames as a video using imageio\n",
        "video_file_path = os.path.join(video_dir, \"cartpole_simulation.mp4\")\n",
        "imageio.mimsave(video_file_path, frames, fps=30)\n",
        "\n",
        "# Close the environment\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2Me6zXWwbjW",
        "outputId": "31a2f6e5-2d7e-4ea8-8120-f57a568b2642"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial state: [ 0.03647037 -0.0892358  -0.05592803 -0.06312564]\n",
            "Step: 0, Action: 1, State: [ 0.03468566  0.10664157 -0.05719054 -0.37291655], Reward: 1.0\n",
            "Step: 1, Action: 1, State: [ 0.03681849  0.30252734 -0.06464887 -0.6830697 ], Reward: 1.0\n",
            "Step: 2, Action: 1, State: [ 0.04286904  0.4984846  -0.07831027 -0.9953851 ], Reward: 1.0\n",
            "Step: 3, Action: 1, State: [ 0.05283873  0.69456166 -0.09821796 -1.311599  ], Reward: 1.0\n",
            "Step: 4, Action: 1, State: [ 0.06672996  0.8907805  -0.12444995 -1.6333385 ], Reward: 1.0\n",
            "Step: 5, Action: 1, State: [ 0.08454557  1.0871245  -0.15711671 -1.9620705 ], Reward: 1.0\n",
            "Step: 6, Action: 1, State: [ 0.10628806  1.2835233  -0.19635813 -2.2990425 ], Reward: 1.0\n",
            "Step: 7, Action: 1, State: [ 0.13195853  1.4798366  -0.24233897 -2.645213  ], Reward: 1.0\n",
            "Pole fell! Resetting environment.\n",
            "Step: 8, Action: 0, State: [ 0.03468566 -0.28351313 -0.05719054  0.21140045], Reward: 1.0\n",
            "Step: 9, Action: 0, State: [ 0.02901539 -0.4777727  -0.05296253  0.4855086 ], Reward: 1.0\n",
            "Step: 10, Action: 0, State: [ 0.01945994 -0.6721088  -0.04325236  0.76104033], Reward: 1.0\n",
            "Step: 11, Action: 1, State: [ 0.00601776 -0.47641858 -0.02803155  0.45506722], Reward: 1.0\n",
            "Step: 12, Action: 1, State: [-0.00351061 -0.2809117  -0.01893021  0.15368195], Reward: 1.0\n",
            "Step: 13, Action: 1, State: [-0.00912884 -0.08552391 -0.01585657 -0.1449124 ], Reward: 1.0\n",
            "Step: 14, Action: 1, State: [-0.01083932  0.10982149 -0.01875482 -0.4425553 ], Reward: 1.0\n",
            "Step: 15, Action: 1, State: [-0.00864289  0.30520374 -0.02760592 -0.7410907 ], Reward: 1.0\n",
            "Step: 16, Action: 0, State: [-0.00253882  0.11047356 -0.04242774 -0.45722187], Reward: 1.0\n",
            "Step: 17, Action: 0, State: [-0.00032935 -0.08402369 -0.05157217 -0.17820856], Reward: 1.0\n",
            "Step: 18, Action: 0, State: [-0.00200982 -0.27837116 -0.05513635  0.09776953], Reward: 1.0\n",
            "Step: 19, Action: 0, State: [-0.00757724 -0.4726613  -0.05318096  0.37256002], Reward: 1.0\n",
            "Step: 20, Action: 0, State: [-0.01703047 -0.666989   -0.04572976  0.6480117 ], Reward: 1.0\n",
            "Step: 21, Action: 0, State: [-0.03037025 -0.86144507 -0.03276952  0.9259509 ], Reward: 1.0\n",
            "Step: 22, Action: 1, State: [-0.04759915 -0.66589624 -0.0142505   0.6231527 ], Reward: 1.0\n",
            "Step: 23, Action: 0, State: [-0.06091708 -0.86081636 -0.00178745  0.9113136 ], Reward: 1.0\n",
            "Step: 24, Action: 1, State: [-0.0781334  -0.6656703   0.01643882  0.6180694 ], Reward: 1.0\n",
            "Step: 25, Action: 1, State: [-0.09144681 -0.47078174  0.02880021  0.33060896], Reward: 1.0\n",
            "Step: 26, Action: 0, State: [-0.10086244 -0.66630155  0.03541239  0.63223314], Reward: 1.0\n",
            "Step: 27, Action: 0, State: [-0.11418847 -0.8618992   0.04805705  0.93585473], Reward: 1.0\n",
            "Step: 28, Action: 1, State: [-0.13142645 -0.66745716  0.06677414  0.6586518 ], Reward: 1.0\n",
            "Step: 29, Action: 1, State: [-0.1447756  -0.47332504  0.07994718  0.38771954], Reward: 1.0\n",
            "Step: 30, Action: 1, State: [-0.1542421  -0.27942353  0.08770157  0.12127574], Reward: 1.0\n",
            "Step: 31, Action: 1, State: [-0.15983057 -0.08566044  0.09012709 -0.14250065], Reward: 1.0\n",
            "Step: 32, Action: 0, State: [-0.16154379 -0.28194982  0.08727708  0.1771999 ], Reward: 1.0\n",
            "Step: 33, Action: 0, State: [-0.16718277 -0.47820532  0.09082107  0.49608958], Reward: 1.0\n",
            "Step: 34, Action: 1, State: [-0.17674689 -0.28447357  0.10074287  0.2333543 ], Reward: 1.0\n",
            "Step: 35, Action: 1, State: [-0.18243636 -0.09092455  0.10540995 -0.02592886], Reward: 1.0\n",
            "Step: 36, Action: 1, State: [-0.18425484  0.10254019  0.10489137 -0.2835821 ], Reward: 1.0\n",
            "Step: 37, Action: 0, State: [-0.18220404 -0.09390926  0.09921973  0.04025409], Reward: 1.0\n",
            "Step: 38, Action: 1, State: [-0.18408222  0.09966024  0.10002481 -0.21954936], Reward: 1.0\n",
            "Step: 39, Action: 0, State: [-0.18208902 -0.09673867  0.09563383  0.1029348 ], Reward: 1.0\n",
            "Step: 40, Action: 0, State: [-0.1840238  -0.2930918   0.09769253  0.4241922 ], Reward: 1.0\n",
            "Step: 41, Action: 1, State: [-0.18988563 -0.09947968  0.10617637  0.16383466], Reward: 1.0\n",
            "Step: 42, Action: 0, State: [-0.19187522 -0.2959487   0.10945306  0.48803583], Reward: 1.0\n",
            "Step: 43, Action: 1, State: [-0.1977942  -0.10252745  0.11921377  0.23175511], Reward: 1.0\n",
            "Step: 44, Action: 1, State: [-0.19984475  0.09070719  0.12384888 -0.02107373], Reward: 1.0\n",
            "Step: 45, Action: 0, State: [-0.1980306  -0.10595318  0.12342741  0.30797592], Reward: 1.0\n",
            "Step: 46, Action: 0, State: [-0.20014967 -0.30259785  0.12958692  0.6368946 ], Reward: 1.0\n",
            "Step: 47, Action: 1, State: [-0.20620163 -0.10949844  0.14232482  0.38766605], Reward: 1.0\n",
            "Step: 48, Action: 0, State: [-0.20839159 -0.30632353  0.15007813  0.72162086], Reward: 1.0\n",
            "Step: 49, Action: 0, State: [-0.21451807 -0.5031677   0.16451055  1.0575256 ], Reward: 1.0\n",
            "Step: 50, Action: 1, State: [-0.22458142 -0.31056204  0.18566106  0.8206661 ], Reward: 1.0\n",
            "Step: 51, Action: 1, State: [-0.23079266 -0.1183998   0.2020744   0.5916477 ], Reward: 1.0\n",
            "Step: 52, Action: 0, State: [-0.23316066 -0.3156915   0.21390735  0.94056994], Reward: 1.0\n",
            "Pole fell! Resetting environment.\n",
            "Step: 53, Action: 1, State: [ 0.03468566  0.10664157 -0.05719054 -0.37291655], Reward: 1.0\n",
            "Step: 54, Action: 0, State: [ 0.03681849 -0.08762328 -0.06464887 -0.09880054], Reward: 1.0\n",
            "Step: 55, Action: 0, State: [ 0.03506602 -0.28176197 -0.06662488  0.17280564], Reward: 1.0\n",
            "Step: 56, Action: 0, State: [ 0.02943078 -0.47587025 -0.06316876  0.44374883], Reward: 1.0\n",
            "Step: 57, Action: 0, State: [ 0.01991338 -0.6700442  -0.05429379  0.71586955], Reward: 1.0\n",
            "Step: 58, Action: 1, State: [ 0.00651249 -0.47421443 -0.0399764   0.40660325], Reward: 1.0\n",
            "Step: 59, Action: 0, State: [-0.00297179 -0.66874737 -0.03184433  0.6864196 ], Reward: 1.0\n",
            "Step: 60, Action: 1, State: [-0.01634674 -0.4731982  -0.01811594  0.3838839 ], Reward: 1.0\n",
            "Step: 61, Action: 1, State: [-0.02581071 -0.27782378 -0.01043826  0.08554456], Reward: 1.0\n",
            "Step: 62, Action: 1, State: [-0.03136718 -0.08255378 -0.00872737 -0.21041329], Reward: 1.0\n",
            "Step: 63, Action: 1, State: [-0.03301826  0.11269186 -0.01293564 -0.5058364 ], Reward: 1.0\n",
            "Step: 64, Action: 0, State: [-0.03076442 -0.08224544 -0.02305237 -0.2172579 ], Reward: 1.0\n",
            "Step: 65, Action: 1, State: [-0.03240933  0.11319834 -0.02739753 -0.5171225 ], Reward: 1.0\n",
            "Step: 66, Action: 1, State: [-0.03014536  0.30869514 -0.03773998 -0.81831145], Reward: 1.0\n",
            "Step: 67, Action: 0, State: [-0.02397146  0.11410954 -0.05410621 -0.53773385], Reward: 1.0\n",
            "Step: 68, Action: 0, State: [-0.02168927 -0.08021164 -0.06486088 -0.2625781 ], Reward: 1.0\n",
            "Step: 69, Action: 1, State: [-0.0232935   0.11577331 -0.07011244 -0.5749931 ], Reward: 1.0\n",
            "Step: 70, Action: 1, State: [-0.02097804  0.31180447 -0.0816123  -0.8889136 ], Reward: 1.0\n",
            "Step: 71, Action: 0, State: [-0.01474195  0.11787924 -0.09939057 -0.62296134], Reward: 1.0\n",
            "Step: 72, Action: 1, State: [-0.01238436  0.3142382  -0.11184981 -0.9452189 ], Reward: 1.0\n",
            "Step: 73, Action: 1, State: [-0.0060996   0.51067454 -0.13075419 -1.2708476 ], Reward: 1.0\n",
            "Step: 74, Action: 1, State: [ 0.00411389  0.7072005  -0.15617113 -1.6014524 ], Reward: 1.0\n",
            "Step: 75, Action: 0, State: [ 0.0182579   0.5142344  -0.18820018 -1.3612537 ], Reward: 1.0\n",
            "Step: 76, Action: 1, State: [ 0.02854259  0.71114963 -0.21542525 -1.7064158 ], Reward: 1.0\n",
            "Pole fell! Resetting environment.\n",
            "Step: 77, Action: 1, State: [ 0.03468566  0.10664157 -0.05719054 -0.37291655], Reward: 1.0\n",
            "Step: 78, Action: 0, State: [ 0.03681849 -0.08762328 -0.06464887 -0.09880054], Reward: 1.0\n",
            "Step: 79, Action: 1, State: [ 0.03506602  0.10836278 -0.06662488 -0.41115904], Reward: 1.0\n",
            "Step: 80, Action: 1, State: [ 0.03723328  0.30436286 -0.07484806 -0.7240801 ], Reward: 1.0\n",
            "Step: 81, Action: 0, State: [ 0.04332054  0.11035147 -0.08932966 -0.4558626 ], Reward: 1.0\n",
            "Step: 82, Action: 1, State: [ 0.04552756  0.30661535 -0.09844691 -0.7753126 ], Reward: 1.0\n",
            "Step: 83, Action: 0, State: [ 0.05165987  0.1129754  -0.11395317 -0.51515573], Reward: 1.0\n",
            "Step: 84, Action: 1, State: [ 0.05391938  0.30950212 -0.12425628 -0.84146374], Reward: 1.0\n",
            "Step: 85, Action: 1, State: [ 0.06010942  0.5060813  -0.14108555 -1.1704965 ], Reward: 1.0\n",
            "Step: 86, Action: 1, State: [ 0.07023105  0.7027274  -0.16449548 -1.5038764 ], Reward: 1.0\n",
            "Step: 87, Action: 1, State: [ 0.08428559  0.89941883 -0.19457301 -1.8430748 ], Reward: 1.0\n",
            "Step: 88, Action: 1, State: [ 0.10227397  1.0960827  -0.23143451 -2.1893482 ], Reward: 1.0\n",
            "Pole fell! Resetting environment.\n",
            "Step: 89, Action: 1, State: [ 0.03468566  0.10664157 -0.05719054 -0.37291655], Reward: 1.0\n",
            "Step: 90, Action: 0, State: [ 0.03681849 -0.08762328 -0.06464887 -0.09880054], Reward: 1.0\n",
            "Step: 91, Action: 1, State: [ 0.03506602  0.10836278 -0.06662488 -0.41115904], Reward: 1.0\n",
            "Step: 92, Action: 0, State: [ 0.03723328 -0.08575453 -0.07484806 -0.14020231], Reward: 1.0\n",
            "Step: 93, Action: 1, State: [ 0.03551819  0.11035518 -0.0776521  -0.45552805], Reward: 1.0\n",
            "Step: 94, Action: 1, State: [ 0.03772529  0.30648425 -0.08676267 -0.7716419 ], Reward: 1.0\n",
            "Step: 95, Action: 0, State: [ 0.04385497  0.11265651 -0.10219551 -0.5074702 ], Reward: 1.0\n",
            "Step: 96, Action: 0, State: [ 0.0461081  -0.08088823 -0.11234491 -0.24866098], Reward: 1.0\n",
            "Step: 97, Action: 0, State: [ 0.04449034 -0.27424145 -0.11731813  0.00658053], Reward: 1.0\n",
            "Step: 98, Action: 0, State: [ 0.03900551 -0.46750256 -0.11718652  0.26006705], Reward: 1.0\n",
            "Step: 99, Action: 1, State: [ 0.02965546 -0.27091968 -0.11198518 -0.06715894], Reward: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(f\"Video saved to {video_file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjqN5WpYwcG-",
        "outputId": "104cb3be-df65-4a03-f43f-352f9bf75815"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video saved to cartpole_video/cartpole_simulation.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-aknVk43YsH0"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}