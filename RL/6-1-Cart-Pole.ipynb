{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7h35dtSccjDao6PqEBEKP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSJeong-me/AI-Innovation-2024/blob/main/RL/6-1-Cart-Pole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZGyFeo4WWGJ",
        "outputId": "6dcc40e7-764b-47c1-8697-23399d1744bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create the CartPole environment\n",
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "\n",
        "# Reset the environment with a custom seed and custom initial position range\n",
        "obs, info = env.reset(seed=123, options={\"low\": -0.1, \"high\": 0.1})\n",
        "\n",
        "# Extract the initial observation (state)\n",
        "state = obs\n",
        "print(f\"Initial state: {state}\")\n",
        "\n",
        "# Run the environment for a few steps to see how it evolves\n",
        "n_steps = 100  # Number of steps to simulate\n",
        "frames = []  # To store frames for rendering\n",
        "\n",
        "for step in range(n_steps):\n",
        "    frames.append(env.render())  # Capture the frame of the environment\n",
        "\n",
        "    action = env.action_space.sample()  # Randomly select an action (left or right)\n",
        "    state, reward, done, truncated, info = env.step(action)  # Take a step\n",
        "\n",
        "    # Print the state, action, and reward for each step\n",
        "    print(f\"Step: {step}, Action: {action}, State: {state}, Reward: {reward}\")\n",
        "\n",
        "    if done:\n",
        "        print(\"Pole fell! Resetting environment.\")\n",
        "        obs, info = env.reset(seed=123, options={\"low\": -0.1, \"high\": 0.1})\n",
        "        state = obs\n",
        "\n",
        "# Render the environment frames to visualize the CartPole problem\n",
        "env.close()\n",
        "\n",
        "# Visualize a sample frame\n",
        "plt.imshow(frames[0])\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1BNUL5eTWXfF",
        "outputId": "b9a17ff6-c7d6-4f7b-e5f9-d26aa534a513"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial state: [ 0.03647037 -0.0892358  -0.05592803 -0.06312564]\n",
            "Step: 0, Action: 0, State: [ 0.03468566 -0.28351313 -0.05719054  0.21140045], Reward: 1.0\n",
            "Step: 1, Action: 0, State: [ 0.02901539 -0.4777727  -0.05296253  0.4855086 ], Reward: 1.0\n",
            "Step: 2, Action: 0, State: [ 0.01945994 -0.6721088  -0.04325236  0.76104033], Reward: 1.0\n",
            "Step: 3, Action: 0, State: [ 0.00601776 -0.8666091  -0.02803155  1.0398057 ], Reward: 1.0\n",
            "Step: 4, Action: 0, State: [-0.01131442 -1.0613476  -0.00723544  1.3235584 ], Reward: 1.0\n",
            "Step: 5, Action: 1, State: [-0.03254137 -0.866135    0.01923573  1.02862   ], Reward: 1.0\n",
            "Step: 6, Action: 1, State: [-0.04986407 -0.6712743   0.03980813  0.74203795], Reward: 1.0\n",
            "Step: 7, Action: 0, State: [-0.06328956 -0.8669225   0.05464889  1.0469782 ], Reward: 1.0\n",
            "Step: 8, Action: 0, State: [-0.08062801 -1.0627254   0.07558846  1.356303  ], Reward: 1.0\n",
            "Step: 9, Action: 0, State: [-0.10188252 -1.25871     0.10271452  1.6716423 ], Reward: 1.0\n",
            "Step: 10, Action: 1, State: [-0.12705672 -1.0649204   0.13614736  1.4126348 ], Reward: 1.0\n",
            "Step: 11, Action: 0, State: [-0.14835513 -1.2614417   0.16440006  1.7445928 ], Reward: 1.0\n",
            "Step: 12, Action: 0, State: [-0.17358395 -1.4580079   0.19929191  2.0835826 ], Reward: 1.0\n",
            "Step: 13, Action: 1, State: [-0.20274411 -1.2653829   0.24096356  1.8585689 ], Reward: 1.0\n",
            "Pole fell! Resetting environment.\n",
            "Step: 14, Action: 1, State: [ 0.03468566  0.10664157 -0.05719054 -0.37291655], Reward: 1.0\n",
            "Step: 15, Action: 1, State: [ 0.03681849  0.30252734 -0.06464887 -0.6830697 ], Reward: 1.0\n",
            "Step: 16, Action: 1, State: [ 0.04286904  0.4984846  -0.07831027 -0.9953851 ], Reward: 1.0\n",
            "Step: 17, Action: 1, State: [ 0.05283873  0.69456166 -0.09821796 -1.311599  ], Reward: 1.0\n",
            "Step: 18, Action: 0, State: [ 0.06672996  0.500811   -0.12444995 -1.0512034 ], Reward: 1.0\n",
            "Step: 19, Action: 0, State: [ 0.07674618  0.30753955 -0.14547402 -0.8000322 ], Reward: 1.0\n",
            "Step: 20, Action: 1, State: [ 0.08289697  0.5043254  -0.16147466 -1.1347117 ], Reward: 1.0\n",
            "Step: 21, Action: 1, State: [ 0.09298348  0.70114887 -0.18416889 -1.4733738 ], Reward: 1.0\n",
            "Step: 22, Action: 0, State: [ 0.10700645  0.5086933  -0.21363637 -1.2434125 ], Reward: 1.0\n",
            "Pole fell! Resetting environment.\n",
            "Step: 23, Action: 1, State: [ 0.03468566  0.10664157 -0.05719054 -0.37291655], Reward: 1.0\n",
            "Step: 24, Action: 1, State: [ 0.03681849  0.30252734 -0.06464887 -0.6830697 ], Reward: 1.0\n",
            "Step: 25, Action: 1, State: [ 0.04286904  0.4984846  -0.07831027 -0.9953851 ], Reward: 1.0\n",
            "Step: 26, Action: 1, State: [ 0.05283873  0.69456166 -0.09821796 -1.311599  ], Reward: 1.0\n",
            "Step: 27, Action: 1, State: [ 0.06672996  0.8907805  -0.12444995 -1.6333385 ], Reward: 1.0\n",
            "Step: 28, Action: 0, State: [ 0.08454557  0.69732004 -0.15711671 -1.3818859 ], Reward: 1.0\n",
            "Step: 29, Action: 0, State: [ 0.09849197  0.50446844 -0.18475443 -1.1421741 ], Reward: 1.0\n",
            "Step: 30, Action: 0, State: [ 0.10858134  0.31217727 -0.20759791 -0.91265553], Reward: 1.0\n",
            "Step: 31, Action: 1, State: [ 0.11482488  0.5094111  -0.22585103 -1.2627504 ], Reward: 1.0\n",
            "Pole fell! Resetting environment.\n",
            "Step: 32, Action: 1, State: [ 0.03468566  0.10664157 -0.05719054 -0.37291655], Reward: 1.0\n",
            "Step: 33, Action: 1, State: [ 0.03681849  0.30252734 -0.06464887 -0.6830697 ], Reward: 1.0\n",
            "Step: 34, Action: 1, State: [ 0.04286904  0.4984846  -0.07831027 -0.9953851 ], Reward: 1.0\n",
            "Step: 35, Action: 1, State: [ 0.05283873  0.69456166 -0.09821796 -1.311599  ], Reward: 1.0\n",
            "Step: 36, Action: 1, State: [ 0.06672996  0.8907805  -0.12444995 -1.6333385 ], Reward: 1.0\n",
            "Step: 37, Action: 0, State: [ 0.08454557  0.69732004 -0.15711671 -1.3818859 ], Reward: 1.0\n",
            "Step: 38, Action: 1, State: [ 0.09849197  0.8940145  -0.18475443 -1.7192959 ], Reward: 1.0\n",
            "Step: 39, Action: 1, State: [ 0.11637226  1.0907106  -0.21914035 -2.0633283 ], Reward: 1.0\n",
            "Pole fell! Resetting environment.\n",
            "Step: 40, Action: 0, State: [ 0.03468566 -0.28351313 -0.05719054  0.21140045], Reward: 1.0\n",
            "Step: 41, Action: 1, State: [ 0.02901539 -0.08762209 -0.05296253 -0.09876057], Reward: 1.0\n",
            "Step: 42, Action: 0, State: [ 0.02726295 -0.28194657 -0.05493774  0.17675371], Reward: 1.0\n",
            "Step: 43, Action: 1, State: [ 0.02162402 -0.08608319 -0.05140267 -0.13274166], Reward: 1.0\n",
            "Step: 44, Action: 0, State: [ 0.01990236 -0.28043258 -0.0540575   0.14329165], Reward: 1.0\n",
            "Step: 45, Action: 0, State: [ 0.01429371 -0.47474036 -0.05119167  0.4184424 ], Reward: 1.0\n",
            "Step: 46, Action: 1, State: [ 0.0047989  -0.2789318  -0.04282282  0.11007052], Reward: 1.0\n",
            "Step: 47, Action: 1, State: [-0.00077974 -0.08322319 -0.04062141 -0.1958093 ], Reward: 1.0\n",
            "Step: 48, Action: 1, State: [-0.0024442   0.11245555 -0.04453759 -0.5010247 ], Reward: 1.0\n",
            "Step: 49, Action: 1, State: [-1.9508998e-04  3.0817610e-01 -5.4558087e-02 -8.0740410e-01], Reward: 1.0\n",
            "Step: 50, Action: 0, State: [ 0.00596843  0.11384265 -0.07070617 -0.5323698 ], Reward: 1.0\n",
            "Step: 51, Action: 1, State: [ 0.00824528  0.30988413 -0.08135357 -0.84646755], Reward: 1.0\n",
            "Step: 52, Action: 1, State: [ 0.01444297  0.5060161  -0.09828292 -1.163584  ], Reward: 1.0\n",
            "Step: 53, Action: 0, State: [ 0.02456329  0.31230146 -0.1215546  -0.90326303], Reward: 1.0\n",
            "Step: 54, Action: 0, State: [ 0.03080932  0.11901708 -0.13961986 -0.65112484], Reward: 1.0\n",
            "Step: 55, Action: 0, State: [ 0.03318966 -0.0739126  -0.15264235 -0.40546146], Reward: 1.0\n",
            "Step: 56, Action: 1, State: [ 0.03171141  0.12300712 -0.16075158 -0.74210936], Reward: 1.0\n",
            "Step: 57, Action: 0, State: [ 0.03417155 -0.06957392 -0.17559378 -0.50401986], Reward: 1.0\n",
            "Step: 58, Action: 0, State: [ 0.03278007 -0.261843   -0.18567418 -0.27141067], Reward: 1.0\n",
            "Step: 59, Action: 0, State: [ 0.02754321 -0.45389724 -0.19110239 -0.04255596], Reward: 1.0\n",
            "Step: 60, Action: 1, State: [ 0.01846527 -0.25662196 -0.19195351 -0.3889247 ], Reward: 1.0\n",
            "Step: 61, Action: 1, State: [ 0.01333283 -0.0593677  -0.19973199 -0.73546016], Reward: 1.0\n",
            "Step: 62, Action: 0, State: [ 0.01214547 -0.25125277 -0.2144412  -0.51168615], Reward: 1.0\n",
            "Pole fell! Resetting environment.\n",
            "Step: 63, Action: 0, State: [ 0.03468566 -0.28351313 -0.05719054  0.21140045], Reward: 1.0\n",
            "Step: 64, Action: 1, State: [ 0.02901539 -0.08762209 -0.05296253 -0.09876057], Reward: 1.0\n",
            "Step: 65, Action: 1, State: [ 0.02726295  0.10821734 -0.05493774 -0.4076715 ], Reward: 1.0\n",
            "Step: 66, Action: 1, State: [ 0.0294273   0.30407348 -0.06309117 -0.71715605], Reward: 1.0\n",
            "Step: 67, Action: 1, State: [ 0.03550877  0.50000924 -0.07743429 -1.0290115 ], Reward: 1.0\n",
            "Step: 68, Action: 0, State: [ 0.04550895  0.30599838 -0.09801452 -0.76161015], Reward: 1.0\n",
            "Step: 69, Action: 0, State: [ 0.05162892  0.11235344 -0.11324672 -0.501307  ], Reward: 1.0\n",
            "Step: 70, Action: 0, State: [ 0.05387599 -0.08100527 -0.12327287 -0.24635021], Reward: 1.0\n",
            "Step: 71, Action: 1, State: [ 0.05225588  0.11564191 -0.12819988 -0.5752331 ], Reward: 1.0\n",
            "Step: 72, Action: 0, State: [ 0.05456872 -0.07747185 -0.13970453 -0.3255272 ], Reward: 1.0\n",
            "Step: 73, Action: 0, State: [ 0.05301929 -0.27035692 -0.14621508 -0.0799581 ], Reward: 1.0\n",
            "Step: 74, Action: 0, State: [ 0.04761215 -0.46311322 -0.14781424  0.16325693], Reward: 1.0\n",
            "Step: 75, Action: 0, State: [ 0.03834988 -0.655844   -0.1445491   0.40590128], Reward: 1.0\n",
            "Step: 76, Action: 1, State: [ 0.025233   -0.45899975 -0.13643107  0.07136469], Reward: 1.0\n",
            "Step: 77, Action: 1, State: [ 0.01605301 -0.26221243 -0.13500378 -0.26105985], Reward: 1.0\n",
            "Step: 78, Action: 0, State: [ 0.01080876 -0.45517483 -0.14022498 -0.01382058], Reward: 1.0\n",
            "Step: 79, Action: 0, State: [ 0.00170526 -0.64803636 -0.14050138  0.23154102], Reward: 1.0\n",
            "Step: 80, Action: 0, State: [-0.01125546 -0.8409003  -0.13587056  0.47681457], Reward: 1.0\n",
            "Step: 81, Action: 1, State: [-0.02807347 -0.6441479  -0.12633428  0.14458269], Reward: 1.0\n",
            "Step: 82, Action: 1, State: [-0.04095643 -0.44746435 -0.12344262 -0.1851349 ], Reward: 1.0\n",
            "Step: 83, Action: 1, State: [-0.04990571 -0.25081223 -0.12714532 -0.5140685 ], Reward: 1.0\n",
            "Step: 84, Action: 0, State: [-0.05492196 -0.4439358  -0.13742669 -0.26400158], Reward: 1.0\n",
            "Step: 85, Action: 0, State: [-0.06380068 -0.63685596 -0.14270672 -0.01762607], Reward: 1.0\n",
            "Step: 86, Action: 0, State: [-0.0765378  -0.82967365 -0.14305924  0.22684683], Reward: 1.0\n",
            "Step: 87, Action: 0, State: [-0.09313127 -1.0224922  -0.13852231  0.4712039 ], Reward: 1.0\n",
            "Step: 88, Action: 1, State: [-0.11358111 -0.82571334 -0.12909822  0.13826762], Reward: 1.0\n",
            "Step: 89, Action: 0, State: [-0.13009538 -1.0187726  -0.12633288  0.38759714], Reward: 1.0\n",
            "Step: 90, Action: 0, State: [-0.15047084 -1.2118961  -0.11858093  0.63793063], Reward: 1.0\n",
            "Step: 91, Action: 0, State: [-0.17470875 -1.4051824  -0.10582232  0.8910428 ], Reward: 1.0\n",
            "Step: 92, Action: 0, State: [-0.2028124  -1.5987219  -0.08800147  1.1486742 ], Reward: 1.0\n",
            "Step: 93, Action: 0, State: [-0.23478684 -1.7925918  -0.06502797  1.4125148 ], Reward: 1.0\n",
            "Step: 94, Action: 1, State: [-0.27063867 -1.5967269  -0.03677768  1.1002337 ], Reward: 1.0\n",
            "Step: 95, Action: 1, State: [-0.3025732  -1.4011407  -0.01477301  0.79624265], Reward: 1.0\n",
            "Step: 96, Action: 1, State: [-3.3059603e-01 -1.2058191e+00  1.1518460e-03  4.9894923e-01], Reward: 1.0\n",
            "Step: 97, Action: 1, State: [-0.3547124  -1.0107135   0.01113083  0.20662951], Reward: 1.0\n",
            "Step: 98, Action: 0, State: [-0.3749267  -1.2059928   0.01526342  0.5028028 ], Reward: 1.0\n",
            "Step: 99, Action: 0, State: [-0.39904654 -1.4013265   0.02531948  0.8002565 ], Reward: 1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "Environment Creation: The code initializes the CartPole-v1 environment.\n",
        "Reset with Custom Options: The environment is reset with a custom seed and initial position bounds for the state.\n",
        "Simulation Loop: The environment is run for 100 steps, with actions randomly chosen using env.action_space.sample().\n",
        "Frame Capture: Each frame is captured for rendering purposes.\n",
        "Rendering and Plotting: At the end, a sample frame is displayed using matplotlib.\n",
        "This code allows you to visualize the behavior of the CartPole over time."
      ],
      "metadata": {
        "id": "mDlb8HIsWnIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import imageio\n",
        "import os\n",
        "\n",
        "# Create the CartPole environment\n",
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "\n",
        "# Reset the environment with a custom seed and custom initial position range\n",
        "obs, info = env.reset(seed=123, options={\"low\": -0.1, \"high\": 0.1})\n",
        "\n",
        "# Extract the initial observation (state)\n",
        "state = obs\n",
        "print(f\"Initial state: {state}\")\n",
        "\n",
        "# Run the environment for a few steps to see how it evolves\n",
        "n_steps = 100  # Number of steps to simulate\n",
        "frames = []  # To store frames for rendering\n",
        "\n",
        "# Directory to save the video\n",
        "video_dir = \"cartpole_video\"\n",
        "if not os.path.exists(video_dir):\n",
        "    os.makedirs(video_dir)\n",
        "\n",
        "# Loop to simulate the environment and collect frames\n",
        "for step in range(n_steps):\n",
        "    frames.append(env.render())  # Capture the frame of the environment\n",
        "\n",
        "    action = env.action_space.sample()  # Randomly select an action (left or right)\n",
        "    state, reward, done, truncated, info = env.step(action)  # Take a step\n",
        "\n",
        "    # Print the state, action, and reward for each step\n",
        "    print(f\"Step: {step}, Action: {action}, State: {state}, Reward: {reward}\")\n",
        "\n",
        "    if done:\n",
        "        print(\"Pole fell! Resetting environment.\")\n",
        "        obs, info = env.reset(seed=123, options={\"low\": -0.1, \"high\": 0.1})\n",
        "        state = obs\n",
        "\n",
        "# Save frames as a video using imageio\n",
        "video_file_path = os.path.join(video_dir, \"cartpole_simulation.mp4\")\n",
        "imageio.mimsave(video_file_path, frames, fps=30)\n",
        "\n",
        "# Close the environment\n",
        "env.close()\n",
        "\n",
        "print(f\"Video saved to {video_file_path}\")\n"
      ],
      "metadata": {
        "id": "xICBgt0pWy1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-aknVk43YsH0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}