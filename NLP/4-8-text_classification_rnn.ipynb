{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSJeong-me/AI-Innovation-2024/blob/main/NLP/4-8-text_classification_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hX4n9TsbGw-f"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "0nbI5DtDGw-i"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TnJztDZGw-n"
      },
      "source": [
        "# Text classification with an RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfN3bMR5Gw-o"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/text/tutorials/text_classification_rnn\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/text_classification_rnn.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/text/blob/master/docs/tutorials/text_classification_rnn.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/text/docs/tutorials/text_classification_rnn.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUWearf0Gw-p"
      },
      "source": [
        "This text classification tutorial trains a [recurrent neural network](https://developers.google.com/machine-learning/glossary/#recurrent_neural_network) on the [IMDB large movie review dataset](http://ai.stanford.edu/~amaas/data/sentiment/) for sentiment analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2VQo4bajwUU"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "z682XYsrjkY9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "tfds.disable_progress_bar()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rXHa-w9JZhb"
      },
      "source": [
        "Import `matplotlib` and create a helper function to plot graphs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Mp1Z7P9pYRSK"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_graphs(history, metric):\n",
        "  plt.plot(history.history[metric])\n",
        "  plt.plot(history.history['val_'+metric], '')\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(metric)\n",
        "  plt.legend([metric, 'val_'+metric])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRmMubr0jrE2"
      },
      "source": [
        "## Setup input pipeline\n",
        "\n",
        "\n",
        "The IMDB large movie review dataset is a *binary classification* datasetâ€”all the reviews have either a *positive* or *negative* sentiment.\n",
        "\n",
        "Download the dataset using [TFDS](https://www.tensorflow.org/datasets). See the [loading text tutorial](https://www.tensorflow.org/tutorials/load_data/text) for details on how to load this sort of data manually.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SHRwRoP2nVHX",
        "outputId": "168b8ce9-6bdf-415e-ecad-1e8891b7a05b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset 80.23 MiB (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\n",
            "Dataset imdb_reviews downloaded and prepared to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
              " TensorSpec(shape=(), dtype=tf.int64, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "dataset, info = tfds.load('imdb_reviews', with_info=True,\n",
        "                          as_supervised=True)\n",
        "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
        "\n",
        "train_dataset.element_spec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWA4c2ir7g6p"
      },
      "source": [
        "Initially this returns a dataset of (text, label pairs):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vd4_BGKyurao",
        "outputId": "93b61c7b-5358-4635-bb13-a3a6675df4be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text:  b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"\n",
            "label:  0\n"
          ]
        }
      ],
      "source": [
        "for example, label in train_dataset.take(1):\n",
        "  print('text: ', example.numpy())\n",
        "  print('label: ', label.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2qVJzcEluH_"
      },
      "source": [
        "Next shuffle the data for training and create batches of these `(text, label)` pairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dDsCaZCDYZgm"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VznrltNOnUc5"
      },
      "outputs": [],
      "source": [
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jqkvdcFv41wC",
        "outputId": "f121beab-6124-48d5-826e-8f9f228837cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "texts:  [b'Sandra Bernhard is quite a character, and certainly one of the funniest women on earth. She began as a stand-up comedienne in the 1970s, but her big break came in 1983 when she starred opposite Jerry Lewis and Robert De Niro in Scorsese\\'s underrated masterpiece, \"The King of Comedy\". Her film career never quite took off, though. She did make a couple of odd but entertaining pictures, such as \"Dallas Doll\" (1994) or \"Dinner Rush\" (2000), but the most amazing parts were those she created for herself.<br /><br />\"Without You I\\'m Nothing\" is undoubtedly her best effort. It\\'s an adaptation of her smash-hit off-Broadway show which made her a superstar \\xc2\\x96 and Madonna\\'s best friend for about four years. In ten perfectly choreographed and staged scenes, Sandra turns from Nina Simone to Diana Ross, talks about her childhood, Andy Warhol and San Francisco and performs songs made famous by Burt Bacharach, Prince, or Sylvester. Director John Boskovich got Sandra to do a 90-minute tour-de-force performance that\\'s both sexy and uniquely funny. If you are a Bernhard fan, you can\\'t miss out this film; it\\'s a tribute as well to her (weird) beauty as to her extremely unconventional talent as a comedienne. And it has influenced filmmakers in their work \\xc2\\x96 \"Hedwig and the Angry Inch\", for instance, would look a lot different if \"Without You I\\'m Nothing\" didn\\'t exist.'\n",
            " b'This is by far one of the most boring and horribly acted accounts of the early days of Adolf Hitler that I have ever watched. Robert Carlyle is a wonderful actor, but to cast him as Hitler is just plain wrong. To cast Liev Schrieber as Hitler\\'s longtime friend and aid, Haefengstal must have emitted cries of despair and anguish from the Simon Wiesenthal Centre. A J-W playing a Nazi supporter, bad bad bad casting. This was not an enjoyable family film with a good historical background. This was Hollywood rubbish at its finest, cashing in on the strength of a strong (but sorely under utilized) supporting cast of actors whom seemed to have all but disappeared from the acting radar in the past 5 years.<br /><br />The fake German accents (vee vill vin zis var) is insulting to German people everywhere. My mother is German and she sat fuming at the sound of the voices which kept switching from American/English/German all in the same sentence. The supporting cast make better cardboard cutouts at the local video store than they do on screen. Jenna Malone as the fated Geli Raubal, was splendid though, she captured the innocence and confusion of this tragic young woman who ultimately ended her own life to escape what her future would have been like in Hitler\\'s shadow.<br /><br />If you would like a tremendously fantastic and historically accurate account of Hitler\\'s early years leading up to and including the war/holocaust, rent \"Inside the Third Reich\" 1983 starring Rutger Hauer as Albert Speer and Derek Jacobi as Hitler. It was good and made more sense then this baloney.<br /><br />As a historical researcher of the Third Reich I can honestly tell you, this had me reaching for my books to confirm its myriad of inaccuracies.'\n",
            " b'Sorry, but every time I see a film wherein a woman sucker-punches a man and the man does nothing but cower, the film looses all credibility. So the new (female) Starbuck immediately tainted the plot before it even got off the ground (no pun intended). Dirk Benedict was so much more plausible as the sensitive hero-type than the new-age Kattee Sackhoff-- whose overacting will probably be henceforth lauded as \"a compelling, exciting, must-see, ground-breaking performance,\" by the politically correct new-speak of today\\'s review copy editors; but in essence, it is just a tired, old image of a woman with a chip on her shoulder as big as a townhouse: the biggest clich\\xc3\\xa9 on screens today. I may give this series one more shot, but human caricatures alone will not keep me tuned in. As James Hilton once bemoaned, \"A story, please; just give me a story.\"']\n",
            "\n",
            "labels:  [1 0 0]\n"
          ]
        }
      ],
      "source": [
        "for example, label in train_dataset.take(1):\n",
        "  print('texts: ', example.numpy()[:3])\n",
        "  print()\n",
        "  print('labels: ', label.numpy()[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5eWCo88voPY"
      },
      "source": [
        "## Create the text encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFevcItw15P_"
      },
      "source": [
        "The raw text loaded by `tfds` needs to be processed before it can be used in a model. The simplest way to process text for training is using the `TextVectorization` layer. This layer has many capabilities, but this tutorial sticks to the default behavior.\n",
        "\n",
        "Create the layer, and pass the dataset's text to the layer's `.adapt` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uC25Lu1Yvuqy"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = 1000\n",
        "encoder = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE)\n",
        "encoder.adapt(train_dataset.map(lambda text, label: text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuQzVBbe3Ldu"
      },
      "source": [
        "The `.adapt` method sets the layer's vocabulary. Here are the first 20 tokens. After the padding and unknown tokens they're sorted by frequency:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tBoyjjWg0Ac9",
        "outputId": "9aaea055-b900-4334-8e9a-6b2463af007d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i',\n",
              "       'this', 'that', 'br', 'was', 'as', 'for', 'with', 'movie', 'but'],\n",
              "      dtype='<U14')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "vocab = np.array(encoder.get_vocabulary())\n",
        "vocab[:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjId5pua3jHQ"
      },
      "source": [
        "Once the vocabulary is set, the layer can encode text into indices. The tensors of indices are 0-padded to the longest sequence in the batch (unless you set a fixed `output_sequence_length`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RGc7C9WiwRWs",
        "outputId": "42a14cf6-f5f4-4fb5-b3c8-2c4b3ec7f73d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  1,   1,   7, ...,   0,   0,   0],\n",
              "       [ 11,   7,  33, ...,   0,   0,   0],\n",
              "       [786,  19, 168, ...,   0,   0,   0]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "encoded_example = encoder(example)[:3].numpy()\n",
        "encoded_example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5cjz0bS39IN"
      },
      "source": [
        "With the default settings, the process is not completely reversible. There are three main reasons for that:\n",
        "\n",
        "1. The default value for `preprocessing.TextVectorization`'s `standardize` argument is `\"lower_and_strip_punctuation\"`.\n",
        "2. The limited vocabulary size and lack of character-based fallback results in some unknown tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "N_tD0QY5wXaK",
        "outputId": "e06e87dd-4734-4a57-9626-337f605dc43a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  b'Sandra Bernhard is quite a character, and certainly one of the funniest women on earth. She began as a stand-up comedienne in the 1970s, but her big break came in 1983 when she starred opposite Jerry Lewis and Robert De Niro in Scorsese\\'s underrated masterpiece, \"The King of Comedy\". Her film career never quite took off, though. She did make a couple of odd but entertaining pictures, such as \"Dallas Doll\" (1994) or \"Dinner Rush\" (2000), but the most amazing parts were those she created for herself.<br /><br />\"Without You I\\'m Nothing\" is undoubtedly her best effort. It\\'s an adaptation of her smash-hit off-Broadway show which made her a superstar \\xc2\\x96 and Madonna\\'s best friend for about four years. In ten perfectly choreographed and staged scenes, Sandra turns from Nina Simone to Diana Ross, talks about her childhood, Andy Warhol and San Francisco and performs songs made famous by Burt Bacharach, Prince, or Sylvester. Director John Boskovich got Sandra to do a 90-minute tour-de-force performance that\\'s both sexy and uniquely funny. If you are a Bernhard fan, you can\\'t miss out this film; it\\'s a tribute as well to her (weird) beauty as to her extremely unconventional talent as a comedienne. And it has influenced filmmakers in their work \\xc2\\x96 \"Hedwig and the Angry Inch\", for instance, would look a lot different if \"Without You I\\'m Nothing\" didn\\'t exist.'\n",
            "Round-trip:  [UNK] [UNK] is quite a character and certainly one of the [UNK] women on earth she [UNK] as a [UNK] [UNK] in the [UNK] but her big [UNK] came in [UNK] when she [UNK] [UNK] [UNK] [UNK] and robert de [UNK] in [UNK] [UNK] [UNK] the king of comedy her film career never quite took off though she did make a couple of [UNK] but entertaining [UNK] such as [UNK] [UNK] [UNK] or [UNK] [UNK] [UNK] but the most amazing parts were those she [UNK] for [UNK] br without you im nothing is [UNK] her best effort its an [UNK] of her [UNK] [UNK] show which made her a [UNK] Â– and [UNK] best friend for about four years in ten perfectly [UNK] and [UNK] scenes [UNK] turns from [UNK] [UNK] to [UNK] [UNK] [UNK] about her [UNK] [UNK] [UNK] and [UNK] [UNK] and [UNK] songs made famous by [UNK] [UNK] [UNK] or [UNK] director john [UNK] got [UNK] to do a [UNK] [UNK] performance thats both [UNK] and [UNK] funny if you are a [UNK] fan you cant miss out this film its a [UNK] as well to her weird beauty as to her extremely [UNK] talent as a [UNK] and it has [UNK] filmmakers in their work Â– [UNK] and the [UNK] [UNK] for [UNK] would look a lot different if without you im nothing didnt [UNK]                                                                                                                                                                                                                                                                                                                                                                                                  \n",
            "\n",
            "Original:  b'This is by far one of the most boring and horribly acted accounts of the early days of Adolf Hitler that I have ever watched. Robert Carlyle is a wonderful actor, but to cast him as Hitler is just plain wrong. To cast Liev Schrieber as Hitler\\'s longtime friend and aid, Haefengstal must have emitted cries of despair and anguish from the Simon Wiesenthal Centre. A J-W playing a Nazi supporter, bad bad bad casting. This was not an enjoyable family film with a good historical background. This was Hollywood rubbish at its finest, cashing in on the strength of a strong (but sorely under utilized) supporting cast of actors whom seemed to have all but disappeared from the acting radar in the past 5 years.<br /><br />The fake German accents (vee vill vin zis var) is insulting to German people everywhere. My mother is German and she sat fuming at the sound of the voices which kept switching from American/English/German all in the same sentence. The supporting cast make better cardboard cutouts at the local video store than they do on screen. Jenna Malone as the fated Geli Raubal, was splendid though, she captured the innocence and confusion of this tragic young woman who ultimately ended her own life to escape what her future would have been like in Hitler\\'s shadow.<br /><br />If you would like a tremendously fantastic and historically accurate account of Hitler\\'s early years leading up to and including the war/holocaust, rent \"Inside the Third Reich\" 1983 starring Rutger Hauer as Albert Speer and Derek Jacobi as Hitler. It was good and made more sense then this baloney.<br /><br />As a historical researcher of the Third Reich I can honestly tell you, this had me reaching for my books to confirm its myriad of inaccuracies.'\n",
            "Round-trip:  this is by far one of the most boring and [UNK] [UNK] [UNK] of the early days of [UNK] [UNK] that i have ever watched robert [UNK] is a wonderful actor but to cast him as [UNK] is just [UNK] wrong to cast [UNK] [UNK] as [UNK] [UNK] friend and [UNK] [UNK] must have [UNK] [UNK] of [UNK] and [UNK] from the [UNK] [UNK] [UNK] a [UNK] playing a [UNK] [UNK] bad bad bad casting this was not an enjoyable family film with a good [UNK] background this was hollywood [UNK] at its [UNK] [UNK] in on the [UNK] of a strong but [UNK] under [UNK] supporting cast of actors whom seemed to have all but [UNK] from the acting [UNK] in the past 5 [UNK] br the [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] is [UNK] to [UNK] people [UNK] my mother is [UNK] and she [UNK] [UNK] at the sound of the [UNK] which kept [UNK] from [UNK] all in the same [UNK] the supporting cast make better [UNK] [UNK] at the local video [UNK] than they do on screen [UNK] [UNK] as the [UNK] [UNK] [UNK] was [UNK] though she [UNK] the [UNK] and [UNK] of this [UNK] young woman who [UNK] [UNK] her own life to [UNK] what her future would have been like in [UNK] [UNK] br if you would like a [UNK] fantastic and [UNK] [UNK] [UNK] of [UNK] early years leading up to and including the [UNK] rent inside the third [UNK] [UNK] [UNK] [UNK] [UNK] as [UNK] [UNK] and [UNK] [UNK] as [UNK] it was good and made more sense then this [UNK] br as a [UNK] [UNK] of the third [UNK] i can [UNK] tell you this had me [UNK] for my [UNK] to [UNK] its [UNK] of [UNK]                                                                                                                                                                                                                                                                                                                            \n",
            "\n",
            "Original:  b'Sorry, but every time I see a film wherein a woman sucker-punches a man and the man does nothing but cower, the film looses all credibility. So the new (female) Starbuck immediately tainted the plot before it even got off the ground (no pun intended). Dirk Benedict was so much more plausible as the sensitive hero-type than the new-age Kattee Sackhoff-- whose overacting will probably be henceforth lauded as \"a compelling, exciting, must-see, ground-breaking performance,\" by the politically correct new-speak of today\\'s review copy editors; but in essence, it is just a tired, old image of a woman with a chip on her shoulder as big as a townhouse: the biggest clich\\xc3\\xa9 on screens today. I may give this series one more shot, but human caricatures alone will not keep me tuned in. As James Hilton once bemoaned, \"A story, please; just give me a story.\"'\n",
            "Round-trip:  sorry but every time i see a film [UNK] a woman [UNK] a man and the man does nothing but [UNK] the film [UNK] all [UNK] so the new female [UNK] [UNK] [UNK] the plot before it even got off the [UNK] no [UNK] [UNK] [UNK] [UNK] was so much more [UNK] as the [UNK] [UNK] than the [UNK] [UNK] [UNK] whose [UNK] will probably be [UNK] [UNK] as a [UNK] [UNK] [UNK] [UNK] performance by the [UNK] [UNK] [UNK] of [UNK] review [UNK] [UNK] but in [UNK] it is just a [UNK] old [UNK] of a woman with a [UNK] on her [UNK] as big as a [UNK] the [UNK] [UNK] on [UNK] today i may give this series one more shot but human [UNK] alone will not keep me [UNK] in as james [UNK] once [UNK] a story please just give me a story                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
            "\n"
          ]
        }
      ],
      "source": [
        "for n in range(3):\n",
        "  print(\"Original: \", example[n].numpy())\n",
        "  print(\"Round-trip: \", \" \".join(vocab[encoded_example[n]]))\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjUqGVBxGw-t"
      },
      "source": [
        "## Create the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7zsmInBOCPO"
      },
      "source": [
        "![A drawing of the information flow in the model](https://github.com/tensorflow/text/blob/master/docs/tutorials/images/bidirectional.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgs6nnSTGw-t"
      },
      "source": [
        "Above is a diagram of the model.\n",
        "\n",
        "1. This model can be build as a `tf.keras.Sequential`.\n",
        "\n",
        "2. The first layer is the `encoder`, which converts the text to a sequence of token indices.\n",
        "\n",
        "3. After the encoder is an embedding layer. An embedding layer stores one vector per word. When called, it converts the sequences of word indices to sequences of vectors. These vectors are trainable. After training (on enough data), words with similar meanings often have similar vectors.\n",
        "\n",
        "  This index-lookup is much more efficient than the equivalent operation of passing a one-hot encoded vector through a `tf.keras.layers.Dense` layer.\n",
        "\n",
        "4. A recurrent neural network (RNN) processes sequence input by iterating through the elements. RNNs pass the outputs from one timestep to their input on the next timestep.\n",
        "\n",
        "  The `tf.keras.layers.Bidirectional` wrapper can also be used with an RNN layer. This propagates the input forward and backwards through the RNN layer and then concatenates the final output.\n",
        "\n",
        "  * The main advantage of a bidirectional RNN is that the signal from the beginning of the input doesn't need to be processed all the way through every timestep to affect the output.  \n",
        "\n",
        "  * The main disadvantage of a bidirectional RNN is that you can't efficiently stream predictions as words are being added to the end.\n",
        "\n",
        "5. After the RNN has converted the sequence to a single vector the two `layers.Dense` do some final processing, and convert from this vector representation to a single logit as the classification output.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4fodCI7soQi"
      },
      "source": [
        "The code to implement this is below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LwfoBkmRYcP3"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    encoder,\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim=len(encoder.get_vocabulary()),\n",
        "        output_dim=64,\n",
        "        # Use masking to handle the variable sequence lengths\n",
        "        mask_zero=True),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIGmIGkkouUb"
      },
      "source": [
        "Please note that Keras sequential model is used here since all the layers in the model only have single input and produce single output. In case you want to use stateful RNN layer, you might want to build your model with Keras functional API or model subclassing so that you can retrieve and reuse the RNN layer states. Please check [Keras RNN guide](https://www.tensorflow.org/guide/keras/rnn#rnn_state_reuse) for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kF-PsCk1LwjY"
      },
      "source": [
        "The embedding layer [uses masking](https://www.tensorflow.org/guide/keras/masking_and_padding) to handle the varying sequence-lengths. All the layers after the `Embedding` support masking:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "87a8-CwfKebw",
        "outputId": "09e257f6-dad3-43bc-861b-bada2e8c191b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[False, True, True, True, True]\n"
          ]
        }
      ],
      "source": [
        "print([layer.supports_masking for layer in model.layers])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlS0iaUIWLpI"
      },
      "source": [
        "To confirm that this works as expected, evaluate a sentence twice. First, alone so there's no padding to mask:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O41gw3KfWHus"
      },
      "outputs": [],
      "source": [
        "# predict on a sample text without padding.\n",
        "\n",
        "# sample_text = ('The movie was cool. The animation and the graphics '\n",
        "#                'were out of this world. I would recommend this movie.')\n",
        "# predictions = model.predict(np.array([sample_text]))\n",
        "# print(predictions[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = ('The movie was cool. The animation and the graphics '\n",
        "               'were out of this world. I would recommend this movie.')\n",
        "\n",
        "# 1. Tokenize the sample text\n",
        "sequences = tokenizer.texts_to_sequences([sample_text])\n",
        "\n",
        "# 2. Pad the sequence to match the model's input shape\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length)\n",
        "\n",
        "# 3. Make predictions\n",
        "predictions = model.predict(padded_sequences)\n",
        "print(predictions[0])"
      ],
      "metadata": {
        "id": "3tOpJ_u_RO5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0VQmGnEWcuz"
      },
      "source": [
        "Now, evaluate it again in a batch with a longer sentence. The result should be identical:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIgpuTeFNDzq"
      },
      "outputs": [],
      "source": [
        "# predict on a sample text with padding\n",
        "\n",
        "padding = \"the \" * 2000\n",
        "predictions = model.predict(np.array([sample_text, padding]))\n",
        "print(predictions[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRI776ZcH3Tf"
      },
      "source": [
        "Compile the Keras model to configure the training process:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kj2xei41YZjC"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIwH3nto596k"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hw86wWS4YgR2"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_dataset, epochs=10,\n",
        "                    validation_data=test_dataset,\n",
        "                    validation_steps=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BaNbXi43YgUT"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss:', test_loss)\n",
        "print('Test Accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZmwt_mzaQJk"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plot_graphs(history, 'accuracy')\n",
        "plt.ylim(None, 1)\n",
        "plt.subplot(1, 2, 2)\n",
        "plot_graphs(history, 'loss')\n",
        "plt.ylim(0, None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwSE_386uhxD"
      },
      "source": [
        "Run a prediction on a new sentence:\n",
        "\n",
        "If the prediction is >= 0.0, it is positive else it is negative."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXgfQSgRW6zU"
      },
      "outputs": [],
      "source": [
        "sample_text = ('The movie was cool. The animation and the graphics '\n",
        "               'were out of this world. I would recommend this movie.')\n",
        "predictions = model.predict(np.array([sample_text]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7g1evcaRpTKm"
      },
      "source": [
        "## Stack two or more LSTM layers\n",
        "\n",
        "Keras recurrent layers have two available modes that are controlled by the `return_sequences` constructor argument:\n",
        "\n",
        "* If `False` it returns only the last output for each input sequence (a 2D tensor of shape (batch_size, output_features)). This is the default, used in the previous model.\n",
        "\n",
        "* If `True` the full sequences of successive outputs for each timestep is returned (a 3D tensor of shape `(batch_size, timesteps, output_features)`).\n",
        "\n",
        "Here is what the flow of information looks like with `return_sequences=True`:\n",
        "\n",
        "![layered_bidirectional](https://github.com/tensorflow/text/blob/master/docs/tutorials/images/layered_bidirectional.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbSClCrG1z8l"
      },
      "source": [
        "The interesting thing about using an `RNN` with `return_sequences=True` is that the output still has 3-axes, like the input, so it can be passed to another RNN layer, like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jo1jjO3vn0jo"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    encoder,\n",
        "    tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 64, mask_zero=True),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEPV5jVGp-is"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeSE-YjdqAeN"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_dataset, epochs=10,\n",
        "                    validation_data=test_dataset,\n",
        "                    validation_steps=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LdwilM1qPM3"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss:', test_loss)\n",
        "print('Test Accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykUKnAoqbycW"
      },
      "outputs": [],
      "source": [
        "# predict on a sample text without padding.\n",
        "\n",
        "sample_text = ('The movie was not good. The animation and the graphics '\n",
        "               'were terrible. I would not recommend this movie.')\n",
        "predictions = model.predict(np.array([sample_text]))\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YYub0EDtwCu"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plot_graphs(history, 'accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plot_graphs(history, 'loss')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xvpE3BaGw_V"
      },
      "source": [
        "Check out other existing recurrent layers such as [GRU layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU).\n",
        "\n",
        "If you're interested in building custom RNNs, see the [Keras RNN Guide](https://www.tensorflow.org/guide/keras/rnn).\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "text_classification_rnn.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}