{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNKLE5k3UYsJo7EXaYAFk0A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSJeong-me/AI-Innovation-2024/blob/main/NLP/4-6-LSTM-predict-word.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## https://machinelearningmastery.com/text-generation-with-lstm-in-pytorch/"
      ],
      "metadata": {
        "id": "pIspsc_nT8gj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQ6S-IjITp-d",
        "outputId": "925e5c14-6793-4451-d828-3fd8369af878"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-07 23:27:46--  https://docs.google.com/uc?export=download&id=1FzTezch20eBDFLFIkMTbkOCtKUnVqMet\n",
            "Resolving docs.google.com (docs.google.com)... 142.251.175.100, 142.251.175.113, 142.251.175.101, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.251.175.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1FzTezch20eBDFLFIkMTbkOCtKUnVqMet&export=download [following]\n",
            "--2024-10-07 23:27:47--  https://drive.usercontent.google.com/download?id=1FzTezch20eBDFLFIkMTbkOCtKUnVqMet&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.251.12.132, 2404:6800:4003:c11::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.251.12.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13903 (14K) [application/octet-stream]\n",
            "Saving to: ‘wonderland.txt’\n",
            "\n",
            "wonderland.txt      100%[===================>]  13.58K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-10-07 23:27:53 (83.1 MB/s) - ‘wonderland.txt’ saved [13903/13903]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1FzTezch20eBDFLFIkMTbkOCtKUnVqMet' -O wonderland.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data"
      ],
      "metadata": {
        "id": "Sg1lEfJoTr7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# load ascii text and covert to lowercase\n",
        "filename = \"wonderland.txt\"\n",
        "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
        "raw_text = raw_text.lower()"
      ],
      "metadata": {
        "id": "PjO7iz8lU1eA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "\n",
        "# summarize the loaded data\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)\n",
        "\n",
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "    seq_in = raw_text[i:i + seq_length]\n",
        "    seq_out = raw_text[i + seq_length]\n",
        "    dataX.append([char_to_int[char] for char in seq_in])\n",
        "    dataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)"
      ],
      "metadata": {
        "id": "q72psMi_U4SZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# reshape X to be [samples, time steps, features]\n",
        "X = torch.tensor(dataX, dtype=torch.float32).reshape(n_patterns, seq_length, 1)\n",
        "X = X / float(n_vocab)\n",
        "y = torch.tensor(dataY)"
      ],
      "metadata": {
        "id": "zAbLrs56U7GQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class CharModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size=1, hidden_size=256, num_layers=2, batch_first=True, dropout=0.2)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.linear = nn.Linear(256, n_vocab)\n",
        "    def forward(self, x):\n",
        "        x, _ = self.lstm(x)\n",
        "        # take only the last output\n",
        "        x = x[:, -1, :]\n",
        "        # produce output\n",
        "        x = self.linear(self.dropout(x))\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Pj4CrmEqU88A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "n_epochs = 40\n",
        "batch_size = 128\n",
        "model = CharModel()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "loss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\n",
        "loader = data.DataLoader(data.TensorDataset(X, y), shuffle=True, batch_size=batch_size)\n",
        "\n",
        "best_model = None\n",
        "best_loss = np.inf\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    for X_batch, y_batch in loader:\n",
        "        y_pred = model(X_batch.to(device))\n",
        "        loss = loss_fn(y_pred, y_batch.to(device))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    loss = 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in loader:\n",
        "            y_pred = model(X_batch.to(device))\n",
        "            loss += loss_fn(y_pred, y_batch.to(device))\n",
        "        if loss < best_loss:\n",
        "            best_loss = loss\n",
        "            best_model = model.state_dict()\n",
        "        print(\"Epoch %d: Cross-entropy: %.4f\" % (epoch, loss))\n",
        "\n",
        "torch.save([best_model, char_to_int], \"single-char.pth\")"
      ],
      "metadata": {
        "id": "CibIEU-NU-Ep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Generation using the trained model\n",
        "best_model, char_to_int = torch.load(\"single-char.pth\")\n",
        "n_vocab = len(char_to_int)\n",
        "int_to_char = dict((i, c) for c, i in char_to_int.items())\n",
        "model.load_state_dict(best_model)\n"
      ],
      "metadata": {
        "id": "43XcZ5psVACY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# randomly generate a prompt\n",
        "filename = \"wonderland.txt\"\n",
        "seq_length = 100\n",
        "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
        "raw_text = raw_text.lower()\n",
        "start = np.random.randint(0, len(raw_text)-seq_length)\n",
        "prompt = raw_text[start:start+seq_length]\n",
        "pattern = [char_to_int[c] for c in prompt]\n"
      ],
      "metadata": {
        "id": "M3SOg4aoVEOi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RFfmqavzVbdF",
        "outputId": "543bd3d5-aaee-439e-f6a9-27bd48698db9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'y sat down, and nobody spoke for some minutes. alice thought to herself, “i don’t see how he can eve'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt = \"nother rush at the stick, and tumbled head over heels in its hurry to get hold of it; then alice, th\""
      ],
      "metadata": {
        "id": "K_-j9wOvVM4I"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.eval()\n",
        "print('Prompt: \"%s\"' % prompt)\n",
        "with torch.no_grad():\n",
        "    for i in range(1000):\n",
        "        # format input array of int into PyTorch tensor\n",
        "        x = np.reshape(pattern, (1, len(pattern), 1)) / float(n_vocab)\n",
        "        x = torch.tensor(x, dtype=torch.float32)\n",
        "        # generate logits as output from the model\n",
        "        prediction = model(x.to(device))\n",
        "        # convert logits into one character\n",
        "        index = int(prediction.argmax())\n",
        "        result = int_to_char[index]\n",
        "        print(result, end=\"\")\n",
        "        # append the new character into the prompt for the next iteration\n",
        "        pattern.append(index)\n",
        "        pattern = pattern[1:]\n",
        "print()\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oIRagzTVF3y",
        "outputId": "87e47553-e827-4101-efe8-c3f710263d26"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: \"y sat down, and nobody spoke for some minutes. alice thought to herself, “i don’t see how he can eve\"\n",
            "r finishe an ifr fear  when all the bourd ne the hoope oe the pame of the had never begore the thie at the white rabbit hurried tprs oi the dour oo aagk the wan oefe the tas oooee of the hoosess ponee of the sert of the had not the san off as her fear  and  shth the was serereene of the hing and the har and gloves, and  as the was sery oook the were all surning and tooce in the dound ne hear the rabbit sere all sorning an ier fear  he hand  whth a large dan in the distance, and she whrh little sas so tee sooee of her head to her fead mede the thiee was so tee ion he dan her feed here thenk the tas off a head uire to tee ion he dan her feed here thenk ier siater and blnvereny dresmed and she sert of the had nedd her feed me her head to her fead hed he i’ve been changed abfore the thie at the white rabbit hurried tprs oi the dour oo aagk the wan oefe the tas oooee of the hoosess ponee of the sert of the had not the san off as her fear  and  shth the was serereene of the hing and the har \n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt: \"nother rush at the stick, and tumbled head over heels in its hurry to get hold of it; then alice, th\""
      ],
      "metadata": {
        "id": "_NlRoZQaT2xt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TC0AeR8gUA_b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}