{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNKfzwVERzjfbt8h5Hopr4Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSJeong-me/AI-Innovation-2024/blob/main/NLP/4-5-Llamaindex02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0auKOsW1Q8R"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain_community openai chromadb tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.document_loaders import TextLoader"
      ],
      "metadata": {
        "id": "yWhUG3ev1Sxi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "33VuE5xi1YJT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data"
      ],
      "metadata": {
        "id": "Z2NQsOpP1eW-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/JSJeong-me/GPT-Web/raw/main/images/paul_graham_essay.txt -O ./data/paul_graham_essay.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRQItrle1hqn",
        "outputId": "f98432a1-5a94-4758-9de1-429fa6a2005a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-07 07:52:49--  https://github.com/JSJeong-me/GPT-Web/raw/main/images/paul_graham_essay.txt\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/JSJeong-me/GPT-Web/main/images/paul_graham_essay.txt [following]\n",
            "--2024-10-07 07:52:49--  https://raw.githubusercontent.com/JSJeong-me/GPT-Web/main/images/paul_graham_essay.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75042 (73K) [text/plain]\n",
            "Saving to: ‘./data/paul_graham_essay.txt’\n",
            "\n",
            "./data/paul_graham_ 100%[===================>]  73.28K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2024-10-07 07:52:49 (27.6 MB/s) - ‘./data/paul_graham_essay.txt’ saved [75042/75042]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: 문서 로드\n",
        "# 여기에 분석하고자 하는 텍스트 파일을 넣으세요.\n",
        "loader = TextLoader('./data/paul_graham_essay.txt')\n",
        "documents = loader.load()\n"
      ],
      "metadata": {
        "id": "YlTmvCa-1ZJB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: 텍스트 분할\n",
        "from langchain.text_splitter import CharacterTextSplitter # import the CharacterTextSplitter class\n",
        "\n",
        "# 문서를 최대 1000 토큰의 청크로 분할합니다.\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0) # reduced chunk size to 1000\n",
        "documents = text_splitter.split_documents(documents)\n",
        "\n",
        "# Step 3: 임베딩 및 Chroma 벡터 스토어 생성\n",
        "embeddings = OpenAIEmbeddings()\n",
        "vectorstore = Chroma.from_documents(documents, embeddings)\n",
        "\n",
        "# Step 4: 검색 및 QA 체인 생성\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=OpenAI(),\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 2}) # only retrieve the top 2 most relevant documents\n",
        ")\n",
        "\n",
        "# Step 5: 질문에 답변\n",
        "query = \"What is the author name?\"\n",
        "answer = qa.run(query)\n",
        "print(f\"Answer: {answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5M_xt349dGI",
        "outputId": "7f1bcbb6-ce06-4bad-aa5b-3a01e179f986"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1004, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1203, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1025, which is longer than the specified 1000\n",
            "<ipython-input-7-a6dba6db56d0>:9: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
            "  embeddings = OpenAIEmbeddings()\n",
            "<ipython-input-7-a6dba6db56d0>:14: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
            "  llm=OpenAI(),\n",
            "<ipython-input-7-a6dba6db56d0>:21: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  answer = qa.run(query)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer:  Paul Graham\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How many legs does a cat have?\"\n",
        "answer = qa.run(query)\n",
        "print(f\"Answer: {answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNsgJ0qb97ju",
        "outputId": "93c5e7ca-0b83-4b52-8669-9559fa1deb04"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer:  I don't know.\n"
          ]
        }
      ]
    }
  ]
}