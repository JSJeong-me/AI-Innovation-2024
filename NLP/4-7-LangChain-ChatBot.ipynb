{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOInZR44OJd7X0b2xy/m2tB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSJeong-me/AI-Innovation-2024/blob/main/NLP/4-7-LangChain-ChatBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "K0bWZgaXiIot"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-core langgraph>0.2.27"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFjuVDQdiLmT",
        "outputId": "8475c09a-c4b0-4e41-a3a9-0ed7decfac09"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/49.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.7/383.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "n8myeQ_ViVn6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "id": "KsNLQZOSigan"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "model.invoke([HumanMessage(content=\"Hi! I'm Bob\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDXqZRw9imU_",
        "outputId": "a68bdab2-754e-4518-96e2-5767a27800e9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hello Bob! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 12, 'total_tokens': 22, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-1428a2b3-07ed-4ef1-b8f0-27a049aac203-0', usage_metadata={'input_tokens': 12, 'output_tokens': 10, 'total_tokens': 22, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.invoke([HumanMessage(content=\"What's my name?\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sohEEZbisJx",
        "outputId": "dfcbecf8-f8d5-47f9-af5d-a6e74ea1e787"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"I'm sorry, but I do not have the ability to know your name unless you tell me. How may I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 12, 'total_tokens': 39, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a4931070-1a73-460f-a25c-3d53f59964e8-0', usage_metadata={'input_tokens': 12, 'output_tokens': 27, 'total_tokens': 39, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Message persistence"
      ],
      "metadata": {
        "id": "GKB15ezajfHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import START, MessagesState, StateGraph"
      ],
      "metadata": {
        "id": "5-3d-XD8jecG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define a new graph\n",
        "workflow = StateGraph(state_schema=MessagesState)"
      ],
      "metadata": {
        "id": "ji6YqX7yjlt-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Define the function that calls the model\n",
        "def call_model(state: MessagesState):\n",
        "    response = model.invoke(state[\"messages\"])\n",
        "    return {\"messages\": response}\n"
      ],
      "metadata": {
        "id": "lV_ld0FTjnUd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define the (single) node in the graph\n",
        "workflow.add_edge(START, \"model\")\n",
        "workflow.add_node(\"model\", call_model)\n",
        "\n",
        "# Add memory\n",
        "memory = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory)"
      ],
      "metadata": {
        "id": "-zE54-I9jpe1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
      ],
      "metadata": {
        "id": "WjjN5vKzjr-z"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Hi! I'm Bob.\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke({\"messages\": input_messages}, config)\n",
        "output[\"messages\"][-1].pretty_print()  # output contains all messages in state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-4iWEmEjyrE",
        "outputId": "3a2fc0e7-e109-4b70-d5da-2d7a9bea5690"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hello Bob! How can I assist you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What's my name?\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke({\"messages\": input_messages}, config)\n",
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foeJFoqmj181",
        "outputId": "89368a03-d785-40d8-bed3-f7d55d6f80fd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Your name is Bob.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"abc234\"}}\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke({\"messages\": input_messages}, config)\n",
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bz9812T1kAPd",
        "outputId": "d8ef5ea8-634f-4942-f2fb-75d6391eccaf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "I'm sorry, but I don't have access to personal information about users.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke({\"messages\": input_messages}, config)\n",
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fR2m3XR2kE4n",
        "outputId": "01a5e808-fb0a-44d8-fcfc-4a1ce0a64e37"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Your name is Bob.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt templates"
      ],
      "metadata": {
        "id": "dVIufuAnkMxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You talk like a pirate. Answer all questions to the best of your ability.\",\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "OF_fHpbzkLXK"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "workflow = StateGraph(state_schema=MessagesState)\n",
        "\n",
        "\n",
        "def call_model(state: MessagesState):\n",
        "    chain = prompt | model\n",
        "    response = chain.invoke(state)\n",
        "    return {\"messages\": response}\n",
        "\n",
        "\n",
        "workflow.add_edge(START, \"model\")\n",
        "workflow.add_node(\"model\", call_model)\n",
        "\n",
        "memory = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory)"
      ],
      "metadata": {
        "id": "5hR5OQG9kbyD"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"abc345\"}}\n",
        "query = \"Hi! I'm Jim.\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke({\"messages\": input_messages}, config)\n",
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yz03ZiZykiUt",
        "outputId": "70da8c9d-a6ef-4cd3-8583-179869002c62"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Ahoy, Jim! What be ye needin' help with today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is my name?\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke({\"messages\": input_messages}, config)\n",
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUcJoPPJkkoX",
        "outputId": "e5bf1966-6fc0-4c4b-f465-83f78c3c1d43"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Yer name be Jim, me hearty! A fine name for a pirate like yerself. Arrr!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 언어 변경"
      ],
      "metadata": {
        "id": "fAwsmcBbk4aQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "pGNUYCWTk75O"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Sequence\n",
        "\n",
        "from langchain_core.messages import BaseMessage\n",
        "from langgraph.graph.message import add_messages\n",
        "from typing_extensions import Annotated, TypedDict\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
        "    language: str\n",
        "\n",
        "\n",
        "workflow = StateGraph(state_schema=State)\n",
        "\n",
        "\n",
        "def call_model(state: State):\n",
        "    chain = prompt | model\n",
        "    response = chain.invoke(state)\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "\n",
        "workflow.add_edge(START, \"model\")\n",
        "workflow.add_node(\"model\", call_model)\n",
        "\n",
        "memory = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory)"
      ],
      "metadata": {
        "id": "w79uhcPBlA-S"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"abc456\"}}\n",
        "query = \"Hi! I'm Bob.\"\n",
        "language = \"Korean\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke(\n",
        "    {\"messages\": input_messages, \"language\": language},\n",
        "    config,\n",
        ")\n",
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLA4zB7AlEbe",
        "outputId": "0d0288a4-40a0-4e27-991e-5aed17d688f6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "안녕하세요, 밥님! 만나서 반가워요. 어떻게 도와드릴까요?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is my name?\" # 당신의 이름은? What is my name?\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke(\n",
        "    {\"messages\": input_messages},\n",
        "    config,\n",
        ")\n",
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCT7g8DLlJHE",
        "outputId": "43c83b0f-da53-4af6-f167-cfa5fdc5f519"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "당신의 이름은 밥이에요.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BNygDnbHljhZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}