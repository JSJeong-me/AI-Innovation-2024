{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSJeong-me/AI-Innovation-2024/blob/main/NLP/4-4-glove_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4IR6UCmeJXy"
      },
      "source": [
        "# GloVe (Global Vectors for Word Representation) 예제\n",
        "\n",
        "이 예제에서는 사전에 훈련된 GloVe 임베딩을 사용하여 텍스트 데이터의 단어 벡터를 불러오고 활용하는 방법을 보여줍니다. `GloVe` 자체를 직접 학습시키려면 매우 큰 말뭉치와 상당한 연산 자원이 필요하지만, 우리는 여기서 이미 훈련된 `GloVe` 벡터를 다운로드해 사용하는 방법을 살펴봅니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# GloVe 임베딩 파일 경로 설정 (glove.6B.100d.txt 다운로드 후 경로 지정)\n",
        "glove_file_path = 'glove.6B.100d.txt' # If you downloaded the file to the same directory as the script\n",
        "\n",
        "# GloVe 임베딩을 로드하는 함수\n",
        "def load_glove_embeddings(file_path):\n",
        "    embeddings = {}\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            values = line.split()\n",
        "            word = values[0]  # 첫 번째 요소는 단어\n",
        "            vector = np.asarray(values[1:], dtype='float32')  # 나머지는 벡터 값\n",
        "            embeddings[word] = vector\n",
        "    return embeddings\n",
        "\n",
        "# GloVe 임베딩 로드\n",
        "glove_embeddings = load_glove_embeddings(glove_file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRUDLEFcef_n",
        "outputId": "be015b47-f8d0-4143-e571-a5b57dadf15d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-01 14:26:57--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2024-10-01 14:26:57--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2024-10-01 14:26:57--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.01MB/s    in 2m 39s  \n",
            "\n",
            "2024-10-01 14:29:36 (5.18 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OnFezBEhfSBO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# GloVe 임베딩 파일 경로 설정 (glove.6B.100d.txt 다운로드 후 경로 지정)\n",
        "glove_file_path = 'glove.6B.100d.txt' # If you downloaded the file to the same directory as the script\n",
        "\n",
        "# GloVe 임베딩을 로드하는 함수\n",
        "def load_glove_embeddings(file_path):\n",
        "    embeddings = {}\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            values = line.split()\n",
        "            word = values[0]  # 첫 번째 요소는 단어\n",
        "            vector = np.asarray(values[1:], dtype='float32')  # 나머지는 벡터 값\n",
        "            embeddings[word] = vector\n",
        "    return embeddings\n",
        "\n",
        "# GloVe 임베딩 로드\n",
        "glove_embeddings = load_glove_embeddings(glove_file_path)"
      ],
      "metadata": {
        "id": "QVd5zscxeXXl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATpkAmleeJX2"
      },
      "source": [
        "## 단어 벡터 얻기\n",
        "특정 단어의 벡터를 가져올 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5OUihqfeJX2",
        "outputId": "53f7720f-05c5-4028-a9a0-5e4744763201"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector for 'king': [-0.32307  -0.87616   0.21977   0.25268   0.22976   0.7388   -0.37954\n",
            " -0.35307  -0.84369  -1.1113   -0.30266   0.33178  -0.25113   0.30448\n",
            " -0.077491 -0.89815   0.092496 -1.1407   -0.58324   0.66869  -0.23122\n",
            " -0.95855   0.28262  -0.078848  0.75315   0.26584   0.3422   -0.33949\n",
            "  0.95608   0.065641  0.45747   0.39835   0.57965   0.39267  -0.21851\n",
            "  0.58795  -0.55999   0.63368  -0.043983 -0.68731  -0.37841   0.38026\n",
            "  0.61641  -0.88269  -0.12346  -0.37928  -0.38318   0.23868   0.6685\n",
            " -0.43321  -0.11065   0.081723  1.1569    0.78958  -0.21223  -2.3211\n",
            " -0.67806   0.44561   0.65707   0.1045    0.46217   0.19912   0.25802\n",
            "  0.057194  0.53443  -0.43133  -0.34311   0.59789  -0.58417   0.068995\n",
            "  0.23944  -0.85181   0.30379  -0.34177  -0.25746  -0.031101 -0.16285\n",
            "  0.45169  -0.91627   0.64521   0.73281  -0.22752   0.30226   0.044801\n",
            " -0.83741   0.55006  -0.52506  -1.7357    0.4751   -0.70487   0.056939\n",
            " -0.7132    0.089623  0.41394  -1.3363   -0.61915  -0.33089  -0.52881\n",
            "  0.16483  -0.98878 ]\n"
          ]
        }
      ],
      "source": [
        "# 단어 벡터 얻기\n",
        "def get_word_vector(word, embeddings):\n",
        "    if word in embeddings:\n",
        "        return embeddings[word]\n",
        "    else:\n",
        "        return f\"'{word}' not found in GloVe embeddings.\"\n",
        "\n",
        "# 예제: 'king'의 벡터 얻기\n",
        "king_vector = get_word_vector('king', glove_embeddings)\n",
        "print(f\"Vector for 'king': {king_vector}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGhuNQS4eJX2"
      },
      "source": [
        "## 단어 간 유사도 측정\n",
        "코사인 유사도를 사용하여 두 단어 간의 유사도를 계산합니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity # Make sure to import the cosine_similarity function from sklearn\n",
        "\n",
        "# GloVe 임베딩 파일 경로 설정 (glove.6B.100d.txt 다운로드 후 경로 지정)\n",
        "glove_file_path = 'glove.6B.100d.txt' # If you downloaded the file to the same directory as the script\n",
        "\n",
        "# GloVe 임베딩을 로드하는 함수\n",
        "def load_glove_embeddings(file_path):\n",
        "    embeddings = {}\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            values = line.split()\n",
        "            word = values[0]  # 첫 번째 요소는 단어\n",
        "            vector = np.asarray(values[1:], dtype='float32')  # 나머지는 벡터 값\n",
        "            embeddings[word] = vector\n",
        "    return embeddings\n",
        "\n",
        "# GloVe 임베딩 로드\n",
        "glove_embeddings = load_glove_embeddings(glove_file_path)\n",
        "\n",
        "# 단어 벡터 얻기\n",
        "def get_word_vector(word, embeddings):\n",
        "    if word in embeddings:\n",
        "        return embeddings[word]\n",
        "    else:\n",
        "        return f\"'{word}' not found in GloVe embeddings.\"\n",
        "\n",
        "# 예제: 'king'의 벡터 얻기\n",
        "king_vector = get_word_vector('king', glove_embeddings)\n",
        "print(f\"Vector for 'king': {king_vector}\")\n",
        "\n",
        "# 단어 유사도 측정\n",
        "def word_similarity(word1, word2, embeddings): # Changed function name to word_similarity to avoid conflict\n",
        "    if word1 in embeddings and word2 in embeddings:\n",
        "        vec1 = embeddings[word1].reshape(1, -1)\n",
        "        vec2 = embeddings[word2].reshape(1, -1)\n",
        "        return cosine_similarity(vec1, vec2)[0][0] # This now refers to the cosine_similarity function from sklearn\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# 예제: 'king'과 'queen'의 유사도 측정\n",
        "similarity = word_similarity('king', 'queen', glove_embeddings) # Call the new function name\n",
        "print(f\"Similarity between 'king' and 'queen': {similarity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZW8vkbkftO8",
        "outputId": "abe0fd00-141a-4328-ef55-3a24a62cb6c9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector for 'king': [-0.32307  -0.87616   0.21977   0.25268   0.22976   0.7388   -0.37954\n",
            " -0.35307  -0.84369  -1.1113   -0.30266   0.33178  -0.25113   0.30448\n",
            " -0.077491 -0.89815   0.092496 -1.1407   -0.58324   0.66869  -0.23122\n",
            " -0.95855   0.28262  -0.078848  0.75315   0.26584   0.3422   -0.33949\n",
            "  0.95608   0.065641  0.45747   0.39835   0.57965   0.39267  -0.21851\n",
            "  0.58795  -0.55999   0.63368  -0.043983 -0.68731  -0.37841   0.38026\n",
            "  0.61641  -0.88269  -0.12346  -0.37928  -0.38318   0.23868   0.6685\n",
            " -0.43321  -0.11065   0.081723  1.1569    0.78958  -0.21223  -2.3211\n",
            " -0.67806   0.44561   0.65707   0.1045    0.46217   0.19912   0.25802\n",
            "  0.057194  0.53443  -0.43133  -0.34311   0.59789  -0.58417   0.068995\n",
            "  0.23944  -0.85181   0.30379  -0.34177  -0.25746  -0.031101 -0.16285\n",
            "  0.45169  -0.91627   0.64521   0.73281  -0.22752   0.30226   0.044801\n",
            " -0.83741   0.55006  -0.52506  -1.7357    0.4751   -0.70487   0.056939\n",
            " -0.7132    0.089623  0.41394  -1.3363   -0.61915  -0.33089  -0.52881\n",
            "  0.16483  -0.98878 ]\n",
            "Similarity between 'king' and 'queen': 0.7508\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8muKAufeJX3"
      },
      "source": [
        "## 유사한 단어 찾기\n",
        "주어진 단어와 가장 유사한 단어를 찾습니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 유사한 단어 찾기\n",
        "def find_most_similar(word, embeddings, top_n=5):\n",
        "    if word not in embeddings:\n",
        "        return f\"'{word}' not found in GloVe embeddings.\"\n",
        "\n",
        "    word_vec = embeddings[word].reshape(1, -1)\n",
        "    similarities = {}\n",
        "\n",
        "    for other_word, other_vec in embeddings.items():\n",
        "        if other_word != word:\n",
        "            other_vec = other_vec.reshape(1, -1)\n",
        "            similarity = word_similarity(word, other_word, embeddings) # Call word_similarity to calculate cosine similarity between word vectors\n",
        "            similarities[other_word] = similarity\n",
        "\n",
        "    # 유사도에 따라 단어를 정렬하고 상위 N개를 반환\n",
        "    most_similar_words = sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
        "    return most_similar_words\n",
        "\n",
        "# 예제: 'king'과 유사한 단어 찾기\n",
        "similar_words = find_most_similar('king', glove_embeddings, top_n=5)\n",
        "print(f\"Words most similar to 'king': {similar_words}\")"
      ],
      "metadata": {
        "id": "a_ENStdWgCsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELcJO9vReJX3"
      },
      "source": [
        "## 단어 벡터 연산\n",
        "GloVe 벡터를 사용해 간단한 벡터 산술 연산을 수행해 봅니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoyiR7GCeJX3"
      },
      "outputs": [],
      "source": [
        "# 단어 벡터 연산 예제\n",
        "def vector_arithmetic(word1, word2, word3, embeddings):\n",
        "    if word1 in embeddings and word2 in embeddings and word3 in embeddings:\n",
        "        result_vector = embeddings[word1] - embeddings[word2] + embeddings[word3]\n",
        "        similarities = {}\n",
        "        for other_word, other_vec in embeddings.items():\n",
        "            other_vec = other_vec.reshape(1, -1)\n",
        "            result_vec = result_vector.reshape(1, -1)\n",
        "            similarity = cosine_similarity(other_vec, result_vec)\n",
        "            similarities[other_word] = similarity\n",
        "\n",
        "        # 결과 벡터와 가장 유사한 단어 찾기\n",
        "        most_similar_word = max(similarities, key=similarities.get)\n",
        "        return most_similar_word\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# 예제: 'king' - 'man' + 'woman'과 가장 유사한 단어 찾기\n",
        "result = vector_arithmetic('king', 'man', 'woman', glove_embeddings)\n",
        "print(f\"Result of 'king' - 'man' + 'woman': {result}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}