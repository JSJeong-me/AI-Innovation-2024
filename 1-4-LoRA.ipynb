{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkF/TsTbP5rbps/TPtT13T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSJeong-me/AI-Innovation-2024/blob/main/1-4-LoRA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Low-Rank Adaptation (LoRA)은 대형 신경망, 특히 트랜스포머 모델의 미세 조정(fine-tuning) 과정에서 훈련해야 하는 파라미터 수를 줄이기 위해 사용되는 기법입니다. 이는 가중치 행렬을 저순위(low-rank) 행렬로 분해하여 계산 부담을 줄이는 방식으로 이루어집니다.\n",
        "\n",
        "아래는 PyTorch를 사용하여 LoRA를 단일 선형 계층에 적용하는 기본 예제입니다."
      ],
      "metadata": {
        "id": "-2TaHeBJYYEP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DONnqq3VYWH1",
        "outputId": "708219e9-540a-475d-a4f2-5ef068321091"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([10, 64])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LoRALinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, rank=4):\n",
        "        super(LoRALinear, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.rank = rank\n",
        "\n",
        "        # 기존 가중치\n",
        "        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n",
        "\n",
        "        # 저순위 분해 행렬들\n",
        "        self.lora_A = nn.Parameter(torch.randn(rank, in_features))\n",
        "        self.lora_B = nn.Parameter(torch.randn(out_features, rank))\n",
        "\n",
        "        # 초기 LoRA 레이어의 효과가 작도록 보장하는 스케일링 인자\n",
        "        self.scaling = 1.0 / rank\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 기존 선형 계층의 계산\n",
        "        output = torch.matmul(x, self.weight.t())\n",
        "\n",
        "        # LoRA 조정\n",
        "        lora_adjustment = torch.matmul(x, self.lora_A.t())\n",
        "        lora_adjustment = torch.matmul(lora_adjustment, self.lora_B.t())\n",
        "\n",
        "        # 원래 출력에 LoRA 조정을 더함\n",
        "        output = output + self.scaling * lora_adjustment\n",
        "        return output\n",
        "\n",
        "# 사용 예시\n",
        "if __name__ == \"__main__\":\n",
        "    # LoRA가 적용된 선형 계층 정의\n",
        "    lora_layer = LoRALinear(in_features=128, out_features=64, rank=4)\n",
        "\n",
        "    # 입력 텐서 예제 (128개의 특징을 가진 10개의 샘플로 구성된 배치)\n",
        "    x = torch.randn(10, 128)\n",
        "\n",
        "    # LoRA가 적용된 선형 계층을 통한 순전파 계산\n",
        "    output = lora_layer(x)\n",
        "\n",
        "    print(f\"Output shape: {output.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LoRALinear 클래스:\n",
        "\n",
        "in_features와 out_features: 이 값들은 선형 계층의 입력 및 출력 차원을 정의합니다.\n",
        "기존 가중치 행렬: 일반적인 선형 계층에서 사용하는 가중치 행렬(self.weight)입니다.\n",
        "LoRA 분해:\n",
        "lora_A: [rank, in_features] 형태의 저순위 행렬입니다.\n",
        "lora_B: [out_features, rank] 형태의 또 다른 저순위 행렬입니다.\n",
        "스케일링 인자: LoRA의 초기 효과가 작도록 하여 훈련의 안정성을 높입니다.\n",
        "순전파 (Forward Pass):\n",
        "\n",
        "순전파 계산은 self.weight를 사용하여 일반적인 선형 변환을 계산하는 것에서 시작합니다.\n",
        "이후, LoRA 조정은 lora_A로 x를 변환한 후, 결과를 lora_B로 다시 변환하여 계산됩니다.\n",
        "최종 출력은 기존 변환 결과에 스케일링된 LoRA 조정을 더한 값이 됩니다.\n",
        "사용법:\n",
        "\n",
        "LoRALinear 클래스는 일반적인 nn.Linear 계층처럼 사용됩니다. 이 예제에서는 입력 텐서 x가 LoRA가 적용된 선형 계층을 통과합니다.\n",
        "LoRA의 장점:\n",
        "파라미터 효율성: LoRA는 전체 가중치 행렬을 직접 훈련하는 대신 저순위 행렬을 사용하여 훈련해야 하는 파라미터 수를 줄입니다.\n",
        "메모리 및 계산 효율성: 파라미터 수가 줄어들면 메모리 사용량과 계산 요구 사항이 줄어들어 대형 모델을 미세 조정하기가 쉬워집니다.\n",
        "이 예제는 LoRA의 개념을 설명하기 위한 간단한 예시입니다. 실제로는 더 복잡한 모델(예: 트랜스포머)에 적용되며, 더 정교한 스케일링 및 정규화 기법이 포함될 수 있습니다."
      ],
      "metadata": {
        "id": "Xtn_eH6GYmjd"
      }
    }
  ]
}