{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTAMqS2hkyoDTRVHDM32ku",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSJeong-me/AI-Innovation-2024/blob/main/Transformer/5-1-Multi-Head-Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing the Scaled-Dot Product Attention\n",
        "from tensorflow import math, matmul, reshape, shape, transpose, cast, float32\n",
        "from tensorflow.keras.layers import Dense, Layer\n",
        "from tensorflow.keras.activations import softmax # Import softmax from tf.keras.activations\n",
        "\n",
        "# Implementing the Scaled-Dot Product Attention\n",
        "class DotProductAttention(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(DotProductAttention, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, queries, keys, values, d_k, mask=None):\n",
        "        # Scoring the queries against the keys after transposing the latter, and scaling\n",
        "        scores = matmul(queries, keys, transpose_b=True) / math.sqrt(cast(d_k, float32))\n",
        "\n",
        "        # Apply mask to the attention scores\n",
        "        if mask is not None:\n",
        "            scores += -1e9 * mask\n",
        "\n",
        "        # Computing the weights by a softmax operation\n",
        "        weights = softmax(scores) # Use softmax from tf.keras.activations\n",
        "\n",
        "        # Computing the attention by a weighted sum of the value vectors\n",
        "        return matmul(weights, values)\n",
        "\n",
        "# ... (rest of the code remains the same)\n",
        "\n",
        "# Implementing the Multi-Head Attention\n",
        "class MultiHeadAttention(Layer):\n",
        "    def __init__(self, h, d_k, d_v, d_model, **kwargs):\n",
        "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
        "        self.attention = DotProductAttention()  # Scaled dot product attention\n",
        "        self.heads = h  # Number of attention heads to use\n",
        "        self.d_k = d_k  # Dimensionality of the linearly projected queries and keys\n",
        "        self.d_v = d_v  # Dimensionality of the linearly projected values\n",
        "        self.d_model = d_model  # Dimensionality of the model\n",
        "        self.W_q = Dense(d_k)  # Learned projection matrix for the queries\n",
        "        self.W_k = Dense(d_k)  # Learned projection matrix for the keys\n",
        "        self.W_v = Dense(d_v)  # Learned projection matrix for the values\n",
        "        self.W_o = Dense(d_model)  # Learned projection matrix for the multi-head output\n",
        "\n",
        "    def reshape_tensor(self, x, heads, flag):\n",
        "        if flag:\n",
        "            # Tensor shape after reshaping and transposing: (batch_size, heads, seq_length, -1)\n",
        "            x = reshape(x, shape=(shape(x)[0], shape(x)[1], heads, -1))\n",
        "            x = transpose(x, perm=(0, 2, 1, 3))\n",
        "        else:\n",
        "            # Reverting the reshaping and transposing operations: (batch_size, seq_length, d_k)\n",
        "            x = transpose(x, perm=(0, 2, 1, 3))\n",
        "            x = reshape(x, shape=(shape(x)[0], shape(x)[1], self.d_k))\n",
        "        return x\n",
        "\n",
        "    def call(self, queries, keys, values, mask=None):\n",
        "        # Rearrange the queries to be able to compute all heads in parallel\n",
        "        q_reshaped = self.reshape_tensor(self.W_q(queries), self.heads, True)\n",
        "\n",
        "        # Compute the multi-head attention output using the reshaped queries, keys and values\n",
        "\n",
        "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
        "\n",
        "        # Rearrange the keys to be able to compute all heads in parallel\n",
        "        k_reshaped = self.reshape_tensor(self.W_k(keys), self.heads, True)\n",
        "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
        "\n",
        "        # Rearrange the values to be able to compute all heads in parallel\n",
        "        v_reshaped = self.reshape_tensor(self.W_v(values), self.heads, True)\n",
        "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
        "\n",
        "        # Compute the multi-head attention output using the reshaped queries, keys and values\n",
        "        o_reshaped = self.attention(q_reshaped, k_reshaped, v_reshaped, mask=mask, d_k=self.d_k) # Pass d_k as keyword argument\n",
        "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
        "\n",
        "        # Rearrange back the output into concatenated form\n",
        "        output = self.reshape_tensor(o_reshaped, self.heads, False)\n",
        "        # Resulting tensor shape: (batch_size, input_seq_length, d_v)\n",
        "\n",
        "        # Apply one final linear projection to the output to generate the multi-head attention\n",
        "        # Resulting tensor shape: (batch_size, input_seq_length, d_model)\n",
        "        return self.W_o(output)"
      ],
      "metadata": {
        "id": "4T9QvohicbHJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import random\n",
        "\n",
        "input_seq_length = 5  # Maximum length of the input sequence\n",
        "h = 8  # Number of self-attention heads\n",
        "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
        "d_v = 64  # Dimensionality of the linearly projected values\n",
        "d_model = 512  # Dimensionality of the model sub-layers' outputs\n",
        "batch_size = 64  # Batch size from the training process\n",
        "\n",
        "queries = random.random((batch_size, input_seq_length, d_k))\n",
        "keys = random.random((batch_size, input_seq_length, d_k))\n",
        "values = random.random((batch_size, input_seq_length, d_v))\n",
        "\n",
        "multihead_attention = MultiHeadAttention(h, d_k, d_v, d_model)\n",
        "print(multihead_attention(queries, keys, values))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDmpg95AcfZh",
        "outputId": "279f67b6-26f6-43d4-daa3-cc3bf83cd037"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[-6.31234720e-02  1.18290134e-01  1.13987789e-01 ... -6.44244924e-02\n",
            "   -1.37954816e-01 -2.35938966e-01]\n",
            "  [-6.16301782e-02  1.18591234e-01  1.15305856e-01 ... -6.20990470e-02\n",
            "   -1.39237195e-01 -2.35218495e-01]\n",
            "  [-6.20680116e-02  1.18751526e-01  1.13772482e-01 ... -6.22441210e-02\n",
            "   -1.39342666e-01 -2.35152274e-01]\n",
            "  [-6.18725643e-02  1.17973648e-01  1.13956407e-01 ... -6.29143566e-02\n",
            "   -1.38948768e-01 -2.34578684e-01]\n",
            "  [-6.08854219e-02  1.17912218e-01  1.13320045e-01 ... -5.95226660e-02\n",
            "   -1.40971050e-01 -2.34650612e-01]]\n",
            "\n",
            " [[-1.97930187e-02  1.00369148e-01 -7.84399062e-02 ... -2.75027975e-02\n",
            "   -7.85642043e-02 -2.54923165e-01]\n",
            "  [-1.90120991e-02  9.56457704e-02 -7.70042837e-02 ... -2.85565779e-02\n",
            "   -7.76824057e-02 -2.51348257e-01]\n",
            "  [-1.88432019e-02  9.84462798e-02 -8.09082910e-02 ... -3.05076148e-02\n",
            "   -7.95387030e-02 -2.55302340e-01]\n",
            "  [-1.81312840e-02  9.61948708e-02 -7.79727623e-02 ... -2.90170442e-02\n",
            "   -7.90269375e-02 -2.54960388e-01]\n",
            "  [-1.70641411e-02  1.00413300e-01 -7.69554898e-02 ... -2.46741045e-02\n",
            "   -7.86117315e-02 -2.52843589e-01]]\n",
            "\n",
            " [[-4.22053002e-02  9.22721252e-02  5.76914959e-02 ... -1.66009031e-02\n",
            "   -4.82611805e-02 -1.87053487e-01]\n",
            "  [-4.25796397e-02  9.01529714e-02  5.44119962e-02 ... -1.45705631e-02\n",
            "   -4.53673527e-02 -1.87574536e-01]\n",
            "  [-4.16367427e-02  9.21445861e-02  5.52480519e-02 ... -1.46897612e-02\n",
            "   -4.72321510e-02 -1.86325833e-01]\n",
            "  [-4.24465574e-02  8.81650150e-02  5.32129705e-02 ... -1.66372228e-02\n",
            "   -4.63914797e-02 -1.85209841e-01]\n",
            "  [-4.22915779e-02  9.11342427e-02  5.42722233e-02 ... -1.80494115e-02\n",
            "   -4.67700623e-02 -1.88510031e-01]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 1.26528472e-01  3.40447396e-01 -1.43647149e-01 ...  1.14697339e-02\n",
            "   -9.68038514e-02 -2.30021387e-01]\n",
            "  [ 1.25023216e-01  3.37125063e-01 -1.49339348e-01 ...  1.04896566e-02\n",
            "   -9.42520052e-02 -2.28603229e-01]\n",
            "  [ 1.25850409e-01  3.34577411e-01 -1.49702772e-01 ...  1.25766806e-02\n",
            "   -9.57199559e-02 -2.30575204e-01]\n",
            "  [ 1.25739172e-01  3.38768154e-01 -1.49473086e-01 ...  1.40932621e-02\n",
            "   -9.30372328e-02 -2.28013575e-01]\n",
            "  [ 1.25458986e-01  3.39621842e-01 -1.43921614e-01 ...  1.28500676e-02\n",
            "   -9.65382010e-02 -2.31837407e-01]]\n",
            "\n",
            " [[ 6.34864196e-02  2.79077552e-02  2.96545606e-02 ... -5.70879318e-02\n",
            "   -4.01595682e-02 -2.93649733e-01]\n",
            "  [ 6.29330799e-02  2.91049965e-02  3.09054274e-02 ... -5.27981035e-02\n",
            "   -4.31797951e-02 -2.94904828e-01]\n",
            "  [ 6.16988726e-02  3.01788524e-02  3.06185279e-02 ... -5.39527088e-02\n",
            "   -3.95385772e-02 -2.91864872e-01]\n",
            "  [ 6.41533285e-02  3.13671306e-02  3.12707573e-02 ... -5.84396012e-02\n",
            "   -4.01644222e-02 -2.90889472e-01]\n",
            "  [ 6.12799823e-02  3.07772011e-02  2.92652119e-02 ... -5.43464795e-02\n",
            "   -4.23089601e-02 -2.97223508e-01]]\n",
            "\n",
            " [[-4.29373048e-02  1.51017725e-01  3.22536156e-02 ... -1.16620967e-02\n",
            "   -1.46666818e-04 -1.43692732e-01]\n",
            "  [-4.28646207e-02  1.50541559e-01  3.03113107e-02 ... -1.04258331e-02\n",
            "   -3.14828561e-04 -1.46747962e-01]\n",
            "  [-4.30205949e-02  1.48510844e-01  3.09187304e-02 ... -1.52322901e-02\n",
            "    1.68129976e-03 -1.44131288e-01]\n",
            "  [-4.16632332e-02  1.50681436e-01  3.10343467e-02 ... -1.08789327e-02\n",
            "    7.91572325e-04 -1.45871356e-01]\n",
            "  [-4.12776843e-02  1.49928406e-01  3.15080769e-02 ... -1.28549132e-02\n",
            "    2.16128724e-03 -1.40272245e-01]]], shape=(64, 5, 512), dtype=float32)\n"
          ]
        }
      ]
    }
  ]
}