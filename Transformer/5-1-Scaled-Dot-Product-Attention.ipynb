{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMSr8xOP7g+YwAv0qEtmPEG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSJeong-me/AI-Innovation-2024/blob/main/Transformer/5-1-Scaled-Dot-Product-Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import matmul, math, cast, float32\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.activations import softmax # Import softmax from tensorflow.keras.activations\n",
        "\n",
        "# Implementing the Scaled-Dot Product Attention\n",
        "class DotProductAttention(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(DotProductAttention, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, queries, keys, values, d_k, mask=None):\n",
        "        # Scoring the queries against the keys after transposing the latter, and scaling\n",
        "        scores = matmul(queries, keys, transpose_b=True) / math.sqrt(cast(d_k, float32))\n",
        "\n",
        "        # Apply mask to the attention scores\n",
        "        if mask is not None:\n",
        "            scores += -1e9 * mask\n",
        "\n",
        "        # Computing the weights by a softmax operation\n",
        "        weights = softmax(scores) # Use softmax from tensorflow.keras.activations\n",
        "\n",
        "        # Computing the attention by a weighted sum of the value vectors\n",
        "        return matmul(weights, values)"
      ],
      "metadata": {
        "id": "fjgwWoA0YISN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import random\n",
        "\n",
        "input_seq_length = 5  # Maximum length of the input sequence\n",
        "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
        "d_v = 64  # Dimensionality of the linearly projected values\n",
        "batch_size = 64  # Batch size from the training process\n",
        "\n",
        "queries = random.random((batch_size, input_seq_length, d_k))\n",
        "keys = random.random((batch_size, input_seq_length, d_k))\n",
        "values = random.random((batch_size, input_seq_length, d_v))\n",
        "\n",
        "attention = DotProductAttention()\n",
        "# Pass d_k as a keyword argument\n",
        "print(attention(queries, keys, values, d_k=d_k))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcPwPXLDYR5D",
        "outputId": "6c9411f3-41ca-49ae-e506-af1ac64313e9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[0.406152   0.6481798  0.61734873 ... 0.3330139  0.62651014 0.47233817]\n",
            "  [0.41655076 0.6454326  0.61695933 ... 0.347259   0.62359726 0.4805547 ]\n",
            "  [0.41623163 0.6274299  0.6196872  ... 0.32457232 0.6173672  0.48080087]\n",
            "  [0.40989777 0.6294844  0.61528504 ... 0.32892424 0.6237523  0.47626266]\n",
            "  [0.42396507 0.6435493  0.6124284  ... 0.3489979  0.62224984 0.4792506 ]]\n",
            "\n",
            " [[0.6930096  0.6724482  0.6079855  ... 0.4888395  0.33188948 0.57895654]\n",
            "  [0.69275737 0.6841609  0.6082085  ... 0.47907    0.33110777 0.58557   ]\n",
            "  [0.68699646 0.6717293  0.6222161  ... 0.49429387 0.3361022  0.57459044]\n",
            "  [0.6810884  0.67848057 0.62003136 ... 0.4848075  0.35164532 0.5719061 ]\n",
            "  [0.6803761  0.6697084  0.62488127 ... 0.49267438 0.35136554 0.56842065]]\n",
            "\n",
            " [[0.7187425  0.59889805 0.4584425  ... 0.5093596  0.24952105 0.52193904]\n",
            "  [0.7213501  0.5821472  0.4867586  ... 0.5209122  0.2686088  0.5374378 ]\n",
            "  [0.7069361  0.6041473  0.46155518 ... 0.5231636  0.2664616  0.567302  ]\n",
            "  [0.72728294 0.603428   0.4743971  ... 0.5058012  0.268442   0.5455024 ]\n",
            "  [0.713943   0.5718584  0.47501254 ... 0.5277738  0.2635711  0.55075896]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.6605303  0.52539945 0.83875483 ... 0.6306731  0.4695051  0.3748374 ]\n",
            "  [0.6729783  0.53602034 0.8397557  ... 0.62838304 0.46666366 0.37433544]\n",
            "  [0.65513146 0.51189    0.83383083 ... 0.63301766 0.47826162 0.38086247]\n",
            "  [0.66352206 0.55765355 0.8392685  ... 0.61056864 0.47690386 0.37948477]\n",
            "  [0.65389144 0.5399141  0.83925354 ... 0.61800426 0.4748148  0.37666965]]\n",
            "\n",
            " [[0.44577456 0.63721204 0.596852   ... 0.5178017  0.7892168  0.6440378 ]\n",
            "  [0.45951548 0.64772767 0.6070827  ... 0.536088   0.790571   0.62468696]\n",
            "  [0.45042497 0.6364297  0.6058798  ... 0.5210146  0.79710734 0.6269651 ]\n",
            "  [0.43936712 0.63222545 0.5902449  ... 0.5091993  0.7885753  0.65053916]\n",
            "  [0.4459716  0.6349148  0.6014142  ... 0.5186634  0.7956792  0.6316007 ]]\n",
            "\n",
            " [[0.3265818  0.6837325  0.501327   ... 0.65804005 0.35647684 0.5788245 ]\n",
            "  [0.3198621  0.67010045 0.5060191  ... 0.673699   0.3454723  0.5714978 ]\n",
            "  [0.32015923 0.65167844 0.48572627 ... 0.6676252  0.36101016 0.59613484]\n",
            "  [0.32294995 0.67194784 0.4858492  ... 0.66088116 0.3609538  0.5985625 ]\n",
            "  [0.31903028 0.65299255 0.49267477 ... 0.67527133 0.34983605 0.59944314]]], shape=(64, 5, 64), dtype=float32)\n"
          ]
        }
      ]
    }
  ]
}