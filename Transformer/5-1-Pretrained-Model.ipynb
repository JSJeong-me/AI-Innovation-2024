{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSJeong-me/AI-Innovation-2024/blob/main/Transformer/5-1-Pretrained-Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af366c49-944d-4ad3-9bf9-cf0b5e386cc6",
      "metadata": {
        "id": "af366c49-944d-4ad3-9bf9-cf0b5e386cc6"
      },
      "source": [
        "## 1. Install dependencies and fix seed\n",
        "\n",
        "Welcome to Lesson 1!\n",
        "\n",
        "If you would like to access the `requirements.txt` file for this course, go to `File` and click on `Open`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb03d167-6ebc-4da9-87c8-11ce1bd4eeaa",
      "metadata": {
        "height": 47,
        "id": "bb03d167-6ebc-4da9-87c8-11ce1bd4eeaa"
      },
      "outputs": [],
      "source": [
        "# Install any packages if it does not exist\n",
        "!pip install transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f3d6b90b-7f1f-4f1f-a5fb-47bea82d3896",
      "metadata": {
        "height": 64,
        "id": "f3d6b90b-7f1f-4f1f-a5fb-47bea82d3896"
      },
      "outputs": [],
      "source": [
        "# Ignore insignificant warnings (ex: deprecations)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b23a1a8-096f-4563-ab87-bbf3c2e699a2",
      "metadata": {
        "height": 183,
        "id": "8b23a1a8-096f-4563-ab87-bbf3c2e699a2"
      },
      "outputs": [],
      "source": [
        "# Set a seed for reproducibility\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# 1. Tokenizer와 모델 불러오기\n",
        "# model_name = \"yanolja/EEVE-Korean-Instruct-10.8B-v1.0\"\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# 2. 프롬프트 정의\n",
        "prompt = \"한국의 전통 음식은 무엇인가요?\"\n",
        "\n",
        "# 3. 입력 텍스트를 토큰화\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# 4. 모델 예측 실행\n",
        "output = model.generate(**inputs, max_length=100, do_sample=True, top_p=0.9, temperature=0.7)\n",
        "\n",
        "# 5. 결과 디코딩\n",
        "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"모델의 응답:\", response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d4d6924-b8a0-4e5b-a66d-e316e14a564d",
      "metadata": {
        "id": "3d4d6924-b8a0-4e5b-a66d-e316e14a564d"
      },
      "source": [
        "## google/gemma-2b Generate text samples\n",
        "\n",
        "https://huggingface.co/google/gemma-2b\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install sentencepiece huggingface_hub # LLaMA 계열 모델에 필요"
      ],
      "metadata": {
        "id": "3KLAppntsVrW"
      },
      "id": "3KLAppntsVrW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face 토큰 설정 (필요 시 사용)\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "# 비밀 정보에서 Hugging Face 토큰 가져오기\n",
        "huggingface_token = userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "Erf9ajDmvd52"
      },
      "id": "Erf9ajDmvd52",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\")\n",
        "\n",
        "input_text = \"Write me a poem about Machine Learning.\"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "outputs = model.generate(**input_ids)\n",
        "print(tokenizer.decode(outputs[0]))\n"
      ],
      "metadata": {
        "id": "jFRgWBAevZiO"
      },
      "id": "jFRgWBAevZiO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# 프롬프트 설정\n",
        "\n",
        "input_tex = \"I am an engineer. I love\"\n",
        "\n"
      ],
      "metadata": {
        "id": "DGXzkxCGu1XV"
      },
      "id": "DGXzkxCGu1XV",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "outputs = model.generate(**input_ids)\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1WeRdL11aLc",
        "outputId": "de65b6c2-d4b9-40dd-e088-693ce125fd3b"
      },
      "id": "l1WeRdL11aLc",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos>Write me a poem about Machine Learning.\n",
            "\n",
            "I’m not sure what you mean by “\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e29640cf-3429-4e0c-be9c-a74833ddc29b",
      "metadata": {
        "id": "e29640cf-3429-4e0c-be9c-a74833ddc29b"
      },
      "source": [
        "## 4. Generate Python samples with pretrained general model\n",
        "\n",
        "Use the model to write a python function called `find_max()` that finds the maximum value in a list of numbers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "e87d6a02-ec21-4352-8dec-203bf2cacf16",
      "metadata": {
        "height": 30,
        "id": "e87d6a02-ec21-4352-8dec-203bf2cacf16"
      },
      "outputs": [],
      "source": [
        "input_tex =  \"def find_max(numbers):\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "outputs = model.generate(**input_ids)\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lh1dyz-u1nN1",
        "outputId": "6649f584-20fd-4232-dcc1-81dde457ea2c"
      },
      "id": "Lh1dyz-u1nN1",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos>Write me a poem about Machine Learning.\n",
            "\n",
            "I’m not sure what you mean by “\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZL6Yft0n18hs"
      },
      "id": "ZL6Yft0n18hs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running the model on a single / multi GPU\n"
      ],
      "metadata": {
        "id": "H1I0Ubb11_xE"
      },
      "id": "H1I0Ubb11_xE"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "sSZDyS7v2PjH"
      },
      "id": "sSZDyS7v2PjH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", device_map=\"auto\")\n",
        "\n",
        "input_text = \"Write me a poem about Machine Learning.\"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**input_ids)\n",
        "print(tokenizer.decode(outputs[0]))\n"
      ],
      "metadata": {
        "id": "cAXLLT1q2CQb"
      },
      "id": "cAXLLT1q2CQb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## google/gemma-2-2b-it"
      ],
      "metadata": {
        "id": "-3EI14_H22wg"
      },
      "id": "-3EI14_H22wg"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers\n",
        "!pip install -U accelerate\n",
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "id": "LlJF0vKA28Si"
      },
      "id": "LlJF0vKA28Si",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"google/gemma-2-2b-it\",\n",
        "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
        "    device=\"cuda\",  # replace with \"mps\" to run on a Mac device\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Who are you? Please, answer in pirate-speak.\"},\n",
        "]\n",
        "\n",
        "outputs = pipe(messages, max_new_tokens=256)\n",
        "assistant_response = outputs[0][\"generated_text\"][-1][\"content\"].strip()\n",
        "print(assistant_response)\n",
        "# Ahoy, matey! I be Gemma, a digital scallywag, a language-slingin' parrot of the digital seas. I be here to help ye with yer wordy woes, answer yer questions, and spin ye yarns of the digital world.  So, what be yer pleasure, eh? 🦜\n"
      ],
      "metadata": {
        "id": "z5_apAfh22d_"
      },
      "id": "z5_apAfh22d_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SoaoE0z12CI3"
      },
      "id": "SoaoE0z12CI3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## prompt =  \"def find_max(numbers):\""
      ],
      "metadata": {
        "id": "Fx9lMLWx3qC8"
      },
      "id": "Fx9lMLWx3qC8"
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"def find_max(numbers):\"},\n",
        "]\n",
        "\n",
        "outputs = pipe(messages, max_new_tokens=256)\n",
        "assistant_response = outputs[0][\"generated_text\"][-1][\"content\"].strip()\n",
        "print(assistant_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUxndxA32B6J",
        "outputId": "d2009f88-e53f-4b41-bca6-3efa2f4520c3"
      },
      "id": "kUxndxA32B6J",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```python\n",
            "def find_max(numbers):\n",
            "  \"\"\"\n",
            "  This function finds the maximum number in a list.\n",
            "\n",
            "  Args:\n",
            "    numbers: A list of numbers.\n",
            "\n",
            "  Returns:\n",
            "    The maximum number in the list.\n",
            "  \"\"\"\n",
            "  if not numbers:\n",
            "    return None  # Handle empty list case\n",
            "  max_number = numbers[0]  # Initialize with the first element\n",
            "  for number in numbers:\n",
            "    if number > max_number:\n",
            "      max_number = number\n",
            "  return max_number\n",
            "```\n",
            "\n",
            "**Explanation:**\n",
            "\n",
            "1. **Function Definition:**\n",
            "   - `def find_max(numbers):` defines a function named `find_max` that takes a list of numbers (`numbers`) as input.\n",
            "\n",
            "2. **Empty List Handling:**\n",
            "   - `if not numbers:` checks if the list is empty. If it is, the function returns `None` to avoid errors.\n",
            "\n",
            "3. **Initialization:**\n",
            "   - `max_number = numbers[0]` sets the initial value of `max_number` to the first element of the list.\n",
            "\n",
            "4. **Iteration and Comparison:**\n",
            "   - `for number in numbers:` iterates through each\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a9b9233-c384-45ee-9fbb-cf39611b83bb",
      "metadata": {
        "height": 370,
        "id": "4a9b9233-c384-45ee-9fbb-cf39611b83bb"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ee8f283-71da-4736-9ac1-ba4ce9d5367f",
      "metadata": {
        "id": "6ee8f283-71da-4736-9ac1-ba4ce9d5367f"
      },
      "source": [
        "## 6. Generate Python samples with pretrained Python model\n",
        "\n",
        "Here you'll use a version of TinySolar-248m-4k that has been further pretrained (a process called **continued pretraining**) on a large selection of python code samples. You can find the model on Hugging Face at [this link](https://huggingface.co/upstage/TinySolar-248m-4k-py).\n",
        "\n",
        "You'll follow the same steps as above to load the model and use it to generate text."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2xW--yxP3ccQ"
      },
      "id": "2xW--yxP3ccQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30a3d3a3-1b57-4e56-98ad-58631485a58c",
      "metadata": {
        "height": 30,
        "id": "30a3d3a3-1b57-4e56-98ad-58631485a58c"
      },
      "outputs": [],
      "source": [
        "model_path_or_name = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db2240fd-19d6-4f44-89dc-9325f4fdc6b0",
      "metadata": {
        "height": 336,
        "id": "db2240fd-19d6-4f44-89dc-9325f4fdc6b0"
      },
      "outputs": [],
      "source": [
        "prompt = \"def find_max(numbers):\"\n",
        "\n",
        "inputs = tiny_custom_tokenizer(\n",
        "    prompt, return_tensors=\"pt\"\n",
        ").to(tiny_custom_model.device)\n",
        "\n",
        "streamer = TextStreamer(\n",
        "    tiny_custom_tokenizer,\n",
        "    skip_prompt=True,\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "outputs = tiny_custom_model.generate(\n",
        "    **inputs, streamer=streamer,\n",
        "    use_cache=True,\n",
        "    max_new_tokens=128,\n",
        "    do_sample=False,\n",
        "    repetition_penalty=1.1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ec94120",
      "metadata": {
        "id": "4ec94120"
      },
      "source": [
        "Try running the python code the model generated above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d109e788-2128-470d-8099-0a641938e062",
      "metadata": {
        "height": 115,
        "id": "d109e788-2128-470d-8099-0a641938e062"
      },
      "outputs": [],
      "source": [
        "def find_max(numbers):\n",
        "   max = 0\n",
        "   for num in numbers:\n",
        "       if num > max:\n",
        "           max = num\n",
        "   return max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "868a767b-b5a1-4986-bef5-156a7e5a7acb",
      "metadata": {
        "height": 30,
        "id": "868a767b-b5a1-4986-bef5-156a7e5a7acb"
      },
      "outputs": [],
      "source": [
        "find_max([1,3,5,1,6,7,2])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}